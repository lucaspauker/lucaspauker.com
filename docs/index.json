[{"content":"Recently, when I was at the airport looking to get a ride share back home I checked the price of both Uber and Lyft. I noticed that an Uber was 25$ and a Lyft was 39$…that is a price difference of 14$, or more than half the price of the Uber! The Lyft ride was 56% more expensive…\nHow can two multibillion publicly traded companies have such different pricing? Both companies have entire teams devoted to determining pricing, yet they are still so off.\nRideshare cars photo from ChatGPT\nAt the end of the day, ride sharing is a market. The app connects riders (buyers of a service) to drivers (sellers of a service). However, this market is different than most financial markets. Namely, each rider has a different start and end point. This means that rides are not fungible. Accurate pricing must include the start location, the end location, time of day, traffic, etc.\nWe propose a solution to have ride sharing pricing be discovered using a free market. By doing so, we can create the most accurate and fair pricing of any ride sharing app. Furthermore, since pricing is discovered using a market, we do not have to spend as many resources on price determination as other players in the space.\nFirst, we will present a simplified version of the idea. Then, we will dive into a more complex, real-world version of the idea.\nSimplified model We will use Chicago as an example. The first problem we must solve is how to create a market that all customers can use. This means that customers that are on different routes should be able to place bids in the same market. The simple way to do this is to use a metric that is common across all rides. Instead of buying and selling rides, we will buy and sell dollars per minute. Since we are only using Chicago, we can assume that all the dollars per minute across the city are comparable. In the marketplace, drivers place offers on dollars per minute. We can intuitively think of this as the rate that the drivers are willing to take to get paid. For other ride share apps, drivers cannot set this in the same way. On the other side of the marketplace, riders place bids on dollars per minute. A rider that needs to go somewhere quickly will just lift the best offer. A rider that wants to wait and save can place a competitive offer. Whenever there is a fill in the market, a driver will give a real ride to the rider for the agreed upon price.\nOne issue with this market is that the price discovery will be difficult, especially with a small number of drivers and riders at the beginning. Furthermore, drivers and riders may not want to place bids and offers from a UX perspective. To solve this, we will have a separate derivatives market where price discovery happens. This market can be used by speculators and market makers and will settle in cash. The market will be set up as a futures market with the settle price occurring during a specific window. The market will settle to the weighted average of the dollar per minute of all rides taken during the window. Sharp participants in the market will take into account events such as a Chicago Bears game happening during the window that would affect supply and demand.\nThen, when a driver or rider logs onto the app, they can see what the market-determined dollar per minute is and use this.\nHow the simplified model works\nGeneralized model The price per minute model fails since there are so many variables that affect the price of a ride such as start location, end location, time of day, traffic, weather, etc. Because there are so many variables, we cannot have a market for every combination of variables. Even if we decide to make markets for buckets of these variables (i.e. rainy, snowy, sunny weather), there are still too many combinations and this is a subjective way to do it. We want to have a more general formulation for getting fair prices.\nFor each ride \\(i\\) that gets requested, we can list off the \\(n\\) features: $$F_i\\in\\mathbb{R}^n.$$ The features can be arbitrary based on what we think is relevant for pricing. Next, in our marketplace, there will be agents that submit their predicted the price of a ride for a given set of features. Each agent \\(k\\) has an algorithm that spits out a price: $$a_k(F_i)\\in\\mathbb{R}.$$ Once a ride is completed, we observe the true transaction price \\(p_i\\). We use this true price as the ground truth to evaluate the accuracy of each agent’s prediction. To incentivize good pricing behavior, each agent is rewarded based on how close their predicted price \\(a_k(F_i)\\) is to the true price \\(p_i\\). This can be done using a scoring rule such as: $$R_{k, i} = - |a_k(F_i) - p_i|,$$ or a smoother version like a Gaussian or log scoring rule that sharply penalizes large deviations and rewards accurate forecasts. Over time, agents who consistently predict well will earn more, while poorly performing agents may be de-weighted or removed. This structure turns price discovery into an open, competitive marketplace for algorithms. Rather than relying on a centralized pricing engine, the platform outsources the pricing function to a distributed set of market participants. Each participant contributes pricing intelligence based on different perspectives, models, or data sources, and is financially incentivized to be as accurate as possible. Importantly, this also allows the system to adapt dynamically. If traffic patterns shift due to a new road closure or there\u0026rsquo;s an unexpected surge in demand near a stadium, agents that can quickly pick up these signals and adjust their pricing logic accordingly will profit. This creates a robust, decentralized forecasting mechanism that improves over time without centralized tuning.\nNow that we have a system for generating decentralized, competitive price predictions, we can layer a UX on top that is intuitive and responsive to real-time market conditions. When a rider opens the app and enters their desired destination, the app computes the ride\u0026rsquo;s feature vector \\(F_i\\) and queries the pricing market. It displays the current predicted fair price based on the aggregate of agent submissions—this could be a median or weighted average of \\(a_k(F_i)\\) values. This could also be rated by the average reward of the algorithm to give more weight to the accurate algorithms. The rider can then choose to:\nAccept the market price and be matched with the best available driver. Place a lower bid if they are willing to wait for a driver to accept a cheaper rate. Request a premium ride with priority matching if they are in a hurry. On the driver side, the app continuously shows nearby ride requests, each with the proposed price and estimated trip details. Drivers can:\nAccept a posted price (either rider-submitted or market-based). Post their own ask if they are only willing to drive at a higher rate. Once a ride is matched and completed, the system logs the final transaction price \\(p_i\\) and uses it to score the agents who predicted that ride’s price. This feedback loop keeps the pricing market aligned with real-world behavior and allows the app to function as a real-time market clearing platform without needing static pricing rules or fixed fare tables. This UX model strikes a balance between market flexibility and ease of use. Riders still see a simple “price to get home,” but that price is rooted in a deeper, incentive-aligned marketplace. At the same time, power users—drivers or riders—can express preferences and benefit from price discovery.\nTo complement the decentralized ride-level pricing system, we can maintain a city-wide futures market that trades contracts on the average ride cost (e.g., dollars per minute) over specific future time windows. This market captures macro-level expectations around supply and demand—such as rush hour, weather disruptions, or events—and provides a benchmark signal that pricing agents can reference or hedge against. It adds liquidity, enables forward-looking speculation, and strengthens overall price discovery by anchoring local predictions to broader market sentiment.\nHow the generalized model works\nComparing the models Feature Simple Model (Dollar/Minute) Generalized Model (Decentralized Pricing Agents) Pricing Metric Flat $ per minute across city Price predicted from ride-specific feature vector \\(F_i\\) Fungibility Assumption Rides are interchangeable by duration Rides are non-fungible, context-sensitive Price Discovery Single market for $ per minute Competing agents submit price predictions per ride Market Participants Riders and drivers directly bid/ask Agents price, riders and drivers accept/submit bids Flexibility Low – can\u0026rsquo;t adapt to ride-specific factors High – adapts to origin, destination, traffic, weather, etc. Futures Market Integration Core mechanism for price discovery Provides macro-level benchmark for local predictions Scalability Limited – breaks down in edge cases Scales with algorithm diversity and feature dimensionality Long-Term Adaptability Manual adjustments required Improves over time via competition and feedback Challenges and limitations While a decentralized, market-driven pricing system offers flexibility, transparency, and adaptability, there are several key challenges that must be acknowledged:\nLiquidity: If there are too few drivers, riders, or pricing agents participating at any given moment, price discovery may become unstable or noisy. In particular, thin markets could lead to large pricing errors or poor matching, especially during off-peak hours or in low-density areas.\nUser experience complexity: Introducing features like bidding, negotiation, or multiple price predictions may confuse users accustomed to the simplicity of “tap and go” ride apps. Balancing the benefits of market-driven flexibility with the need for an intuitive UX is non-trivial.\nLatency: Real-time matching and pricing require extremely low-latency systems. If pricing agents must compute and submit predictions in milliseconds, or if auctions are conducted for each ride, the infrastructure must be robust and fast enough to avoid slowing down ride dispatch.\nGaming the system: Both pricing agents and users (riders/drivers) could try to manipulate the system—for example, by submitting misleading bids or gaming the prediction scoring mechanism. Designing incentive-compatible mechanisms is critical.\nCold start: In the early phase of the platform, pricing agents may lack enough data to produce accurate models, and user participation may be too low to support price discovery. This makes it difficult to reach equilibrium or to calibrate the reward mechanism for agents.\nRegulatory and legal constraints: A fully decentralized pricing model could conflict with local regulations around transportation services, fare transparency, or algorithmic accountability. These systems would need to be carefully evaluated for legal compliance.\nAI cars with Newber\n","permalink":"https://lucaspauker.com/articles/newber/","summary":"Recently, when I was at the airport looking to get a ride share back home I checked the price of both Uber and Lyft. I noticed that an Uber was 25$ and a Lyft was 39$…that is a price difference of 14$, or more than half the price of the Uber! The Lyft ride was 56% more expensive…\nHow can two multibillion publicly traded companies have such different pricing? Both companies have entire teams devoted to determining pricing, yet they are still so off.","title":"Newber"},{"content":"I remember watching a video a while ago where a pro card counter answers questions about gambling. In one of his answers, he said that what makes someone a true gambler is that the joy of winning a bet is more than the sadness from losing a bet. This has stuck with me since I watched the video over a year ago. Since then, I sometimes think about why people gamble and what makes some people more prone to gambling. I play poker with my friends weekly, and in my group, there are many different kinds of gamblers. Some people are there for fun, some care more about making money, and some just like the thrill of gambling.\nWhy would you ever gamble? Most gambling has negative expected value (EV), which is the average amount you can expect to win or lose per bet if you were to play an infinite number of times. When you play any game at the casino (besides poker, perhaps), you have negative EV. This means that even if you play the game perfectly, you will lose money in expectation over time. Of course, you can still make money by getting lucky, but over time with enough bets you will lose money. Don\u0026rsquo;t believe me? Look at how big and expensive the casinos are in Vegas\u0026hellip;they didn\u0026rsquo;t get that big by losing money in their own games.\nI made a simulation of the amount of money a player will have in a game where the house has a 1% edge. In the simulation, the player always bets 5% of their money and they play 100 games each trial. The game has a 50% chance of winning: if you lose you lose your bet and if you win you get your bet * 1.98 back. So the EV of this game is 0.5 * 1.98 = 0.99.\nAs we can see, the game is negative EV\u0026ndash;you are expected to lose about 35$ after playing 100 times\u0026hellip;and most casino games have a greater than 1% edge. So then why would you ever gamble? Are gamblers irrational or dumb? I don\u0026rsquo;t think this is the case\u0026ndash;I think there are actually a few different types of gamblers. Most of these gamblers are making rational decisions, but some of their decision making criteria may be flawed. I will describe them more in depth but here are the three types:\nRecreational gambler Misinformed gambler Rational gambler General framework In economics, there is a concept of utility. Utility is how much value someone gets from certain goods, services, or outcomes. Utility varies from person to person. For example, I spend my money on musical instruments since I get utility from playing music, while someone else might instead spend their money on unicycles or juggling clubs since they get utility from that. So utility is a general framework for understanding why people make certain decisions. If someone makes a decision to do something they expect to get more utility from doing that than the alternative. So, even if gambling is negative EV in terms of money, it can be positive EV in terms of utility. However, each type of gambler derives their expected utility from a different source.\nFormally, each person has a utility function \\(U\\) which takes as inputs the assets and experiences the person has. Therefore, the \\(\\Delta U(\\text{action})\\) function is how much the utility will change after a certain action. Since we are dealing with gambling, which is uncertain, we will usually think of \\(U\\) and \\(\\Delta U\\) as the expected utility and expected change in utility, respectively. Some of the equations in general will not be exact, but I am just trying to communicate the main ideas.\nIn order for it to make sense to gamble, we need to have $$\\Delta U(\\text{making bet}) \u0026gt; \\Delta U(\\text{not making bet}) = 0.$$ We assume that the expected change in utility of not making a bet is zero for simplicity.\nFor a risk-averse person who does not gamble, the expected change in utility for making a bet might look like this: $$ \\Delta U(\\text{making bet}) = p_\\text{win} * \\Delta U(\\text{win payout}) + (1-p_\\text{win}) * \\Delta U(\\text{losing bet}) \u0026lt; 0, $$ where \\(p_\\text{win}\\) is the chance of winning the bet. Because a risk averse person would rather not lose money than take the chance to win money, the utility of making the bet is negative and they will therefore not gamble.\nRecreational gambler The first type of gambler is the recreational gambler. This is the kind of person that likes to play poker for fun or go to the casino once in a while with friends for a good time. The recreational gambler understands that they are going to lose money on average, but they derive utility from the act of gambling, since the thrill is fun.\nSo for the recreational gambler, we have $$\\Delta U(\\text{gambling}) = \\Delta U(\\text{enjoyment of gambling}) + \\Delta U(\\text{making bet}) \u0026gt; 0$$ and therefore $$\\Delta U(\\text{enjoyment of gambling}) \u0026gt; -\\Delta U(\\text{making bet}).$$ So for the recreational gambler, the utility gained from the enjoyment of gambling outweighs the negative utility of placing the bet.\nMisinformed gambler The second type of gambler is the misinformed gambler. This is someone who THINKS they have positive EV but actually has negative EV. The place where I see this happen the most is sports betting. Because people watch football, they think that they can beat the books and predict the outcomes of games better than the books. While it is certainly possible to beat the books, most people do not succeed in doing so. It is also possible to be a misinformed gambler in a game like poker, where a player may overestimate their skill.\nFor the misinformed gambler, their expected utility function does not match the true utility of making the bet. $$ \\begin{align} \\Delta U(\\text{making bet}) =\u0026amp; \\ p_\\text{win perceived} * \\Delta U(\\text{win payout}) \\\\ \u0026amp;+ (1-p_\\text{win perceived}) * \\Delta U(\\text{losing bet}) \\\\ \u0026amp;\u0026gt; 0, \\end{align} $$ where \\(p_\\text{win perceived}\\) is the perceived chance of winning the bet.\nRational gambler The last category of gambler is the rational gambler. A rational gambler is someone who gets more joy from winning a bet than sadness from losing the bet. In order to understand the rational gambler, we need to look at the marginal utility function for money. The marginal utility is the amount of utility someone gets from their Nth dollar. So basically we think of how much utility do I get from the 1,000th dollar in my bank account compared to the 10,000th dollar. For a non-gambler, money has less utility as they get richer. This is because there is really a fixed amount of things to spend money on and after the first superyacht\u0026hellip;will you really be much happier with a second?\nHowever, for a rational gambler, the marginal utility function is flipped: it has a positive slope. This means that the gambler gets more utility from being rich than from being poor.\nWith this marginal utility curve, the utility of making a bet becomes $$ \\Delta U(\\text{making bet}) = p_\\text{win} * \\Delta U(\\text{win payout}) + (1-p_\\text{win}) * \\Delta U(\\text{losing bet}) \u0026gt; 0, $$ meaning that taking a bet is positive EV in utility, even though the gambler is expected to lose money.\nThe question is why would someone have a utility function like this? In general, the reason is that the utility of money is based on what you can spend with the money.\nThe first explanation of why this curve makes sense is for someone who is a compulsive spender. Imagine that you are someone who spends all the money you have on a meal at the end of the day and you earn $10 every day. So that means you can get a Subway sandwich every day. Or instead, you could gamble that $10 and risk eating beans and rice ($1) for the chance at getting a steak dinner ($80). Let\u0026rsquo;s say that nine times out of 10 you end up eating beans and rice, but one time out of ten you get the steak dinner. So would you rather eat Subway every day or take the gamble and mostly eat beans and rice but sometimes get a very nice dinner? To me, this choice is not very clear and it makes sense why one would choose the gamble.\nA second explanation has to do with someone who thinks they are unable to make enough money to get out of their current socioeconomic class or acheive a financial goal. For example, if you cannot save much and see that it will take a hundred years to buy a house, then how can you deal with this? One option is to save for a hundred years and never get your house, or instead you can buy a lottery ticket every week and have a chance at acheiving a house in the short term. Sometimes, this seems like the only way.\nIn reality, marginal utility is not a simple straight line or decreasing function for just about anyone. For example, money to spend on food and rent has much more utility than money spent on recreation. Furthermore, even for a gambler, there is a certain point where money has decreasing value. So the marginal utility curves need to be thought of as a guide to understanding decision making, not a fact of how everybody thinks.\nConclusion In articles about gambling, I think that there is not enough emphasis into understanding why people decide to gamble in the first place. Often, gamblers are treated as addicts with a problem or irrational people. I hope that increasing understanding into the motives behind gambling in the first place can lead to better policy regarding gambling.\n","permalink":"https://lucaspauker.com/articles/gambling-rationality/","summary":"I remember watching a video a while ago where a pro card counter answers questions about gambling. In one of his answers, he said that what makes someone a true gambler is that the joy of winning a bet is more than the sadness from losing a bet. This has stuck with me since I watched the video over a year ago. Since then, I sometimes think about why people gamble and what makes some people more prone to gambling.","title":"Why Gambling is (more) Rational (than you think)"},{"content":" With holiday season in full swing, people may be more enticed to give to charity. However, there is a question of how to optimize your charitable donations. Specifically, I would like to optimize charitable donations for maximum impact. It seems to me that there are two parts to this equation:\nWhich charity(ies)/cause(s) should I give to How much should I give? The answer to the first question is deeply personal, so I want to focus on the second question. A naive answer to this question might be to donate as much as you can. However, this actually might not be the right choice if you believe you can compound your money and donate more in the future. The other extreme is to compound your money until you die and donate it all then. However, there are people that need help now so this seems like a bad solution as well.\nI want to create a framework for deciding how much to give each year (based on some intuitive parameters).\nExisting frameworks for charitable giving Before diving into my framework, it is interesting to review some existing approaches to charitable giving, many of which are grounded in religious traditions.\nIn Jewish tradition, giving to charity, or tzedakah, is a core principle. The Torah emphasizes the importance of helping those in need, and many interpretations suggest donating approximately 10% of one’s income. This practice is often referred to as ma\u0026rsquo;aser kesafim (giving a tenth).\nIn Islam, Zakat is one of the Five Pillars, making charitable giving a fundamental religious duty. Muslims are generally required to donate 2.5% of their accumulated wealth (beyond basic needs) each year.\nWhile these frameworks provide clear guidelines, they tend to use a fixed percentage of wealth or income. This approach, while simple and effective, may overlook important considerations such as individual financial circumstances, the diminishing marginal utility of wealth, and the impact of time on charitable effectiveness.\nHere is a simulation of donating a fixed percentage of a portfolio that grows at a fixed rate plus income.\nConstant percentage calculator Initial Portfolio Value (V₀): $1000000 1K 1M Annual Portfolio Growth Rate (p%): 8% 0% 30% Annual Income Minus Expenses (iₜ): $0 0 1M % Donated: 5 0 100 Framework for optimizing donations Now we will build up a framework for understanding first how a portfolio evolves with donations and then how varying impact of individual donations can change how much to donate. The core idea of the framework is to balance immediate charitable impact with the future growth of wealth.\nJump to simulation 1\nJump to simulation 2\nInput variables Current/initial portfolio value: \\( V_0 \\) (your starting wealth) Life expectancy: \\( T \\) years (remaining years you expect to live) Annual portfolio growth rate: \\( p \\) (as a percentage) Desired wealth at death: \\( Y \\) (how much you want to leave behind, e.g., for family or legacy purposes) Yearly net income: \\( i_t \\) (your income after expenses in year \\( t \\)) Impact discount rate: \\( d \\) (percentage decrease in the effectiveness of charitable donations over time due to compounding issues like inflation, diminishing needs, etc.) Impact Function: \\( I(t) \\) (a function of time that determines the relative impact of donations in year \\( t \\); by definition, \\( \\sum_{t=1}^T I(t) = 1 \\) Output variable The yearly amount to donate: \\( d_t \\).\nBuilding the framework The model starts with the evolution of your wealth year over year. Here’s the basic equation:\n$$V_t = p \\ V_{t-1} + i_t - d_t$$\nWhere:\n\\( V_t \\) is your portfolio value at the end of year \\( t \\). \\( p \\ V_{t-1} \\) represents the growth of your wealth from investments. \\( i_t \\) is your net income for year \\( t \\). \\( d_t \\) is the amount you donate in year \\( t \\). We impose the terminal condition that you want to end with exactly your desired wealth \\( Y \\) at the end of your life (year \\( T \\)):\n$$ V_T = Y $$\nWe can work recursively to express \\( d_t \\) as a function of the input variables. Expanding the wealth equation step by step:\nStarting with year 1: $$ V_1 = p \\ V_0 + i_1 - d_1 $$\nFor year 2: $$ V_2 = p \\ V_1 + i_2 - d_2 $$ Substitute \\( V_1 \\): $$ V_2 = p \\ (p \\ V_0 + i_1 - d_1) + i_2 - d_2 $$\nGeneralizing this, and using the terminal condition \\( V_T = Y \\), the portfolio value at year \\( T \\) becomes:\n$$ V_T = Y = p^T \\ V_0 + \\sum_{t=1}^T p^{T-t} \\ i_t - \\sum_{t=1}^T p^{T-t} \\ d_t $$\nRearranging for the summation of donations \\( d_t \\): $$\\sum_{t=1}^T p^{T-t} \\ d_t = p^T \\ V_0 + \\sum_{t=1}^T p^{T-t} \\ i_t - Y = k $$ where \\(k\\) is a constant.\nNow, we have a constraint formula for donations as a function of the portfolio return rate, initial portfolio value, life expectancy, income, and desired end of life portfolio value. Now, we want to see which \\(d_t\\) are available. There are many \\(d_t\\) that satisfy the constraint above. One such function is $$ d_t = \\begin{cases} 0 \u0026amp; \\text{if } t \u0026lt; T \\\\ p \\ V_{T-1} + i_T - Y \u0026amp; \\text{if } t = T \\end{cases} $$ This is just donating everything at the end of your life up to \\(Y\\). This is the way to maximize the total amount donated over your lifetime, assuming income and portfolio returns are positive. However, we do not necessarily want to do this because we want to balance the total amount donated with immediate impact.\nConstant percentage donations An intuitive way to determine donations is to use a constant percentage of our portfolio for donations. So we have \\(d_t=u \\ V_t\\) where \\(u\\) is the percent of our current portfolio to donate. We can determine \\(u\\) using our constraint equation, so we get $$\\sum_{t=1}^T p^{T-t} \\ d_t = \\sum_{t=1}^T p^{T-t}\\ u \\ V_t = u\\ \\sum_{t=1}^T p^{T-t} \\ V_t = k $$ so therefore $$u = \\frac{k}{ \\sum_{t=1}^T p^{T-t} \\ V_t} $$ and also $$V_t = \\frac{p \\ V_{t-1} + i_t}{1+u}$$\nWe can numerically solve for \\(u\\) but finding a closed form is hard due to \\(V_t\\) dependence on \\(u\\). Here is a simulation for finding the optimal fixed percentage based on the input variables.\nOptimal percentage calculator Initial portfolio value (V₀): $1000000 1K 1M Time horizon (T years): 10 1 50 Annual portfolio growth rate (p%): 8% 0% 30% Annual income minus expenses (iₜ): $0 0 500K Desired wealth at horizon (Y): $1000000 0 3M Optimal annual donation: 0 Including impact Jump to simulation 2\nIn reality, however, the personal impact of donations changes from year to year. This is because you would like to donate to causes that are important to you which can change. We will therefore try to maximize the net impact $$ U = \\sum_{t=1}^T {I(t)\\ d_t} $$\nIf \\(I(t)\\) is stochastic, then we are trying to maximize the expectation of \\(U\\). Let\u0026rsquo;s assume that \\(I(t) \\sim \\mathcal{N}(\\mu, \\sigma^2)\\).\nIf we simply maximize \\(U\\), then we will not donate money every year. So instead, we maximize $$U_{\\text{risk-averse}} = \\sum_{t=1}^T I(t)\\ d_t - \\gamma \\cdot \\sigma^2 \\sum_{t=1}^T d_t^2$$ where \\(\\gamma\\) is a constant. This measure takes into account the variance of the utility.\nRecalling we have the constraint $$\\sum_{t=1}^T p^{T-t} \\ d_t = k,$$ in order to maximize \\(U_{\\text{risk-averse}}\\), we write the Lagrangian $$\\mathcal{L} =\\sum_{t=1}^T I(t)\\ d_t - \\gamma \\cdot \\sigma^2 \\sum_{t=1}^T d_t^2 + \\lambda \\left( k - \\sum_{t=1}^T p^{T-t} \\ d_t \\right)$$\nThen we calculate the partial derivatives and set them to zero: $$\\frac{\\partial \\mathcal{L}}{\\partial d_t} = I(t) - 2 \\gamma \\ \\sigma^2 \\ d_t - \\lambda \\ p^{T-t} = 0$$\nSolving for \\(d_t\\), we get $$d_t = \\frac{I(t) - \\lambda \\ p^{T-t}}{2 \\gamma \\ \\sigma^2}$$\nWe then solve for \\(\\lambda\\) by substituting the expression for \\(d_t\\) into the constraint $$\\sum_{t=1}^T p^{T-t} \\ \\frac{I(t) - \\lambda \\ p^{T-t}}{2 \\gamma \\ \\sigma^2} = k,$$ which simplifies to $$\\lambda = \\frac{\\sum_{t=1}^T p^{T-t} \\ I(t) - 2 \\gamma \\ \\sigma^2 \\ k}{\\sum_{t=1}^T p^{2(T-t)}}$$\nNow we have an expression for \\(d_t\\) in terms of the input variables! We can simulate some impact functions and test what the results of this method are.\nRisk averse calculator Initial portfolio value (V₀): $1000000 1K 1M Time horizon (T years): 10 1 50 Annual portfolio growth rate (p%): 8% 0% 30% Annual income minus expenses (iₜ): $0 0 500K Desired wealth at horizon (Y): $1000000 0 3M Risk aversion parameter (γ): 0.5 0 1 Impact standard deviation (σ): 0.5 0 1 Impact in the first period (I(1)): 0.65 0 1 # Simulations: 5 1 100 First year donation: 0 Conclusion In this article, we have looked at three frameworks for deciding how much to donate based on a changing portfolio:\nFixed percentage of portfolio or income each year based on religious rules. Fixed percentage of portfolio each year based on desired portfolio value after \\(T\\) years. Varying percentage of portfolio each year based on impact-weighted model. These models have different degrees of complexity and parameters. Hopefully by thinking about these models and looking at simulations, we can be more intentional about giving.\n","permalink":"https://lucaspauker.com/articles/charity-calculator/","summary":"With holiday season in full swing, people may be more enticed to give to charity. However, there is a question of how to optimize your charitable donations. Specifically, I would like to optimize charitable donations for maximum impact. It seems to me that there are two parts to this equation:\nWhich charity(ies)/cause(s) should I give to How much should I give? The answer to the first question is deeply personal, so I want to focus on the second question.","title":"Charity Calculator"},{"content":" Why home runs? Some of the best moments in baseball games are home runs. Something about hitting the ball out of the park is satisfying. Since baseball season just started, I wanted to model a part of the game. I decided to model home runs since they are pretty rare events but should still be able to be accurately predicted. When I say accurately predicted, I mean that we can accurately predict the probability of a player hitting a home run. At the end of the day, we can only compare to the true data\u0026hellip;but we have many years of baseball data (baseball is nice since there are very detailed statistics and metrics for each player for 100+ years).\nWe are going to model the probability that a player hits a home run in a game. There are only two outcomes: a player hits a home run in the game or they don\u0026rsquo;t. We will use the past couple years of MLB data to build the model, and if we need more data in the future it is easy to add.\nThere are many challenges that we\u0026rsquo;ll have to overcome in order to build a good model. First, even though modeling the probability of a player hitting a HR in a game seems relatively simple, there are lots of variables that can affect this probability:\nBatter skill Which pitcher(s) the batter goes against Batter skill against different pitchers How many times a batter bats per game Stadium (different stadiums have different geometry. And location matters: at Coors Field in Denver, balls travel further due to the less dense air.) Weather Probably many other factors I think the most important variables to consider are batter skill, pitcher skill, stadium, and weather. We will start by building a simple model and then add more feature variables to improve the model. We can also iterate on the model itself.\nData data data In order to build a good model, we need good data. Luckily, baseball-reference.com has detailed data from decades of baseball history. For example, we can see that for a given random game, the website lists which players got home runs.\nIn order to think of the data we need, it is helpful to me to think of the interface that we want for our data in order to do the analysis and make it extendable in the future.\nGame class Game: id -\u0026gt; int time -\u0026gt; timestamp location -\u0026gt; str home_team -\u0026gt; str away_team -\u0026gt; str home_team_lineup -\u0026gt; list of Players away_team_lineup -\u0026gt; list of Players Player class Player: _id -\u0026gt; int name -\u0026gt; str pitcher_or_hitter -\u0026gt; bool team_name -\u0026gt; str stats -\u0026gt; dict Batting average -\u0026gt; int On base % -\u0026gt; int ... functions: get_stats_before_game( game_id ) -\u0026gt; dict did_player_hit_homerun( game_id ) -\u0026gt; bool Training data To build the training data, we can do:\nX, y = [], [] for each game in training data: for each player in game: X.append( player.get_stats_before_game( game._id ) ) y.append( player.did_player_hit_homerun( game._id ) ) Then, we can train any binary classification model.\nGraphs After downloading the data for 2022-2023, we will explore it with some graphs and preliminary data analysis. Let\u0026rsquo;s explore some of the data we have. We will look at data from the 2022 and 2023 regular season. One thing we have to consider is that stats become more consistent as more games are played. For example, if a player has a bad first game they could have a batting average of zero, which will then level out to a more consistent level as more games are played. We can look at the data to see when stats level out. Then, when we train and test our model we can disregard these games. Here are a few stats for Matt Olson, the 2013 batting average leader.\nMatt Olson season stats\nWe can see from this graph that these stats tend to even out after 10-20 games. Let\u0026rsquo;s also look at batting average for the entire league after a certain number of games:\nLeague batting average over a number of games after first game played\nWe can see that from the top graph it seems that after 20 games, there is a lot less variation in batting average. From the standard deviation in the bottom graph, it also seems that standard deviation levels off after around 20 games. Therefore, we will require at least 20 previous games for a player to train and predict whether they got a home run.\nData analysis We can analyze our 2022/2023 data and see how whether a player hit home runs correlates to our stats. For each game in the 2022 and 2023 regular seasons, we aggregate the stats of each player before the game and whether they hit a home run in that game.\nStat Mean when player did not hit home run Mean when player hit home run Percent difference Batting average 0.246 0.249 +1.27% On base % 0.317 0.322 +1.88% Slugging % 0.401 0.429 +6.87% RBIs per AB 0.125 0.138 +10.21% Home runs per AB 0.033 0.041 +24.60% At Bats Per Game 3.216 3.369.041 +4.74% The stats that change the most are RBIs per AB and home runs per AB. Home runs per AB is the stat that has the most difference between players that hit home runs in a game and those that did not. This makes sense that home runs per AB is a probably a good predictor of if a player will hit a home run in a game.\nWe can see that batting average has only a 1% increase in players that hit a home run in a game compared to players that did not. In contrast, slugging percent increased by 7% between players that hit home runs in a game and those that did not. This makes sense because slugging more directly indicates home run power than batting average. Also players who hit lots of home runs often strike out a lot.\nBuilding the model We will build a binary classification model with logistic regression to predict home run probability per game.\nFeatures We will first build a base model that we can later improve on. For our base model, we only use data from the batter (i.e. batting average, slugging, etc.). Later, we will add data about the starting pitcher and other data. We gather the following stats for each player before each game and use these as our features for the model:\nBatting average Slugging % On base % Home runs per AB RBIs per AB Average ABs per game The last feature, average ABs per game, is important because we are modeling home runs per game not AB. Therefore, a pinch hitter is less likely to hit a home run in a game than a starter since they have less ABs total.\nTrain/test/split We divide our data in train/val/test with proportions of 70% / 10% / 20%. Overall, we have 60k training examples, 8.5k validation examples, and 17k test examples.\nLogistic regression We will use a logistic regression model to do binary classification on whether a player will hit a home run. Logistic regression is good for a base model to do binary classification. To formalize our problem, we assume that we have the \\(i\\text{th}\\) data pair \\(\\left(x_i\\in\\mathbb{R}^m, y_i\\in\\{0,1\\}\\right)\\). \\(x_i\\) is the feature vector and \\(y_i\\) is whether a player hit a home run (1=true and 0=false). For logistic regression, we have our hypothesis function $$h_\\theta(x_i)=\\frac{1}{1-e^{-\\theta^{\\ T}x_i}},$$ This is the logistic function. We are modeling probability \\(p_i\\), $$p_i=P(y_i=1|x_i;\\theta)=h_\\theta(x_i)$$ $$1-p_i=P(y_i=0|x_i;\\theta)=1-h_\\theta(x_i).$$ We see that the likelihood function \\(L\\) is $$L(\\theta)=\\prod_i^{n} \\ p_i^{y_i} \\ \\left(1-p_i\\right)^{1-y_i}.$$\nNow, we maximize the log likelihood function with stochastic gradient descent [1]. We use the sklearn LogisticRegression class to train and test the model.\nEvaluating the model For evaluation, we see that our logistic regression model outputs probabilities, but our ground truth data is only 1 or 0. Therefore, there are two ways we can evaluate our model. The first is to see if the probabilities our model outputs correspond to the estimated true probabilities. The second is to pick a threshold value to convert the probabilities into 1 or 0 and use normal binary classification metrics.\nProbability bins Hitting a home run is inherently random. Even the best player in the best conditions against the best pitcher will not always hit a home run. This is because hitting a home run in baseball is really hard! For evaluation, however, we do not have the true probabilities\u0026hellip;we only have whether the player hit a home run in the game or not. Therefore, for evaluation we should simulate a bunch of games with our probabilities and see if it lines up with the real-world results. This process is building probability bins.\nWe have that our logistic regression model \\(f(x_i)=h_{\\theta^*}(x_i)=p_{i, \\text{pred}}\\) outputs a probability of the positive class (home run). We want to look at bins of our logistic model outputs and compare them to the true results.\nFirst, we can divide our \\(N\\) datapoints into groups of 2000 elements. Since our training data had 60k examples, we have 30 total bins.\nFor each bin \\(b\\), there is a set of predicted probs from our model \\(\\alpha=\\{p_{\\text{pred},i\\in b}\\}\\) and a set of true outcomes \\(\\beta=\\{y_{\\text{pred},i\\in b}\\}\\). We then calculate the mean of \\(\\alpha\\) and the mean of \\(\\beta\\) per bin. If our model is accurate, these means should be close.\nProbability bins\nWe can see that the logistic regression does a good job of approximating the true probabilities. It seems at the lower end of the range between 0.05 and 0.10, the model underpredicts pretty consistently. Between 0.10 and 0.15, the model seems to have more variation than at the lower end of the range however.\nIt is also interesting to see how the model predictions are distributed: Predicted probability histogram\nWe can see that our model typically outputs probabilities between 0.05 and 0.16.\nBinary classification metrics Our logistic regression model \\(h_{\\theta^*}(x_i)=p_{i, \\text{pred}}\\) outputs a probability of the positive class (home run). We then predict \\(y_i\\) by $$ y_{i, \\text{pred}} = \\begin{cases} 0, \u0026amp; \\text{if $p_{i,\\text{pred}}\\geq t$} \\\\ 1, \u0026amp; \\text{if $p_{i,\\text{pred}}\u0026lt; t$} \\end{cases}\\ , $$ where \\(0\\lt t\\lt 1\\) is the threshold value. We decide to use \\(t\\) as the mean of our predicted probabilities. Here are some binary classification metrics for our model:\nAccuracy 0.575 Precision 0.145 Recall 0.605 F-1 0.234 ROC AUC 0.617 Receiver operating characteristic (ROC) Changing the threshold value affects the true positive rate $$TPR=TP/(TP+FN)=E_i\\left[P(p_{i,\\text{pred}} \\geq t\\ |\\ y_i=1)\\right]$$ and false positive rate $$FPR=FP/(FP+TN)=E_i\\left[P(p_{i,\\text{pred}} \\geq t\\ |\\ y_i=0)\\right],$$ where we have # of true positives \\(TP\\), # false positives \\(FP\\), # true negatives \\(TN\\), and # false negatives \\(FN\\). Graphically, we can look at the distributions of our predicted probabilities for the true positive and negative classes:\nPredicted probability histogram for positive and negative classes\nBy moving the threshold \\(t\\) on the x axis, we can see that the \\(TPR\\) and \\(FPR\\) will be correlated but will change at different rates. If the two distributions were on top of each other, then we would have a random model with no predictive power. On the other hand, if there was no overlap between the distributions, then our model would be able to make predictions perfectly. The receiver operating characteristic (ROC) curve is a graph of the false positive rate compared to the true positive rate.\nROC curve\nRandom guessing produces a straight line on on the ROC graph. The better our model, the higher the area under the curve (ROC) will be.\nFeature importance Stat Model weight Accuracy of model trained without feature Batting average -0.372 0.551 On base % 0.144 0.489 Slugging % 0.287 0.548 Home runs per AB 0.091 0.547 RBIs per AB -0.009 0.551 At bats per game -0.289 0.594 Future improvements It seems that this model performs pretty well overall. However, we are using only batter data right now, so in the future we will add more data about the opposing pitching and other data such as location. We can also try new models to see if it improves our metrics. Lastly, we can train the model on more data and see if it improves. The code used for the data and graphs is here: link.\nCitations https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf ","permalink":"https://lucaspauker.com/articles/home-run-modeling/","summary":"Why home runs? Some of the best moments in baseball games are home runs. Something about hitting the ball out of the park is satisfying. Since baseball season just started, I wanted to model a part of the game. I decided to model home runs since they are pretty rare events but should still be able to be accurately predicted. When I say accurately predicted, I mean that we can accurately predict the probability of a player hitting a home run.","title":"Home Run Modeling"},{"content":"Introduction The goal of this article is to explore the latency of different OpenAI models. When using AI models in production, latency is an important factor to consider.\nComparing Model Architectures First, I test the latency for different OpenAI models. I test the following models: gpt-4, gpt-4-0613, gpt-3.5-turbo, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613, text-davinci-003, text-davinci-002, text-davinci-001, text-curie-001, text-babbage-001, text-ada-001, davinci-002, babbage-002, davinci, curie, babbage, and ada. These are all the OpenAI models that are available for inference through the chat and completions endpoints. The models can be divided into chat models, instruct models, and base models. Chat models are gpt-4 and gpt-3.5 and are LLMs that are optimized for chat. Instruct models are models that are trained with reinforcement learning through human feedback to follow instructions [1].\nThe most powerful OpenAI models are the gpt-4 models. The \u0026ldquo;0613\u0026rdquo; part of the model means that it is a checkpoint from 6/13/23. The gpt-3.5-turbo models are the newest and most powerful GPT-3 models. GPT-3 and GPT-4 are two different LLM model architectures. text-davinci-003 and text-davinci-002 are also GPT-3 models. text-davinci-001, text-curie-001, text-babbage-001, and text-ada-001 are GPT-3 models fine-tuned on instruction-following tasks (similar to InstructGPT), while davinci-002, babbage-002, davinci, curie, babbage, and ada are GPT-3 models that are not fine-tuned. Usuaully, the not fine-tuned models would be fine-tuned in order to make them useful for various tasks.\nFor the input prompt, I use \u0026ldquo;Once upon a time, in a land far, far away\u0026hellip;\u0026rdquo;. I use this prompt to make sure that the model will not stop its output early. I also set the max tokens for the output to be 100. The input prompt is 13 tokens for a total of 113 tokens in the input + output. The temperature is set to the default value of 1. Lastly, I test the response of each models 100 times. Here is a plot of the response times for the different OpenAI models. I also include error bars which show the standard deviation of response times for each model.\nAverage response time for OpenAI models\nWe can see that the slowest models from the group are gpt-4 and gpt-4-0613. Both of these models take about 6 seconds per request, which is two times slower than the next slowest model. The next slowest model is davinci by almost a second. Comparing text-davinci-001, text-curie-001, text-babbage-001, and text-ada-001 to davinci, curie, babbage, and ada, we see that the text models in general have faster response times. We can also see that davinci-002 and babbage-002 have faster response times than the analogous davinci and babbage.\nHere is a graph of the number of parameters compared to the average response time. These are the numbers of parameters for different models [2]:\n1.7 trillion (estimated [3]): gpt-4 and gpt-4-0613 175 billion: gpt-3.5-turbo, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613, text-davinci-003, text-davinci-002, text-davinci-001, and davinci 13 billion: text-curie-001 and curie 3 billion: text-babbage-001 and babbage 350 million: text-ada-001 and ada Note that the x axis is on a log scale.\nNumber of parameters vs. average response time\nWe see that as the number of parameters increases, the average response time increases.\nComparing Fine-Tuned vs. Base Models OpenAI allows users to fine-tune their models through their API. There are three models available for fine-tuning: gpt-turbo-3.5, davinci-002, and babbage-002 [4]. I wanted to test how the latency of fine-tuned OpenAI models compared to the latency of base models.\nI fine-tuned a model for each architecture on a Spanish to English translation dataset. Then, similarly to above, we send 100 requests to each model and measure the mean and standard deviation of the response times. The input prompt is a Spanish poem by Gustavo Adolfo Bécquer. The max tokens is set to 100 and temperature is set to the default value of 1. Here is the plot of the response times for fine-tuned vs. base models:\nAverage response time for fine-tuned vs. base OpenAI models\nWe can see that for all the model architectures, the average response time is slower for the fine-tuned model. However, for gpt-turbo-3.5, the error bars overlap.\nLatency at Different Times of Day I also tested the latency of OpenAI models at different times of day. I tested gpt-4 and gpt-3.5-turbo. For each model, I sent 5 requests every 15 minutes for 24 hours on a Wednesday and plotted the mean response time. I used the same settings as the first section: the \u0026ldquo;Once upon a time\u0026rdquo; prompt, 100 max tokens, and temperature of 1.\nOpenAI model latency over time\nThe entire graph is in central time. We can see that the two models follow a similar trend: the average response time is higher before 2pm or so then is lower until 10pm. We can also see that in the hours of 2am to 10am, there are a lot of dropped responses (indicated by the lack of a dot). However, more requests would be necessary to smooth this graph out. Furthermore, using the median response time may be better than using the mean.\nCitations All the code used to generate the plots can be found here.\nhttps://openai.com/research/instruction-following https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/llm-models https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/ https://platform.openai.com/docs/guides/fine-tuning ","permalink":"https://lucaspauker.com/articles/openai-model-timing/","summary":"Introduction The goal of this article is to explore the latency of different OpenAI models. When using AI models in production, latency is an important factor to consider.\nComparing Model Architectures First, I test the latency for different OpenAI models. I test the following models: gpt-4, gpt-4-0613, gpt-3.5-turbo, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613, text-davinci-003, text-davinci-002, text-davinci-001, text-curie-001, text-babbage-001, text-ada-001, davinci-002, babbage-002, davinci, curie, babbage, and ada. These are all the OpenAI models that are available for inference through the chat and completions endpoints.","title":"OpenAI Model Timing"},{"content":"Disclaimer: This article mentions https://terra-cotta.ai/, an LLM experimentation platform I am building\nIntroduction ChatGPT, Bard, and other large language models (LLMs) are very useful for a wide variety of tasks from writing code to answering complex questions to aiding with education. However, these models are ultimately limited by the data that they are trained on. Also, these models are trained to be able to answer a wide variety of questions which may not be sufficient for domain-specific questions. Fine-tuning is essential in order to make these models accurately answer domain-specific questions and be useful for difficult tasks. Furthermore, fine-tuning may be cheaper for inference.\nFine-tuning is the process of training an LLM on your own custom data. The fine-tuning process begins with a generic LLM that is pretrained on a large amount of text data. Fine tuning updates the generic model’s parameters or weights by using a smaller dataset for a target task.\nOne example of a task where fine-tuning is necessary is getting a medical diagnosis from raw medical records, such as electronic health records or medical imaging reports. ChatGPT is unlikely to perform this task well since it lacks specialized knowledge in medical domains and direct experience with real medical cases. ChatGPT will usually generate a confident response to any query, but the response cannot always be trusted due to hallucinations, where the model returns incorrect information [1]. Hallucinations are common for difficult tasks. Fine-tuning a language model specifically on validated, trusted medical data is essential to ensure accurate and reliable results. A fine-tuned model would learn the domain-specific knowledge and likely be able to return accurate diagnoses for this task.\nThe idea of fine-tuning has a strong research pedigree. The approach can be traced back to 2018, when two influential papers were published. The first paper, “Improving Language Understanding by Generative Pre-Training” by Radford et al. [2] introduces the GPT model that is now used in ChatGPT. GPT used self-supervised learning to train an LLM on a large amount of unlabeled data. In the paper, the authors show that their GPT model could achieve state-of-the-art results on multiple tasks by fine-tuning their model on specific datasets. Similarly, “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding” by Devlin et al. [3] introduced BERT, a novel transformer model and showed the ability for it to be fine-tuned and achieve state-of-the-art performance on multiple tasks after fine-tuning. The image below shows how BERT can be fine-tuned on specific tasks such as SQuAD.\nImage from BERT paper [3]\nFine-tuning vs. prompting Fine-tuning and prompting are two ways to solve tasks with LLMs. Fine-tuning adapts an LLM with a dataset by updating the model’s parameters. On the other hand, prompting refers to a user inputting specific instructions or text as prompts into a generalized (not fine-tuned) LLM to guide the model\u0026rsquo;s response. One-shot and few-shot prompting are examples of prompting techniques. In the image below, we can see how zero-shot, one-shot, and few-shot prompting compare to fine-tuning. Note that the prompts for the prompting examples are longer than the fine-tuning prompt.\nImage from GPT-3 paper [4]\nBoth fine-tuning and prompting are valuable techniques for using LLMs. Depending on the specific use case, one method or the other may be better. In general, fine-tuning is better for complex tasks where the user has a labeled dataset and may be cheaper in the long run due to cheaper inference costs, depending on the LLM provider.\nFor simple tasks, prompting has advantages over fine-tuning. First, prompting is faster to iterate on since you do not need to train a new model every time you update the prompt or change your dataset. Second, fine-tuning requires a labeled dataset, while prompting does not. Therefore, if you do not have training data or only have a few examples, prompting could be better. In general, it makes sense to start with prompting and seeing if it can solve your task before trying fine-tuning.\nFine tuning is better for complex tasks where the model’s generated output must be accurate and trusted. In the GPT-3 paper, “Language Models are Few-Shot Learners” by Brown et al. [4], the authors compare few shot prompting on GPT-3 to fine-tuned models for many different tasks. Often, GPT-3 cannot outperform the fine-tuned models, especially on complex tasks. Each task is measured with performance measures such as accuracy or BLEU score (an NLP metric). One task where few-shot GPT-3 greatly underperforms fine-tuned models is natural language inference (NLI) tasks. In this task, there are two sentences and the model has to predict if the second sentence logically follows from the first. The non-fine-tuned model likely does not perform well since this task is difficult and requires an understanding of logic. Intuitively, fine-tuning outperforms prompting for complex tasks since one can train on an unlimited number of domain specific data points.\nImage from GPT-3 paper [4]\nFurthermore, inference is significantly cheaper with fine-tuned models when compared to prompted models due to the reduction in the amount of instruction required during prediction. Since fine-tuning allows developers to incorporate task-specific knowledge directly into the model\u0026rsquo;s parameters,, fine-tuned models can generate accurate responses with minimal need for explicit instructions or prompts during inference. On the other hand, prompted models heavily rely on explicit prompts or instructions for each prediction, which can be computationally expensive and resource-intensive, especially for large-scale applications.\nSteps to fine-tune a model Fine-tuning an LLM involves many steps including curating your data and picking the best architecture. Here are the general steps to fine-tune a LLM model:\nDefine task and dataset Select LLM architecture Update model weights Select hyperparameters Evaluate model Deploy model OpenAI lets you fine-tune their GPT-3 models on your custom data (in the future, they will add support for GPT-3.5 and GPT-4). We built a free platform to easily guide you through the steps above to fine-tune OpenAI LLMs here: https://terra-cotta.ai/. With the platform, you can also compare fine-tuned models to prompted models.\nIn conclusion, fine-tuning is a powerful technique that allows developers to leverage the knowledge and capabilities of pretrained language models while adapting them to specific real-world tasks, leading to improved accuracy and cost performance for a wide range of natural language processing applications.\nCitations https://arxiv.org/pdf/2305.14552v1.pdf https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf https://arxiv.org/pdf/1810.04805.pdf https://arxiv.org/pdf/2005.14165.pdf ","permalink":"https://lucaspauker.com/articles/llms-unleashed-the-power-of-fine-tuning/","summary":"Disclaimer: This article mentions https://terra-cotta.ai/, an LLM experimentation platform I am building\nIntroduction ChatGPT, Bard, and other large language models (LLMs) are very useful for a wide variety of tasks from writing code to answering complex questions to aiding with education. However, these models are ultimately limited by the data that they are trained on. Also, these models are trained to be able to answer a wide variety of questions which may not be sufficient for domain-specific questions.","title":"LLMs Unleashed: The Power of Fine-Tuning"},{"content":"Advancements in artificial intelligence and language models have made significant impacts in various fields from healthcare to finance to entertainment. Here are 50 practical applications of AI that are currently in use or have the potential to be implemented in various industries. Let me know if any of these ideas inspire you or if you build any of them!\nText Analysis Automatically generate outlines or summaries of news articles. Find fake news and provide a citation with the real source. Determine the political affiliation and bias of a news article. Generate better captions/subtitles for shows translated from foreign languages. Generate lyrics for songs based on a topic and song structure. Generate summary of court opinions or court transcripts to understand legal proceedings better. Annotate legal documents for specific clauses (https://github.com/lucaspauker/extract-contract). Find similar academic papers to an input prompt. Generate lyric annotations (like Genius) automatically by using information on the internet. Better Google search suggestions with language models. Generate personalized poems for someone. AI Assistance Generate code from comments or pseudocode. Translate code from one coding language to another. Generate automatic email reply suggestions based on your writing style. Generate a cover letter based on your resume and the company website you are applying for. Suggest edits to a paper like Grammarly. Automatically generate simple math questions (for example arithmetic word problems). Automatically grade essays or automatically grade the ACT/SAT writing section. Suggest next workout based on previous workouts and goals. For doctors, given a patient, find similar patients and summarize their diagnoses in order to help diagnose the patient. Generate slogan based on product description. Generate Latex notes from a transcript of a lecture. Generate presentation slides based on a paper. Write an abstract for an academic paper given the rest of the paper. Do a command on a browser given text input for example: “buy a plane ticket on United from SFO to ORD.” Autocomplete sentences in Google docs using AI. Travel and Leisure Generate travel itinerary based on user preferences. Generate meal plans based on caloric goals and food preferences. Suggest outfits to wear based on a user’s fashion preferences. Suggest meals based on ingredients someone has in their kitchen. Generate sports game commentary based on context of what’s happening in the game. Generate a text-based game with AI that has infinite possibilities. Automatically generate responses for video game characters using AI. Image Grade a student’s work with a picture of the student’s assignment and a picture of the answer key. Improve accessibility on the web by automatically generating captions for images. Convert handwritten text from an image to digital text. Diagnose medical conditions by analyzing medical images. Generate images for advertising campaigns based on existing images. Create design assets in Figma based on text ideas and pictures. Count the number of cars or buildings in satellite images. Video Generate a transcript by lip-reading a video. Create annotations for key events in a video. Detect and label important objects in video footage. Generate a Tinder profile based on someone’s Instagram profile. Increase the frame rate of animation by generating extra frames. Audio Generate lyrics from audio. Given audio and the lyrics, output the timestamp of each lyric. Transcribe speech from audio. Generate sheet music or MIDI from audio. Critique a song by analyzing its musical elements and style given audio. ","permalink":"https://lucaspauker.com/articles/50-practical-applications-of-artificial-intelligence-and-language-models/","summary":"Advancements in artificial intelligence and language models have made significant impacts in various fields from healthcare to finance to entertainment. Here are 50 practical applications of AI that are currently in use or have the potential to be implemented in various industries. Let me know if any of these ideas inspire you or if you build any of them!\nText Analysis Automatically generate outlines or summaries of news articles. Find fake news and provide a citation with the real source.","title":"50 AI Applications"},{"content":"Below are reflections of our startup journey participating in the Floodgate Reactor Program. We have strong opinions, loosely held and while these opinions reflect our current state of mind and understanding of the world might change as we continue our entrepreneurial journeys.\nIntroduction H-2A is a US government program that allows farmers to bring foreign seasonal agricultural labor to the US. To learn more about the H-2A program, check out our other blog posts or the link here.\nThe program brings in hundreds of thousands of workers each year, and it allows farmers to get experienced foreign labor and farm workers to get paid more than they would in their native country. However, the program is far from perfect and there are many problems with the program for both farms and farmworkers.\nProblems for Farmers For farmers, the H-2A program is difficult to use and contains many hurdles to get foreign labor. Some of these problems include the paperwork, recruiting workers, providing housing and transportation, and competing with farms that use illegal labor.\nCompleting the paperwork for hiring H-2A workers involves communicating with four government agencies and the whole process takes months. This complex process requires most farms to use a lawyer to help them with the paperwork, or use a preparer, which is a company that does the paperwork for you and typically helps with recruiting as well. Most attorneys and preparers also charge the same price every year to do the paperwork, even though most of the forms are the same. We believe that using technology to automate the paperwork and fill out forms year-over-year could greatly reduce the complexity of H-2A paperwork.\nAnother problem for farmers is recruiting foreign workers. Although there are many workers who are interested in coming to the US to work, it is difficult to contact them since they are in a foreign country and often don’t speak English. Furthermore, farmers want workers that have worked on farms before and ideally are familiar with the crop that they grow. Usually, farms will use recruiters to find workers. However, some recruiters are involved in shady practices and from conversations we have had with farmers, the workers the recruiters recommend are not always good. One farmer we talked to had to go to rural Mexico with her husband to find workers. She went into a grocery store, asked to use the P.A. system, then announced that she was interested in hiring people to work in the US. She then manually wrote down the names of people that came up to her, and ended up hiring them for her farm. This story illustrates the difficulties of hiring workers and the lengths that farmers will go to in order to find the right workers.\nFarmers also have to spend lots of time and money to get housing and provide transportation for H-2A workers. A requirement for the H-2A program is that the farmer needs to provide housing for the workers as well as transportation to the farm in the US. Providing adequate housing is a difficult task, and was consistently one of the most difficult aspects of the program from conversations we had with farmers. In addition to building housing being expensive, there are often challenges with state and county codes that make building houses legally very difficult.\nLastly, a challenge for farmers that use the H-2A program is competing with farms that hire illegal immigrants. Undocumented immigrants are estimated to make up 50% of the farmworkers on US farms (1). Because of the costs associated with the H-2A program, including high wages and providing free housing and transportation, using the H-2A program is more expensive than hiring undocumented workers. Therefore, farmers that use legal immigration lose a competitive edge compared to farmers that hire undocumented workers. By making the H-2A program easier and cheaper, as well as making paths for undocumented workers to obtain legal status, farmers will likely increasingly use legal labor.\nProblems for Farmworkers For farmworkers, the H-2A program is far from perfect and they face a variety of challenges, including dealing with sketchy recruiters, having poor working and housing conditions, and the difficulty of changing employers.\nMost H-2A workers find jobs in the US through a recruiter in Mexico or the US. The recruiters connect labor to farms that are looking to hire workers. Due to the dependence of farmworkers on recruiters and the eagerness of foreign workers to find jobs in the US, there is a host of exploitative practices that recruiters engage in. Some recruiters are fraudulent, advertising jobs that don’t exist. Oftentimes, migrant workers will pay hundreds or thousands of dollars to recruiters for fictitious jobs (2). There is not an effective system to prevent this, so it happens regularly. Furthermore, the rules of the H-2A program clearly state that recruiters are not allowed to charge migrant workers any fees. However, according to a study done by Centro de los Derechos del Migrante (CDM), over 25% of workers pay recruitment fees as high as $$$4,500 (3). Workers often have to take out loans to pay these fees, resulting in them being less free to leave an abusive environment in the US. Lastly, recruiters oftentimes engage in discriminatory practices. Recruiters favor men over women and rarely hire older adults (2). This is illegal according to US law, however since many recruiters operate outside of the US, it is a legal gray area.\nFarmworkers also are often subject to poor working and housing conditions. According to a study done by CDM, 45% of H-2A workers live in overcrowded or unsanitary housing, 35% do not have the necessary safety equipment, and 27% did not receive adequate training (3). From the Department of Labor Wage and Hour Division compliance data, there are over 130,000 recorded H-2A violations against farmers for not following the rules (4). Due to these violations, farmers have paid over 35 million dollars in back wages to employees and over 44 million in civil monetary penalties. To search this data in a user-friendly way, you can look at www.h2avision.org. Furthermore, sexual harrassment and verbal abuse are common for H-2A workers (3).\nLastly, it is difficult for H-2A workers to change employers. The H-2A visa is tied to a single employer, which leaves them vulnerable for abuse and unwilling to challenge unfair practices (5). H-2A workers are working in a foreign country and often don’t speak English. By making it easier to change employers if the farmworker is not happy with their current employer, it would reduce abuse and malpractice.\nConclusion Overall, this blog post has detailed some of the problems with the H-2A program from the perspective of the farmer as well as the worker. We believe that more transparency and resources would help.\nSources https://www.fwd.us/news/immigrant-farmworkers-and-americas-food-production-5-things-to-know/ https://uknowledge.uky.edu/cgi/viewcontent.cgi?article=1148\u0026amp;context=kjeanrl https://cdmigrante.org/wp-content/uploads/2020/04/Ripe-for-Reform.pdf https://enforcedata.dol.gov/views/data_summary.php https://www.farmworkerjustice.org/wp-content/uploads/2012/05/7.2.a.6-No-Way-To-Treat-A-Guest-H-2A-Report.pdf ","permalink":"https://lucaspauker.com/articles/challenges-facing-h-2a-farmers-and-workers/","summary":"Below are reflections of our startup journey participating in the Floodgate Reactor Program. We have strong opinions, loosely held and while these opinions reflect our current state of mind and understanding of the world might change as we continue our entrepreneurial journeys.\nIntroduction H-2A is a US government program that allows farmers to bring foreign seasonal agricultural labor to the US. To learn more about the H-2A program, check out our other blog posts or the link here.","title":"Challenges facing H-2A Farmers and Workers"},{"content":"At the beginning of 2022, we began the Floodgate Reactor program with the hopes of finding a successful startup idea and learning how to build a successful company. We started the program with an idea in cybersecurity related to vulnerability patch management. By the end of the summer, we were working on an idea that focused on solving farmer labor shortages and building a global talent marketplace. In this blog post, we will discuss our journey and how our ideas changed over time.\nPicture of us driving through Salinas, California.\nDay 1: First Day — Pivot! Coming into the Floodgate Reactor program, we were working on building a better way for companies to patch security vulnerabilities in their systems. We were interested in this idea since we both had experience in cybersecurity and knew that patching vulnerabilities was a huge problem. By this time, we had a pilot partner that gave us access to their patching ticket data. We used this data to prove that we could help automate some of the security patching process, however the pilot partner told us that this issue was not top-of-mind, and in fact they preferred the manual data entry for liability reasons. After hearing this, we saw that our main hypothesis (that people would want automation) might be wrong. Furthermore, we were worried about the market size of this industry and the competition that already does patch automation.\nBecause of these reasons, we decided to go back to the drawing board. We made a list of every idea we thought was interesting and after only a day of brainstorming, the list had over 50 ideas. Some of these ideas included:\nWeb3 smart contract patching Financial credit coaching for young people Shit tech: connecting people to a network of bathrooms We had lots of ideas in many different industries…\nDay 7: Narrowing Down Ideas After coming up with a list of ideas, we needed to find the best ones. In order to narrow down the list, we made criteria to evaluate the ideas and scored each idea the criteria. Our criteria were:\nUsefulness + meaningfulness Technical feasibility Market size Experience/connections Potential to be a profitable business These aren’t necessarily the right or the best criteria to evaluate the ideas, but they gave us a framework to think about the ideas in comparison to one another.\nAfter this process, we had a list of 10 or so ideas. We researched these ideas in depth and tried to poke holes in them. After doing this, there were three ideas that stood out to us over the others. These ideas were:\nInvestment visas (EB-5 program) Labor market for retail and fast food workers Smart contract understanding Now, we successfully narrowed down 50 ideas to 3. We decided to dive deeply into the investment visa idea first and be ready to pivot to the other ideas if we hit a wall. Our idea at this point was related to the EB-5 program, which is a US visa program that allows foreign investors to invest around $$$1M in a US business in exchange for a green card. We saw that there were many players in the space and that the process was complicated. Therefore, we thought that we could use technology to streamline the process.\nDay 14: Calling, Calling, Calling Now that we had a space and problem to focus on, we had to learn more. We decided we needed to contact people that work in the immigration space. We made a list of different stakeholders in the EB-5 program and gathered emails and cell phone numbers for immigration lawyers, EB-5 visa recipients, and the real estate investments that people could invest in via EB-5. We spent time going through our list of leads by emailing, setting up meetings, and cold calling. We took notes for each conversation and learned a lot by talking to different players in the industry.\nAfter some of these conversations, we learned that the EB-5 program functions pretty well already and that the market size of the EB-5 program is small for a venture-backable business. We then looked at other visa categories and realized that focusing only on the EB-5 visa was too narrow-minded. The visa market as a whole was huge, and we thought that there could be interesting opportunities in other visa categories.\nEventually, we decided to explore every US visa in-depth, from student visas to farmworker visas. We did research on each visa and talked to as many people as we could to learn quickly. After exploring all the different kinds of visas, we found one visa category that was the most appealing due to the number of visas issued every year: H-2A visas. These visas are temporary (seasonal) work visas, where workers tend to stay in the US for 9 months or less each year.\nWe found that we connected to this problem space on a deep level. Lucas’ father and grandparents immigrated to the US from Colombia, and through hard work his dad became successful in engineering and finance. Shreyas grew up on a farm and his parents came to the US through an immigration program. We have both seen the power of the American dream and want to help as many people achieve this dream as possible.\nDay 21: Emailing Fail and Lessons For the H-2A program, the Department of Labor (DOL) releases a list of every company that uses these visas, along with the emails and phone numbers of the point person for each company and the attorney that helped them file the visa. To us, this was a gold mine since we could set up many conversations with companies that use these programs and find what the biggest pain points are.\nBefore we sent out cold emails, we decided to build a website to establish credibility in the space and encourage people to talk with us. We built a website that provided data analysis for the DOL H-2B data: www.visadetective.com. Users could interact with the visualizations and use them to gain insights on the biggest employers and attorneys in the space.\nAfter building this tool, we set out to start a mass email campaign to attorneys that process H-2 visas and companies that use the H-2 program. We used snov.io to run the campaign from our @visadetective.com emails. Overall, we sent out thousands of emails…and almost all of them went to spam. We learned that you need to warm up your inbox with a service like Woodpecker or else your emails will get flagged as spam. Also, it is better to use your .edu email, since it will likely not get flagged as spam.\nAfter this experience, we came to the conclusion that cold email campaigns may not be the best way to get leads since they are impersonal and don’t look real, even if they don’t get sent to spam. We found much better success with direct emailing with personalized emails and using LinkedIn to connect to people in the industry.\nDay 28: Unit Tests By this point, we had spoken to many people in the H-2A industry, including attorneys, farmers, worker rights groups, and other companies trying to simplify the process. We had conviction that the process was broken and we could improve it. So the next step was to start planning how to begin to build and test out our hypotheses. Our main thesis was that we could build a tech-enabled marketplace solution to connect employers to short-term, seasonal employees. We decided to build “unit tests” that would each serve to validate a hypothesis that was vital for proving our main thesis. Here are the unit tests we came up with:\nCan we source a large number of employers who are currently utilizing the H-2A program and convince them to switch to our method? Can we source a large number of employers who are NOT currently utilizing the H-2A program and convince them to use it? Can we find cheaper housing solutions for H-2A farmers? Can we find and collaborate with an established immigration attorney to handle the legal aspects of H-2A visa sponsorship for cheaper at scale? Can we successfully recruit a large number of prospective foreign candidates? Can we successfully vet candidates that have relevant experience, dependably, and reliably entirely remotely? Can we improve/automate the existing workflows of an immigration attorney or firm to reduce the cost structure? We ordered them in terms of how important they were for our underlying hypothesis to be true. We then started to work on the first unit test.\nDay 35: Visiting Farms in Salinas As part of our outreach and quest for knowledge, we met a Stanford researcher who focuses on the impact of a previous immigration program known as the Bracero Program. He suggested that we visit the Salinas valley, where he would put us in contact with farmers, business people, and politicians in the area.\nWe were able to set up a trip and ended up meeting many awesome people in Salinas. We found that everybody was kind and willing to help–one person gifted us a book he was reading only minutes after we met him! We talked to large farmers, small farmers, politicians, and people representing many farmers. We learned more intimately how the farming industry works. Overall, the trip gave us confidence that some of our ideas could have a huge impact.\nFarm in Salinas, California.\nDay 42: Wrapping Up After going through the idea maze and changing our idea from cybersecurity to farming visas, we felt that we had successfully found an interesting and important problem to solve. Due to personal commitments, we decided to not pursue the idea after the Floodgate program ended. However, we are considering working on aspects of it in the coming months. We also used our knowledge of the space to build a tool here: www.h2avision.org that helps track violations of the H-2A program by farms.\nWe hope that this journey has been useful for anyone thinking of starting a company and who is still in the idea stage.\n","permalink":"https://lucaspauker.com/articles/the-process-of-narrowing-down-an-idea-and-navigating-the-idea-maze/","summary":"At the beginning of 2022, we began the Floodgate Reactor program with the hopes of finding a successful startup idea and learning how to build a successful company. We started the program with an idea in cybersecurity related to vulnerability patch management. By the end of the summer, we were working on an idea that focused on solving farmer labor shortages and building a global talent marketplace. In this blog post, we will discuss our journey and how our ideas changed over time.","title":"Navigating the Idea Maze"},{"content":"Below are reflections of our startup journey participating in the Floodgate Reactor Program. We have strong opinions, loosely held and while these opinions reflect our current state of mind and understanding of the world might change as we continue our entrepreneurial journeys.\nIntroduction H-2A is a US government program that allows farmers to bring foreign seasonal agricultural labor to the US. To learn more about the H-2A program, check out our other blog posts or the link here. In this blog post, we will describe some ideas about what should be improved in the H-2A process.\nAutomated Documents Filing the necessary documents to apply for the H-2A program is tedious and expensive. Completing the paperwork for hiring H-2A workers involves communicating with four government agencies. For each application, the farm must file a job order with the state workforce agency, file the temporary employment certification with the DOL, oversee that the domestic recruiting is done correctly, and verify that the housing for employees is up to regulations. Because of how complicated this is, most people use attorneys or preparers to help them fill out the documents. However, these third-party services are expensive and typically cost around $$$10k.\nBy leveraging document automation software, we believe that it is possible to fill out the forms automatically, then have an attorney review the key sections only. Overall, this would mean that the attorney would have to do less work and therefore would mean a cheaper cost for doing paperwork.\nFurthermore, we have seen that the H-2A application is very similar year over year. However, most attorneys and preparers charge the same fee each year. We believe that by using information from past year’s applications, the process of generating new paperwork for an H-2A application is much easier and should therefore cost less.\nTalent Marketplace As we wrote about in other blog posts, there are many problems with the program that impact both farmers and workers. We believe that more transparency into the hiring process could improve the quality of labor and reduce abuses. When farmers hire H-2A workers, they want people who have worked on farms before and ideally are familiar with the specific crop they are growing. A common misconception about H-2A is that it is “unskilled” labor. In fact, there are many nuances to picking different kinds of fruits and vegetables. Furthermore, we believe that foreign workers should have more say into which farm they work on. They should be able to access data about past abuses of the H-2A program for each farm and be able to filter by pay.\nIn the future, we envision a talent marketplace where farms can find the best workers and workers can find the best farms. We believe a marketplace is a good solution since this is a two-sided issue: farms want the best workers and workers want to work in the best farm. The current way that recruiting is done offers little transparency and choice into which candidates the farmers will get and which farm workers will be working at.\nFor farmers, some things we think are important to know about workers before hiring them are their experience, which languages they speak, and how long they want to work in the US. For workers, we believe it is important to know which farms have a history of abuse and malpractice and which farms pay the most. A marketplace solution could improve the hiring process for everyone involved.\nTechnology-Enabled Recruiting The current recruiting process for H-2A is fractured and fraudulent. For more details on the fraudulent aspects of H-2A recruiting, check out our “Challenges” blog post. Currently, there are many recruiting entities and some advertise jobs that may not exist, or illegally charge foreign workers money. We believe that recruiting can be done better and at a low cost by using technology-enabled solutions.\nWe believe that a recruiting solution that is integrated with existing social media groups on social media platforms like Facebook and chats on WhatsApp would be able to reach many prospective candidates. By using automated intake forms by using the WhatsApp API, recruiting could be done cheaply and efficiently.\nDuring our interviews with employers, it seems like a majority of their workforce are returning year after year. When the demand for labor grows, traditionally the labor crew has a “pseudo-leader” who is able to find additional workers who are usually relatives of members of the crew or living in the same hometown. We hypothesize that a majority of the recruiting is done organically through social media and then primarily through referrals. We believe that a referral network could exponentially increase the ability to identify prospective candidates. By creating shareable, forwardable, WhatsApp-friendly landing pages with small rewards for referrals, it can democratize the ability for people to recruit their community and family/friends.\nBetter Housing Solutions Through our conversations with farmers, we found that a consistently difficult part of the H-2A program was providing housing for workers. This is a requirement for the program, and finding affordable housing is difficult, especially with housing codes in states like California that sometimes prevent new houses from being built at all.\nWe believe that this is a difficult problem, and the path forward to improving housing availability is not clear. Here are some ideas of ways to make affordable housing more accessible:\nMotels: We believe that using existing housing such as motels could be a fruitful avenue for providing affordable housing for workers. Booking all their rooms for 6 or 9 months at a flat rate may be cheap enough for farmers to pay to use the motels for their temporary workers. Furthermore, the motels would be able to cut costs by not employing as many cleaning staff and hospitality workers since H-2A workers do not need their rooms cleaned and in general do not need as much service. Unused houses on farms: Many farms that do not use H-2A often have extra housing facilities on their farms. These facilities could be rented to nearby farms that do use H-2A, providing extra income to farms that do not use the program and affordable housing for farms that do. Modular housing units: Many farms struggle to quickly build the housing required. Perhaps there are novel ways to navigate building permits by building modular, prefabricated housing units using homes provided by people like: palletshelter.com. Trailer park: Perhaps purchasing a trailer park lot which has the correct permitting and then supplying trailer park homes for the workers could be an affordable housing option. Conclusion In this blog post, we have discussed many avenues forward for improving the current state of the H-2A program. To recap, the ideas are to automate the documentation required to apply for H-2A, building a talent marketplace to connect farmers to labor, using technology to recruit more effectively, and build better housing solutions.\n","permalink":"https://lucaspauker.com/articles/what-should-change-in-h-2a/","summary":"Below are reflections of our startup journey participating in the Floodgate Reactor Program. We have strong opinions, loosely held and while these opinions reflect our current state of mind and understanding of the world might change as we continue our entrepreneurial journeys.\nIntroduction H-2A is a US government program that allows farmers to bring foreign seasonal agricultural labor to the US. To learn more about the H-2A program, check out our other blog posts or the link here.","title":"What Should Change in H-2A?"},{"content":"Introduction Everyone has a framework, a basis by which they make decisions and navigate the uncertainty of the world. Frameworks are developed over time through knowledge and experience. Learning is a process in which we can incorporate new knowledge and experience into our existing framework. Working on a startup opened the floodgates to new knowledge and experience rushing into our frameworks about building a company. In a small amount of time, it allowed us to understand how to rapidly gain new experience and knowledge….BY OURSELVES! After a decade and a half of following a syllabus or expecting grades, we had free range to explore ANYTHING and EVERYTHING. The summer was a unique experience that has rapidly impacted our framework on both the startup and the process of starting up. Here we attempt to communicate some of those lessons that are now in our framework of starting a startup.\nHere are five lessons that we learned:\nGood problems are the start of bad ideas, which are the start to good ones Good or bad….get a reaction! Ruthless honesty and prioritization Disprove your idea first: urgency, market size, feasibility, and then competition People are customers and customers are people Lesson 1: Good problems are the start of bad ideas, which are the start to good ones Before the summer, our thinking of how good ideas came about was that one would encounter an idea and immediately be able to tell whether it is good or bad. Through the process, we saw just how wrong that thinking is. Reducing an idea to good or bad takes out the nuance and complexity of problems in a way that is almost offensive to those affected by it.\nStarting with the idea and not the problem first is the biggest mistake we made at first and the one we often see others make. Problems drive innovation…they give us a reason to come up with an idea, not the other way around.\nTo be able to assess the quality of a problem, it requires potentially significant research and deep thinking about the problem. When we realized the first idea we were working on initially wasn’t working because it wasn’t an important enough problem, we decided to take a step back not just from the idea, but the entire problem space. We spent far too much time working on an idea and not on the problem. People told us this but we were stubborn; we sure as heck could have saved more time if we listened to this earlier. But, given how common and easy it is to fall into the trap, we are glad we experienced it ourselves. In the wise words of George W. Bush: “There’s an old saying in Tennessee-I know it’s in Texas, probably in Tennessee-that says, fool me once, shame on-shame on you. Fool me — you can’t get fooled again.” Let’s hope we don’t get fooled again with this.\nAfter stepping away from our first idea, we decided to come up with a list of at least 50 problem areas that were exciting to us. No problem was too dumb or outlandish. We got a free license to complain about anything and also channel in complaints we’d heard from friends/family/coworkers and anyone with a problem. Some problem areas were bizarre…like how do you help people deal with their most urgent biological needs when they need it most. As the famous children’s book confirms, it is a universal problem. When we started digging into problems that on the face seemed simple, we discovered complexity and nuance, even in the problem space of bathrooms. We could very quickly iterate on any single problem in our list to an even more refined problem statement or compelling adjacent problem. Although we ended up honing our list from 50 to 10 to 3 front-runners of problems to work on, we realized that we could have picked almost any problem and turned it into a bad idea with a little work. Although we would still be far from a good idea, a bad idea is the starting point. Instead of struggling to get “good ideas” on paper, coming up with problems and then bad ideas for those problems was actually quite easy.\nLesson 2: Good or bad….get a reaction! As we wrote in our journey blog, the first idea we worked on was better processes for patching vulnerabilities to increase cybersecurity. This idea SOUNDED like a good idea to everyone, but wasn’t a RADICAL idea to a single person. The problem wasn’t “hair-on-fire,” but the solution seemed logical. We realized that our bar for what excitement around a problem or solution looked like from a stakeholder was far too low. Getting a mid-way response SEEMED like a good sign, something like: “looks great, keep in touch” or “you guys might be onto something”, but it turns out that is the worst response. It’s even worse than getting shut down because it gives you a glimmer of hope rather than a direct answer that could save you from dozens of hours of work. Furthermore, if someone is strongly OPPOSED to the idea, your idea may be unique and something that hasn’t been done before. So a strong reaction is better than a weak one.\nAfter realizing that we couldn’t build a company or thesis from mid-way responses, instead of worrying whether the response was negative or positive, we tried to evoke STRONG reactions. Strong reactions guided us effectively. It wouldn’t be uncommon that we would start talking to a stakeholder on the phone and they’d just unleash a 10 minute monologue about the difficulties of the problem. The passion, energy and frustration from them was a clear sign that the problem mattered a lot to them and these strong reactions helped shape a solution.\nAs one of the guest speakers, Steve Blank said to our cohort: “If at the end of your meeting with a stakeholder, if they aren’t refusing to let you leave until you give them your solution, it’s not a big enough problem or a good enough solution to that problem.” We first thought of that as an unrealistic bar, but as we started to ramp up, we really started to feel that…where stakeholders wouldn’t let us leave the table until we had decided on next steps.\nLesson 3: Ruthless honesty and prioritization The easiest person to lie to is yourself. When you are dedicated to making something work, you want to fall in love with each idea and truly believe that it is the answer to all the world’s problems….but that’s unrealistic. We have learned through the process, just how important ruthless honesty and prioritization is.\nOn the honesty front, it’s about being inflexible when you decide what your true North Star is for that sprint. For example, we would set a goal of what we wanted to learn by the end of the week and how we would go about learning that. Each week, we would assess not only whether we achieved that goal, but if that was the right goal in the first place! By learning to accept that we were initially wrong, we were able to get better at setting North Star questions to drive our inquiries. It was extremely helpful to be in a cohort and have weekly meetings with our investors…the cohort and investors kept us honest and in check. It’s easy to believe your version of events, but it’s hard to convince smart people the version of events that always paints you in the best light. Having this sounding board is key.\nOn the prioritization front, we had to quickly triage all the tasks of the business at all times. Given that our team was only two of us, we realized that there was no way we could do EVERYTHING at every single moment. We decided not to build a website for a long time, not because we didn’t think it was worthwhile, but because it wasn’t the most pressing thing. Our North Star goal was to talk to as many stakeholders as possible and if we were able to get high conversion on our outbound meeting request, then we didn’t need a website yet. Also, since we weren’t exactly sure of what the problem we were trying to solve was, then making a website to only have to remake it later would be a waste of time…something we learned during our first iteration of the company when we were working on a totally different space.\nLesson 4: Disprove your idea first: urgency, market size, feasibility, and then competition As we got better at being ruthlessly honest and prioritizing, we started to shift from trying to validate to trying to invalidate. We decided to adopt the mindset that our goal was not to find what worked, but all the things that didn’t work. The absence of finding reasons why it didn’t work would be the starting point to discovering reasons that it might. Once we isolated the reasons it wouldn’t work, it was clear that there were some obstacles that might be difficult, but not impossible. We just had to decide whether or not they were worth doing.\nWe adopted a pretty simple process to check how good an idea was. We would assess the urgency of the problem, the market size (or the scale of the reward), how technologically/operationally feasible it was, and whether other people were willing to overcome the same obstacle and their success in solving the problem. If the problem was urgent, had a large market, had some degree of feasibility, and not enough people working on solving it, it was an idea worth pursuing. Each step of the way was trying to identify why the problem WASN’T urgent, why the market was SMALL, why it was technologically/operationally intensive, or there were far too good competitors. When we started this process, we were able to easily whittle down all our list of ideas to a couple based purely on a few of the criteria.\nUrgency: Assessing urgency is where we had to be the most honest; we needed someone DEMANDING a better solution for it to truly be urgent. Anything less was not enough. When we spoke to business owners about their staffing issues, they lamented that if they didn’t get their workers, they’d have to shut down. The problem we were tackling was an existential risk that easily cleared the urgency criterion. Market Size: The market size criterion was interesting because it helped us define what services we would provide to those with the problem and more importantly, what services we wouldn’t provide. By crafting back of the envelope assumptions and numbers with a little bit of research, we could estimate just how much money there was to be made by doing the services we outlined. Instead of relying on some market research number or making a WAG (wild a** guess), we came up with our own numbers. Throughout the summer, we shifted our understanding of the importance of market size from it being about a number to it being a directional indicator of “how worth” the problem was….it gave a sense of just how worth the problem was tackling and how important it was to customers. Prior to the summer, market size numbers were nonsensical to us…they seemed like just a random pie in the sky number, especially after we had seen pitch decks where someone would be tackling the “pet sneaker market” and claim that it was $8.5B. We realized that by conducting the market sizing analysis in a diligent way, it was actually one of the most useful parts of invalidating or validating an idea. Feasibility: For thinking about the feasibility of an idea, we thought about three different factors: technical feasibility, operational feasibility, and financial feasibility. The technical feasibility refers to how easy the idea is to build out, and how much we can leverage existing technology. The operational feasibility refers to how difficult it would be to execute the idea. One example of this is that to help with certain visas for immigration, you need to apply through the government, which can take up to a year. We quickly ruled these visas out. Lastly, financial feasibility refers to how much it would cost. Building software is cheaper than building houses, and we had limited cash. Competition: When it came to the competition criterion, we realized having competition isn’t bad…this should NOT be the only reason you invalidate a problem space. Instead, current competition serves as an indicator of where the industry is headed. Furthermore, the market must be big enough to support multiple businesses. Thinking about competition and how we approach a crowded market is something we didn’t really spend too much time on since our idea was in a niche market. Even in our small niche, we could identify at least 30 possible competitors to some degree who operate at semi-scale. Nobody had built a billion dollar business yet which was both a good sign and a bad one. Good in that perhaps there was a lot of opportunity, bad because perhaps people had tried and it’s not there. In our case, there was only one venture backed company in the space and they were still at the Series A…there is still a lot of room for a differentiated view to catch up…at least we think so? Lesson 5: People are customers and customers are people he most fun part of the startup process was meeting people. People from all walks of life who all had amazingly interesting stories and lives filled with things we could have never experienced if not through them. We heard stories of multi-generational businesses, Americans living paycheck to paycheck, stories of immigrants hailing from across the world, and so many more. The humans we were able to connect with made it so easy to forget that we were working on a startup. And perhaps, sometimes that was a good thing.\nIt is easy to think of an amorphous entity called a “customer” who would be willing to pay us money for our product or service. Through the summer, however, we realized that if we truly wanted to have a successful product or service to someone’s problems, we needed to know THEM. They are the people we should be thinking about at all times. We stopped thinking of people as customers and started seeing them as people, and listened and understood their stories.\nWe stopped calling these “customers” by the name of their business, but more so on a first name basis. We learned about their lives, what excited them, what scared them about the future, and why they did what they did. We heard inspirational stories of a 3rd generation farmer who wasn’t sure whether he’d see the next harvest, an 80-year old woman continuing to run legal processes for her family’s farm, and an aspiring immigrant who secretly studied American culture so he’d feel like he belonged when he got here. But, similarly, we heard stories that struck fear. Stories of abusive labor contractors who took advantage of workers who didn’t know the rules, stories of scam artists stealing hundreds of thousands of dollars from immigrants, and just how messed up the policies around foreign labor were. We are grateful to all those who we learned from…their stories have forever changed our lives. Startups might seem like they are about technology, big money, and front covers of magazines, but in reality it is about the people.\nConclusion This only scratches the surface of what we learned. We are grateful for the people who have helped us learn these lessons, from our investors to our mentors to our customers to our teachers to our accelerator cohort…and most importantly grateful to each other for being partners on this continual learning journey.\n","permalink":"https://lucaspauker.com/articles/lessons-from-floodgate-s-summer-accelerator/","summary":"Introduction Everyone has a framework, a basis by which they make decisions and navigate the uncertainty of the world. Frameworks are developed over time through knowledge and experience. Learning is a process in which we can incorporate new knowledge and experience into our existing framework. Working on a startup opened the floodgates to new knowledge and experience rushing into our frameworks about building a company. In a small amount of time, it allowed us to understand how to rapidly gain new experience and knowledge….","title":"What We Learned from Floodgate's Summer Accelerator"},{"content":"In this blog post, we will explain why H-2A is a crucial program that supports the livelihood of countless farmers and workers.\nH-2A is a US government program that allows farmers to bring foreign seasonal agricultural labor to the US. In 2021, over 300,000 workers came to the US under this program and over 16,000 farms across the country used the program. The H-2A visa lets workers stay in the US for one year, but this can be extended for up to three years. However, the program is designed for workers to not live in the US permanently; they must stay in their home country for at least three months every three years they are on the visa. Overall, the H-2A program is essential for farms to have the labor they need to stay in business, and it helps foreign workers make more money than they can in their home country.\nUsage of the H-2A program has increased drastically over the last couple of decades. Over the past decade alone, the number of farms using H-2A has doubled and the number of H-2A workers approved has tripled. The reasons for this are varied, but includes the fact that US domestic workers are not interested in farm work, despite rising wages (1). Many farms that we spoke to indicated that they could not find domestic workers, which is why they turned to H-2A labor.\nThe H-2A program is essential for farms to operate. Around 30% of hired farmworkers are employed using the H-2A program. During COVID, border restrictions and delays in visa processing caused H-2A workers to not arrive in the US on time. This meant that many farms struggled to find labor to operate their farm and harvest the crop (3,4).\nThe H-2A program is also important for the livelihoods of many foreign workers. Most foreign workers willingly return year over year, indicating that the H-2A program is attractive. The main driver for interest in working in the US is the high wages offered to temporary workers. The average annualized wage for H-2A workers was $25,000 in 2019, while the Mexico annualized minimum wage for farmworkers was less than $1,200 (1). Therefore, working in the US represents a 20 time increase in wages. Furthermore, the H-2A program requires housing and food to be provided to workers.\nThe H-2A program provides a legal path for workers to come into the US to work, and therefore reduces illegal immigration. From the perspective of farmers, having legal labor is more desirable than illegal labor since it reduces the risk of fines. From the perspective of migrant workers, H-2A is more desirable than illegal immigration since it guarantees higher payment and workers rights and reduces the possibility of deportation. According to the Cato Institute, there is a strong relationship between increase in Mexican H-2A workers and a reduction in Mexican illegal immigrants at the border (5).\nOverall, we can see that the H-2A program is important for farmers to have labor to operate their farms and migrant workers who want to get paid more to support themselves and their families. Furthermore, the program encourages legal migration. The H-2A program is not perfect, however. There are many problems, including worker abuse, processing times, and regulations. However, the program is still vital to many parties despite these problems.\nSources https://www.cato.org/publications/immigration-research-policy-brief/h-2a-visas-agriculture-complex-process-farmers-hire https://www.ncdemography.org/2020/04/16/counting-farmworkers-in-the-2020-census/ https://www.washingtonpost.com/business/2020/03/18/seasonal-farmworkers-visas-food-supply-chain-coronavirus/ https://www.reuters.com/article/us-health-coronavirus-usa-wheat/u-s-farmers-scramble-for-help-as-covid-19-scuttles-immigrant-workforce-idUSKBN2431BQ https://www.cato.org/blog/h-2-visas-reduced-mexican-illegal-immigration ","permalink":"https://lucaspauker.com/articles/why-is-h-2a-important/","summary":"In this blog post, we will explain why H-2A is a crucial program that supports the livelihood of countless farmers and workers.\nH-2A is a US government program that allows farmers to bring foreign seasonal agricultural labor to the US. In 2021, over 300,000 workers came to the US under this program and over 16,000 farms across the country used the program. The H-2A visa lets workers stay in the US for one year, but this can be extended for up to three years.","title":"Why H-2A is Important"},{"content":"Introduction I recently decided to restore an old Peugeot bike. I wanted to have a nice fixed gear bike to ride and learn more about how bikes work.\nThe bike before restoration.\nIn this blog post, I\u0026rsquo;ll talk about the process for disassembling the bike, repainting the frame, and assembling it again. I\u0026rsquo;ll also talk about some mistakes I made and lessons I learned that could be useful for someone attempting a similar project.\nProcess In order to repaint the frame, I first needed to strip the bike down to the frame. This proved to be a bit challenging since the bike was rusted over and in general was in a rough condition. However, after talking to some people at a local bike shop that were very knowledgable about bikes, I was able to disassemble everything except the cranks. This is because the cranks were attached using cotter pins and I didn\u0026rsquo;t have the correct tools to remove them. The only tools I used to disassemble the bike were some wrenches and a hammer. I also bought a chain tool to remove and reattach the chain.\nAfter stripping the bike, I had to get the frame and fork ready to be painted. Since there was lots of exposed rust and since paint doesn\u0026rsquo;t stick well to smooth metal, I used steel wool to remove the rust from the frame and to scuff the surface. This ended up being a time consuming process and was pretty messy due to steel wool and rust getting everywhere. After using the steel wool on the frame and fork, I used masking tape to cover up all the areas I didn\u0026rsquo;t want to be painted. I used plastic bags to cover up the pedals and cranks since I didn\u0026rsquo;t remove them as mentioned earlier.\nFrame and fork ready to paint.\nI spray painted the bike outdoors since I didn\u0026rsquo;t have a garage or indoor painting space. I hung the bike to a tree using some fishing line. For painting, I used three coats of primer, three coats of red paint, then two coats of clear paint.\nFrame and fork after primer.\nBike frame after painting.\nAfter painting the bike, I let the paint cure for a couple of weeks. I noticed that it took especially long since I put so much paint on the bike. I then reassembled the bike, which was a much faster process than taking it apart since I knew how everything worked.\nBike mostly assembled.\nLastly, I added grip tape and caps to the handlebars.\nFinal product.\nMaterials Materials for disassembling/assembling the bike:\nWrench Chain tool Hammer Materials for painting the bike:\nSteel wool Fishing line Masking tape Primer Paint Clear coat Other materials:\nHandlebar tape Handlebar caps Lessons Learned I made a lot of \u0026ldquo;rookie\u0026rdquo; mistakes when restoring the bike.\nPainting correctly. I made a couple of mistakes while spray painting. First, I used too much paint overall. As I mentioned in the previous section, I did around 8 layers of primer/paint/clear coat in total. This turned out to be excessive and caused the paint to take a long time to cure. In general, light coats are better and just enough spray paint is best. Second, I waited too long to apply coats. It is recommended to finish all the coats of paint within an hour and if not, wait 48 hours between coats. However, I waited too long between coats and did not get it all done in an hour. This resulted in some of the paint applied in later stages not sticking. Remembering how to assemble. When I took the bike apart, I didn\u0026rsquo;t keep good track of how it all fit together. After taking it apart, I had all the bike pieces scattered in various places around my room. When I went to assemble the bike again, I had trouble remembering how it all fit together. Specifically, the fork was difficult to attach to the frame. I was able to assemble it still, but if I had taken more pictures when taking it apart it would have been easier. Better paint setup. Lastly, my spray paint setup was a bit flawed. When I hung up the bike, I attached the fishing line directly to the frame. Then, when I spray painted the bike, there were two rings that the paint did not get to where the fishing line was attached. If I was going to do it again, I would attach the line to the parts of the bike with masking tape on them. Conclusion In general, more planning and foresight would have helped me avoid the mistakes I mentioned. However, for a first time restoration, I am happy with the final product and I learned a lot about how bikes are built.\n","permalink":"https://lucaspauker.com/articles/lessons-learned-from-restoring-a-bike/","summary":"Introduction I recently decided to restore an old Peugeot bike. I wanted to have a nice fixed gear bike to ride and learn more about how bikes work.\nThe bike before restoration.\nIn this blog post, I\u0026rsquo;ll talk about the process for disassembling the bike, repainting the frame, and assembling it again. I\u0026rsquo;ll also talk about some mistakes I made and lessons I learned that could be useful for someone attempting a similar project.","title":"Restoring a Bike"},{"content":"Introduction I recently read Ed Thorpe\u0026rsquo;s Beat the Dealer, a book about how Thorpe, a mathematician, found a way to gain an edge in blackjack. In the book, Thorpe uses computer simulations to calculate the best blackjack strategy as well as card-counting strategies. Since I took a reinforcement learning class last quarter, I wanted to apply one of the most common algorithms, Q-learning, to to find the best strategy for blackjack.\nBeat the Dealer book cover. Taken from here\nRules In order to find the best way to play blackjack, it is essential to understand all the rules. For my simulations, I used a simplified version of blackjack where the player can only hit or stay. In a game, the player plays against the dealer. The round starts by dealing both the player and the dealer two cards. The player\u0026rsquo;s cards are dealt face up, while the dealer has one face down card and one face up card.\nTaken from here\nWinning the game The goal of the game is for the player\u0026rsquo;s cards to sum up to as close to 21 as possible without going over 21, which is called busting. All face cards have a value of 10 and aces can take a value of either 1 or 11. The player wins if their cards are higher than the dealer\u0026rsquo;s without busting. If the dealer busts and the player doesn\u0026rsquo;t then the player wins. Else, the dealer wins.\nPlayer actions In the simplified version of blackjack I used for this article, after the cards are dealt, the player has a choice of two actions\u0026ndash;they can stay or hit. If the player stays, then their turn is over. If the player hits, then they are dealt another card.\nDealer actions The dealer\u0026rsquo;s actions are deterministic, i.e. the dealer has no agency. The dealer must hit on hands with a value of 16 or less. Furthermore, they hit on hands that have a value of 17 if one of the cards is an ace (and the ace is counted as an 11). Hands with an ace where the ace is counted as an 11 are known as \u0026ldquo;soft\u0026rdquo; hands since if the hand value goes over 21, the ace can turn into a 1. So the dealer must hit on soft 17s.\nOther considerations Another consideration that we must take into account are the number of decks. Usually there are between 1 and 8 decks at a casino. I used 6 decks for my tests.\nState Space and Actions To frame the blackjack game as a reinforcement learning problem, we have to define the set of states and the set of actions.\nStates For blackjack, the only information the player has is what they can see on the table. This includes their hand and the face up dealer card (dealer hole card). Since only the values of the cards matter, we can define the state space as the value of the hand as well as the dealer hole card. However, since aces can have a value of either 1 or 11, we must include whether there is an ace in the hand that can have a value of either 1 or 11 without going over 21. As noted earlier, hands with such an ace are called soft. If the value of a certain hand is high enough, the ace is forced to be a 1 so that the player doesn\u0026rsquo;t bust. Such a hand is considered hard, not soft even though there is an ace in the hand. So our state space is the set of all possible values of hands and dealer face up cards, where the value of a hand is the sum of all the card values as well as whether the hand is soft or hard. We can therefore represent the state of a game as a tuple: (hand sum, is the hand soft, dealer hole card).\nThe numerical value of a hand can take on any value between 4 and 21 (18 possible values). Values of hands 12 or above can be either soft or hard, so there are 28 total possible hand values. The dealer card can take on 10 distinct values. Therefore, our state space includes 28*10=280 possible states.\nPolicies As discussed before, the player has two possible actions: stay and hit. A policy is a map of states to actions. A policy decides what action to take given a state. The goal of reinforcement learning is to find the optimal policy, which is the optimal action for each state. There are therefore \\(2^{280}=1.94*10^{84}\\) possible policies that we could create. Clearly, iterating through all such policies would be impossible with a laptop, so we will instead find better ways to find an optimal policy.\nQ-Learning Since we can model blackjack as a Markov decision process, using reinforcement learning (RL) to learn an optimal policy is natural. I used Q-learning to find the best way to play blackjack. I will describe what Q-learning is in this section.\nQ function The \\(Q\\) function, or the state-action value function is central to Q-learning. \\(Q\\) is defined with respect to a state \\(s\\in\\mathcal{S}\\) and an action \\(a\\in\\mathcal{A}\\), where \\(\\mathcal{S}\\) is the set of all states and \\(\\mathcal{A}\\) is the set of all actions.\n$$Q^\\pi(s,a)$$\nThe function is parameterized by a policy \\(\\pi\\) and represents the expected return if one starts from state \\(s\\), takes action \\(a\\) and then follows policy \\(\\pi\\). Essentially, it represents how good an action is from a state.\nQ update (learning) A core idea that we will use for learning an optimal policy is testing out different actions, then evaluating the actions. This is represented in the following diagram:\nRelationship between world and agent, taken from the CS234 lecture slides (Winter 2021 lecture 4)\nThe \\(Q\\) function is updated as new data comes in. Each data point is in the form of a (state, action, reward, next state) tuple. For a timestep \\(t\\), this is represented as \\((s_t, a_t, r_t, s_{t+1})\\). Note that since we use \\(Q(s_{t+1}, a)\\) in the Q update, we are boostrapping. The Q update is described below.\nQ update: $$Q(s_t,a_t)\\leftarrow Q(s_t,a_t) + \\alpha * (r_t + \\gamma * \\text{arg}\\max_a Q(s_{t+1}, a) - Q(s_t,a_t))$$\n\\(\\alpha\\) is the learning rate, and \\(\\gamma\\) is the discount factor. \\(\\alpha\\) controls how fast the policy will update based on new information. \\(\\gamma\\) controls the weighting of data in the future vs. data now. Both \\(\\alpha\\) and \\(\\gamma\\) are values between 0 and 1. To understand the Q update, we can break it down into simpler components.\nError term: $$r_t + \\gamma * \\text{arg}\\max_a Q(s_{t+1}, a_t) - Q(s_t,a_t)$$\nWe can see that \\(r_t + \\gamma * \\text{arg}\\max_a Q(s_{t+1}, a)\\) is the discounted future reward starting from state $s_t$ and taking action \\(a_t\\). The error term represents how far off the predicted value of the \\(Q\\) function is versus the stored value.\nNow that we understand the \\(Q\\) function and the Q update, we will discuss some properties of Q-learning. These properties will be important for tuning the algorithm and acheiving good results.\nBootstrapping Q-learning uses bootstrapping to estimate the state-value function. Bootstrapping in the context of reinforcement learning means that the model uses estimated \\(Q\\) function values to update the \\(Q\\) function. It is clear that the Q update uses bootstrapping since it uses estimated \\(Q\\) function values when it computes the \\(Q(s_{t+1}, a)\\) for all \\(a\\).\nModel-free and off-policy Q-learning is a model-free RL algorithm, meaning that it does not keep a \u0026ldquo;model\u0026rdquo; of the world. A model of the world is the transition probabilities and rewards between states. Q-learning, instead of keeping track of an estimated model, directly learns the \\(Q\\) function from the data.\nQ-learning is also an off-policy RL algorithm since it estimates the value of the optimal policy while acting on a different policy. This idea is present in this term in the Q update: $$\\text{arg}\\max_a Q(s_{t+1}, a_t).$$ The on-policy update would be to use \\(Q(s_{t+1}, a_{t+1})\\). This on-policy update would estimate the value of the current policy. However, when we add the max term, we have off-policy learning.\nExploration vs. exploitation How will we decide which actions to take in blackjack to best update our \\(Q\\) function? On one hand, we want to maximize the expected return from the Q function, which means we want to exploit the values we have alreadly learned. However, we also want to explore new actions in case we got unlucky in a trial, leading to an incorrect Q value. One way to balance exploration and exploitation is with an epsilon-greedy policy. For some value of \\(\\epsilon\\) between 0 and 1, we take a random action with probability $\\epsilon$ and we take the best action with probability \\(1-\\epsilon\\). Therefore, by adjusting \\(\\epsilon\\), we can control the amount of exploration that our agent does. We typically take \\(\\epsilon\\) to zero after many timesteps.\nModel We will now focus on how to learn the best policy, which is the function that decides which action to take for each state. There are two steps to do model-free policy iteration: policy evaluation and policy improvement. The policy evaluation step consists of computing the Q values for a specific policy. The policy improvement step consists of updating the policy given the new Q values. Here is some pseudo-code for these two steps as used to learn a good blackjack policy:\ndef learn_policy(): t = 0 repeat NUMBER_OF_TIMESTEPS times: simulate 100,000 games of blackjack using the learned policy update Q values based on the (state, action, reward, next state) tuples from the simulations update the policy to be epsilon greedy with respect to the learned Q values t += 1 Simulating the games of blackjack and updating the Q values constitutes the policy evaluation step. The policy improvement step is simply creating the new policy with respect to the Q values.\nTuning the Model Alpha Recall that \\(\\alpha\\) is the learning rate for the model. It controls how fast the Q values will be updated. To ensure that Q-learning converges to the optimal Q values, we must visit all state-action pairs infinitely often, and the step sizes $\\alpha_t$ must satisfy the Robbins-Munro sequence. To satisfy the first condition, we can uniformly pick states and actions to make sure that we visit all state-action pairs infinitely often. For the second condition, we choose \\(\\alpha_t=1/t\\). This satisfies the Robbins-Munro sequence, which is the following two inequalities: $$\\sum_{t=1}^\\infty\\alpha_t=\\infty$$ and $$\\sum_{t=1}^\\infty\\alpha_t^2\u0026lt;\\infty.$$\nEpsilon \\(\\epsilon\\) is the amount of exploration we conduct with the \\(\\epsilon\\)-greedy policy. To ensure that Q-learning converges to the optimal policy, we must visit all state-action pairs infinitely often and ensure that the $\\epsilon$-greedy policy converges to a greedy policy. A common strategy to satisfy these conditions (known as GLIE) is setting \\(\\epsilon_t=1/t\\).\nGamma \\(\\gamma\\) is the discount rate. I set \\(\\gamma\\)=1 since it performed the best in trials.\nResults Now, we will see how well our Q-learning function does compared to the optimal blackjack strategy! I took the optimal strategy from this website. The optimal strategy has a win rate of 0.428 and a loss rate of 0.478.\nBaselines I used two suboptimal baseline strategies to compare against our learned policies: random strategy and stand only strategy. The random strategy has a win rate of 0.282 and a loss rate of 0.676. The stand only strategy has a win rate of 0.382 and a loss rate of 0.566.\nWin rate We will see how the Q-learning strategy compares with the baselines as well as the optimal strategy after many time steps. Here is a plot of the win rate of the Q-learning strategy versus number of time steps. Each time steps is 100,000 games of blackjack.\nWin rate versus number of time steps\nWe can see that the Q-learning strategy reaches the optimal win rate in around 50 time steps. Furthermore, as time goes on, the win rate seems to stabilize, indicating it is converging on the optimal strategy.\nPolicy Here is the optimal policy for the game. S=stand and H=hit.\nHard total optimal policy\nSoft total optimal policy\nHere is the learned policy for the game after 250 iterations. Places where the learned policy deviates from the optimal policy are crossed out.\nHard total learned policy\nSoft total learned policy\nAs we can see, our learned policy is very close to the optimal policy: we have two total errors. We can see from the win rate vs. time steps graph that this small variation in policy does not affect the win rate much. I believe that with more time steps, it is possible to achieve the optimal policy.\nConclusion I hope that this article gave some insight into Q-learning and showed an application for the algorithm. Here is the code I used for my simulation: https://github.com/lucaspauker/blackjack_reinforcement_learning. Feel free to play with it and see if there is a way to improve the algorithm.\n","permalink":"https://lucaspauker.com/articles/reinforcement-learning-applied-to-blackjack/","summary":"Introduction I recently read Ed Thorpe\u0026rsquo;s Beat the Dealer, a book about how Thorpe, a mathematician, found a way to gain an edge in blackjack. In the book, Thorpe uses computer simulations to calculate the best blackjack strategy as well as card-counting strategies. Since I took a reinforcement learning class last quarter, I wanted to apply one of the most common algorithms, Q-learning, to to find the best strategy for blackjack.","title":"Blackjack Reinforcement Learning"},{"content":"Faster than a Supercomputer? In the 1980s, American physicist Richard Feynman proposed the idea of quantum computers to model complex quantum systems. In October 2019, around 40 years later, Google AI and NASA scientists unveiled a quantum computer which ran an experiment in a few minutes that would take the fastest supercomputer 10,000 years. The quantum computer sped up the computation by a factor of 1 billion! This was one of the first major successes in the nascent field of quantum computing.\nTo understand why quantum computers are faster than non-quantum computers, we have to understand how they work. The computers that we use every day encode data such as pictures and text with sequences of bits. Individual bits can be either be off or on, a zero or a one. The equivalent of a bit in a quantum computer is a qubit (quantum bit) that can be in the state zero, one, or a combination of zeros and ones. However, when a qubit is measured, it must be either a zero or one. A qubit is like a coin spinning on a table. A spinning coin isn’t either heads or tails, instead it’s a combination of both, yet when it stops spinning and lies flat on the table, it must be either heads or tails. This idea that a qubit or a coin can be in multiple states (i.e., both heads and tails) at the same time is known as superposition.\nThe property of superposition in qubits allows for fast computations on quantum computers that would take ages for typical computers. One problem that can be solved quickly using quantum computing is factoring numbers with an algorithm called Shor’s algorithm. Shor’s algorithm is significantly faster than any known algorithm not on a quantum computer. Practically, Shor’s algorithm would allow a quantum computer to break encryption schemes that rely on the difficulty of factoring large numbers. Such a cyber-attack, however, would require factoring numbers that are hundreds of digits long. To date, Shor’s algorithm has only been used to factor small numbers such as 15 and 21 on quantum computers, so we do not need to worry about quantum computers hacking our bank accounts!\nNow that we understand some of the basics of quantum computing, let’s go back to the experiment. For the experiment, the Google team built a quantum processor called Sycamore. Sycamore is the most advanced quantum computer to date. It contains 54 qubits, more than other quantum computer, and can therefore run previously undoable experiments. The scientists ran an experiment showing that Sycamore could do a task significantly faster than a non-quantum computer. This idea of running an algorithm faster on a quantum computer than a regular computer is known as quantum supremacy. Quantum supremacy is important since it shows that quantum computers are better than regular computers for certain tasks, essentially proving that they are useful. Thus, achieving quantum supremacy, as Google claims, is a major advancement in the quantum computing field and paves the way for new experiments and computers.\nThe Experiment: Gates and Random Circuits Building Sycamore, a big quantum computer that runs with low error, was possible due to improvements in quantum hardware, specifically quantum gates. Quantum computers (as well as regular computers) are composed of many gates. A gate takes an input of qubits (or bits for regular computers) and outputs some qubits (or bits). A gate is like a factory: it takes in some materials (qubits), transforms and processes them, and outputs different materials (qubits). Gates allow computers to run complicated algorithms using simple building blocks; gates are the DNA of computers.\nSince gates are implemented in hardware, they always have some error. Building anything perfectly in the real world is impossible! For regular computers, gates are very accurate. However, for quantum computers, gates tend to be inaccurate since qubits are inherently fragile. Bits in a regular computer can be stored in a magnetic hard drive, which can last for years without much error. Qubits, however, are more complicated. One way to build qubits is by trapping electrons in small chunks of silicon. Such qubits are sensitive to electromagnetic fields, heat, and other neighboring atoms. That is a lot of sources of error! These errors can cause decoherence, which means that the information encoded in the qubit is lost. As time goes on, the qubit is more likely to decohere, so it is difficult to build a computer that can run programs with long execution times.\nErrors in qubits causes qubits to change their state, which is bad for reproducible science. A single bit flip over the course of a trial of an algorithm will lead to an incorrect result. Before the quantum supremacy experiment was performed, the Google lab worked on building low-error quantum gates. On average, the error for an individual gate used in the experiment is around half a percent, meaning that for every 200 trials, there is expected to be a single error. Clearly, there is still more work to be done to reduce the error of quantum gates, but this error was good enough to perform the experiment and achieve accurate results.\nUsing their accurate gates, the Google experiment was designed to show that quantum computers can run certain computations faster than a classical computer. From a small set of quantum gates, the scientists created random configurations of around one thousand gates. This is similar to a monkey writing random code on the quantum computer for each trial. The scientists then ran the randomly configured quantum computer and recorded the results. They then simulated the results of the quantum circuit on a (non-quantum) supercomputer, verified them, and compared runtimes between the quantum circuit and the simulated circuit. It is very difficult for a non-quantum computer to simulate these random quantum circuits since there is not any structure to exploit, so the non-quantum computer ran much slower than the quantum computer. In fact, as the number of gates grew, the simulation problem became exponentially more time consuming for a non-quantum computer.\nImplications: Why is this Experiment Important? This experiment was important for the field of quantum computing since it showed that a 54-qubit quantum computer could work with reasonably low error. More qubits in the computer means that more complex systems can be modeled. In the case of the Google computer, there are around 10 quadrillion possible configurations of qubits! Also, the fact that such a large computer works shows the robustness of quantum mechanics. The Google team ran into no unexpected physics that prohibited the project from working. Therefore, in the future, it should be possible to build even bigger and more complex quantum computers. The computer used in this experiment can be used in other fields where simulations often help advance the science, such as physics and quantum chemistry. For example, this quantum computer could be used to model the structure of molecules, which is difficult and computationally expensive on regular computers. There are also proposed applications of quantum computing in finance, healthcare, and computer science, where processors similar to Sycamore could be used. For example, quantum computing could help speed up calculations to optimize the selection of a portfolio of financial assets. Also, the quantum computer developed by Google can be used to create certifiably random numbers. This contrasts with most random number generators that are pseudorandom, meaning they could be predicted given certain information. True randomness is important for computer science applications. For example, in cryptography, randomness is used to generate encryption keys. The more random the numbers, the more secure the system.\nControversy: IBM and Quantum Supremacy Although nobody can deny the difficulty of the Google experiment and the scientific value of the results, there is some controversy about how much of a speedup the quantum computer has over a normal supercomputer. Google claimed that their experiment would take a supercomputer 10,000 years to run. However, IBM, another leader in the quantum computing space wrote that they would be able to perform the experiment in two or three days using their Oak Ridge Summit supercomputer. IBM stated that the experiment did not achieve quantum supremacy because it would still be possible to simulate it on a supercomputer in a few days. At the end of the day, lots of the controversy around the experiment revolves around how to define “quantum supremacy.” However, as one adds more qubits, the computations would get exponentially more difficult and would largely be impossible to simulate on a non-quantum supercomputer, meaning that even if IBM can simulate the 54-qubit processor, it would be almost impossible for them to simulate a 75-qubit or 100-qubit processor.\nLooking to the Future In conclusion, we can see that this experiment is a first step in building quantum computers that are better at certain tasks than regular supercomputers. The field of quantum computing is still young, but this experiment shows promise for the future. The field is growing at a fast rate and there will be more exciting advancements in the coming years.\nSources https://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html https://www.nature.com/articles/s41586-019-1666-5 ","permalink":"https://lucaspauker.com/articles/achieving-quantum-supremacy-qubit-by-qubit/","summary":"Faster than a Supercomputer? In the 1980s, American physicist Richard Feynman proposed the idea of quantum computers to model complex quantum systems. In October 2019, around 40 years later, Google AI and NASA scientists unveiled a quantum computer which ran an experiment in a few minutes that would take the fastest supercomputer 10,000 years. The quantum computer sped up the computation by a factor of 1 billion! This was one of the first major successes in the nascent field of quantum computing.","title":"Achieving Quantum Supremacy, Qubit by Qubit"},{"content":"Introduction In this blog post, I will implement a few simple time series models of a stock price over time. I will also see how they do if we trade using them. We will look at moving averages (MA) and exponential moving averages (EMA).\nData First, we need to download the price data. For this article, we will use SPY historical open price data. We can download this from Yahoo Finance. Now, let\u0026rsquo;s process the data into a dataframe and split the data into test and train datasets.\nimport pandas as pd data = pd.read_csv(\u0026#39;./SPY.csv\u0026#39;) data[\u0026#39;Datetime\u0026#39;] = [datetime.strptime(d, \u0026#39;%Y-%m-%d\u0026#39;) for d in data[\u0026#39;Date\u0026#39;]] percent_train = 0.7 train_data = data.head(round(len(data) * percent_train)) test_data = data.tail(round(len(data) * (1 - percent_train))) print(len(train_data), len(test_data)) Output: 4912 2105\nMoving Average Moving averages have a single parameter: the number of datapoints we look back.\nn_param = 30 # Number of days to look back with Now let\u0026rsquo;s build the moving average data.\nimport numpy as np ma = [] ma += ([None] * n_param) for i in range(len(dataset) - n_param): ma.append(np.mean(dataset[\u0026#39;Open\u0026#39;].iloc[i: i + n_param])) dataset[\u0026#39;MA\u0026#39;] = np.array(ma) dataset[\u0026#39;MA\u0026#39;] = dataset[\u0026#39;MA\u0026#39;].dropna() Let\u0026rsquo;s plot the data.\nimport matplotlib.pyplot as plt fig = plt.gcf() fig.set_size_inches(10, 5) r = range(0, 400) plt.plot(dataset[\u0026#39;Datetime\u0026#39;].iloc[r], dataset[\u0026#39;MA\u0026#39;].iloc[r], label=\u0026#39;MA\u0026#39;) plt.plot(dataset[\u0026#39;Datetime\u0026#39;].iloc[r], dataset[\u0026#39;Open\u0026#39;].iloc[r], label=\u0026#39;Open Price\u0026#39;) plt.title(\u0026#39;SPY Open Prices\u0026#39;) plt.xlabel(\u0026#39;Time\u0026#39;) plt.ylabel(\u0026#39;Price\u0026#39;) plt.legend() plt.grid() plt.show() Moving average (n=30) for subset of SPY data\nNow let\u0026rsquo;s try a simple mean reversion trading strategy. If the MA is below the open price by a THRESHOLD, then we will sell SPY. If it is above the open price by a THRESHOLD, then we will buy. In principle, if the moving average is a better indicator of true value than the price today, this strategy will do well.\nMAX_POSITION = 50 THRESHOLD = 1 position = 0 cash = 5000 positions = [position] capital = [cash] for i in range(1, len(dataset)): capital.append(cash + position * dataset[\u0026#39;Open\u0026#39;].iloc[i]) positions.append(position) diff = dataset[\u0026#39;Open\u0026#39;].iloc[i] - dataset[\u0026#39;MA\u0026#39;].iloc[i] if abs(diff) \u0026lt;= THRESHOLD: continue if np.sign(diff) == -1: # Buy if abs(position - 1) \u0026gt; MAX_POSITION: continue cash += dataset[\u0026#39;Open\u0026#39;].iloc[i] position -= 1 elif np.sign(diff) == 1: # Sell if abs(position + 1) \u0026gt; MAX_POSITION: continue cash -= dataset[\u0026#39;Open\u0026#39;].iloc[i] position += 1 sign = np.sign(position) for _ in range(abs(position)): # Get rid of remaining position cash += sign * dataset[\u0026#39;Open\u0026#39;].iloc[-1] position -= sign * 1 assert(position == 0) print(cash) Output: 10163.34\nBeing long SPY would have left us with $15,468, so this strategy is pretty bad. However, we arbitrarily chose n_param=30. Let\u0026rsquo;s see what parameter does best on the training dataset. Here is the result of the trading strategy above for n_param in the range [0, 500), sampled every 10 numbers.\nCash after trading versus different MA parameters\nWe can see that around n_param=370 is the best parameter for our training dataset. Let\u0026rsquo;s use this parameter on the test dataset and see how it does.\nMA vs. long SPY returns\nAs we can see, the MA strategy does worse than being long SPY. It also looks like there is more variance in returns. This probably has to do with the fact that our test data is from 2013-2020, which was a bull market. Thus, our MA strategy is mostly long and doesn\u0026rsquo;t differentiate itself from the baseline strategy very much.\n##Exponential Moving Average The idea behind EMA is that more recent data has a higher weight than old data. EMA takes a smoothing parameter $\\alpha$ The formula for the EMA of a time series with prices $(p_1,p_2,\u0026hellip;)$ is:\n$$ EMA_t = \\alpha * p_t + (1 - \\alpha) * EMA_{t-1} $$ for $t \u0026gt; 1$. Also, $$ EMA_1 = p_1. $$ Expanding this equation, we get $$ \\begin{eqnarray} EMA_t \u0026amp;=\u0026amp;\\alpha *[p_t + (1 - \\alpha) *p_{t-1} + (1 - \\alpha)^2 *p_{t-2} +\u0026hellip;]\\\\ \u0026amp;=\u0026amp;\\alpha *\\sum_i (1 - \\alpha)^i *p_{t-i}. \\end{eqnarray} $$ Therefore, we can see that points further back in time are weighted less. Here is the code to generate the EMA from our data:\nema = [dataset[\u0026#39;Open\u0026#39;].iloc[0]] for i in range(1, len(dataset)): mult = 2 / (n_param + 1) prev_ema = ema[i - 1] ema.append(mult * (dataset[\u0026#39;Open\u0026#39;].iloc[i] - prev_ema) + prev_ema) dataset[\u0026#39;EMA\u0026#39;] = np.array(ema) dataset[\u0026#39;EMA\u0026#39;] = dataset[\u0026#39;EMA\u0026#39;].dropna() Let\u0026rsquo;s compare the EMA to the simple MA.\nfig = plt.gcf() fig.set_size_inches(10, 5) r = range(0, 400) plt.plot(dataset[\u0026#39;Datetime\u0026#39;].iloc[r], dataset[\u0026#39;MA\u0026#39;].iloc[r], label=\u0026#39;MA\u0026#39;) plt.plot(dataset[\u0026#39;Datetime\u0026#39;].iloc[r], dataset[\u0026#39;EMA\u0026#39;].iloc[r], label=\u0026#39;EMA\u0026#39;) plt.plot(dataset[\u0026#39;Datetime\u0026#39;].iloc[r], dataset[\u0026#39;Open\u0026#39;].iloc[r], label=\u0026#39;Open Price\u0026#39;) plt.title(\u0026#39;SPY Open Prices\u0026#39;) plt.xlabel(\u0026#39;Time\u0026#39;) plt.ylabel(\u0026#39;Price\u0026#39;) plt.legend() plt.grid() plt.show() EMA and MA (n=30) for subset of SPY data\nConducting the same analysis as the simple MA, we see that we maximize return on the training dataset when n_param=370. Similar to the MA, however, when we run the mean reversion strategy on the test dataset, it performs slightly worse than simply being long SPY.\nConclusion We can see that trading using this methods does not yield any edge. This is expected however, as these models are very simple and don\u0026rsquo;t take into account any information besides the price of SPY. I hope that the analysis is interesting and that this post clearly shows how to build and test simple models.\nSources https://en.wikipedia.org/wiki/moving_average#exponential_moving_average ","permalink":"https://lucaspauker.com/articles/simple-stock-market-models-with-python/","summary":"Introduction In this blog post, I will implement a few simple time series models of a stock price over time. I will also see how they do if we trade using them. We will look at moving averages (MA) and exponential moving averages (EMA).\nData First, we need to download the price data. For this article, we will use SPY historical open price data. We can download this from Yahoo Finance.","title":"Simple Stock Market Models with Python"},{"content":"Introduction This blog post will discuss the delivery options for someone who is short US Treasury (UST) futures contracts. During the last month of trading, the short \u0026ldquo;delivers\u0026rdquo; the USTs specified in the futures contract to the long. The short has various options when they make delivery of the treasuries. Understanding these options is interesting and important for anyone who trades UST futures and the UST basis.\nUST futures are some of the most liquid financial contracts in the world. UST futures began being traded in 1977 and the Chicago Board of Trade has consistently introduced more Treasury futures products due to their popularity. UST futures are unique from other futures contracts because the short can deliver any UST from a basket of bonds/notes. For example, for the 10-year T-note futures (TN), one can deliver,\n\u0026ldquo;U.S. Treasury notes with a remaining term to maturity of at least six and a half years, but not more than 10 years, from the first day of the delivery month\u0026rdquo; (CME Website).\nSince there are many notes with maturities between six and a half years and 10 years, the short who is delivering bonds has a wide range of notes to choose from. This optionality to choose from a variety of securities, as well as the fact that the short can choose any day in the delivery month to make delivery, creates a variety of valuable options for the short.\nTrading of a UST futures contract terminates a week before the end of the delivery month. The options the short has to change the contract they are delivering during this week are known as the “end-of-month” option. The options before trading expires to change the contract are known as the “switch” option. Lastly, the short has a timing option since they can choose when to deliver throughout the month. I will go through these options separately, starting with the switch option.\nFor this article, I will assume some understanding of US treasuries, yields, conversion factors, and cheapest to deliver contracts. I will include links to supplemental articles.\nThe Switch Option The switch option, which is the short\u0026rsquo;s option to switch the delivery contract before the last week of the month, has value due to possible changes in the cheapest to deliver contract. The switch option depends on a number of factors, namely changes in yields, changes in the slope of the yield curve, and new UST issues.\nChanges in Yields As yields change, as mentioned, the cheapest to deliver contract may change. There are two rules of thumb that give intuition for how yields affect the cheapest issue to deliver.\nDuration rule: For bonds with the same yield below 6%, the bond with the lowest duration will be cheapest to deliver. For bonds with the same yield above 6%, the bond with the highest duration will be cheapest to deliver. Yield rule: For bonds with the same duration, the bond with the highest yield will be cheapest to deliver. The duration of a bond is the weighted average time of cash flow payments. As a result, prices of higher duration bonds are more sensitive to yield changes. 6% is an important number since the conversion factor of a bond is the price at which it would trade if its yield to maturity was 6%. Therefore, if all bonds in the delivery basket yielded 6% during the delivery month, all bonds would have a converted price of 100 and there would be no cheapest to deliver bond. So for two bonds with the same yield but different durations, if yields fall from 6%, the price of the bond with the higher duration will rise relatively more, so the lower duration bond will be cheaper to deliver. If yields instead rise from 6%, the higher duration bond\u0026rsquo;s price will fall more than the bond with the lower duration and thus the higher duration bond will be cheaper to deliver. Thus, we have the first rule.\nIf we instead have two bonds with the same duration but different yields, the converted price for the higher yield bond will be lower than the lower yield bond so the higher yield bond will be cheaper to deliver. This is the second rule.\nThese two rules help us understand how changes in yield will change the cheapest contract to deliver. The ability to change the delivery contract is valuable to the short since they can choose to deliver the cheapest contract.\nChanges in the Yield Curve There is a tendency for the yield curve to flatten as rates increase and steepen as they decrease (here is my tool to see today\u0026rsquo;s yield curve). Bond yields with different maturities do not move in parallel. Failing to take this fact into account would result in a mispricing of the short\u0026rsquo;s switch option value. The yield curve tends to flatten as rates increase and steepen as rates decrease. Because of this, the value of the switch option is reduced.\nTo see this, imagine we have two bonds A and B such that A has a longer maturity than B. If yields fall, then the yield curve tends to steepen so the shorter maturity bond (B) will have a larger change in yield than A. This means that B will be relatively more expensive compared to A. Falling yields however tend to cause lower duration bonds (B) to become cheaper, so this effect means that a larger change in yields is required for a change in the cheapest to deliver contract. If yields rise, then the yield curve tends to flatten so the shorter maturity bond (B) will have a larger upward change in yield than A. This means that A will be relatively more expensive compared to B. Rising yields however tend to cause higher duration bonds (A) to become cheaper, so this effect also means that a larger change in yields is required for a change in the cheapest to deliver contract. The result of the relative price changes between A and B is that the switch option will be less valuable since a change in the cheapest bond to deliver requires a larger change in yields.\nNonsystematic events may also result in yield spread changes. When a specific single issue becomes expensive to deliver relative to other issues, it causes nonsystematic changes in yield spreads. The examples of nonsystematic changes that Belton et al. give for this is changes in issuance, temporary squeezes, and Fed buyback programs. The effect of this type of event is that the yield curve may flatten as yields fall or that the yield curve may steepen as yields rise. This results in the opposite effect of systematic changes. Nonsystematic yield curve changes add value to the delivery option if nonsystematic changes are likely enough and sufficiently large to cause a change in the cheapest to deliver.\nNew Issues Bonds and notes issued before the first delivery day are eligible for delivery. If yields are high, higher durations are favored, according to the first rule of thumb above. Since the new issue will have a low duration due to its high coupon, it will not likely be the cheapest to deliver. If yields are low, lower durations are favored. Since the new issue will have a high duration due to its low coupon it will also not likely be the cheapest to deliver. Furthermore, new issues (\u0026ldquo;on the run\u0026rdquo; issues) tend to trade at a premium for liquidity so they are typically expensive. Thus, we can see that under normal circumstances, the new issue is rarely the cheapest to deliver. This means that the new issue option is not very valuable.\nAn exception to this rule is when yields are high but falling. High yields mean that high duration bonds are cheaper to deliver. But since yields are falling, the new issue has the lowest yield and longest maturity in the delivery basket, meaning it has a high duration, which would make it cheapest to deliver.\nThe End-of-Month Option The last day of trading for a futures contract is the seventh business day before the last business day of the delivery month. After the last trading day, the futures price for each bond is fixed, however the cash treasuries still trade. Therefore, a sufficiently large change in yields might change the cash price of assets enough to change which contract is cheapest to deliver. Changes in the cheapest to deliver contract are driven by different reasons during the end-of-month period compared to regular trading. Since the futures invoice price is fixed during the end-of-month period, the net cost of delivering a bond into the futures contract only depends on the absolute cash price. Therefore, high yield, low duration bonds tend to be cheaper as yields rise during the end-of-month period. This contrasts with the first rule of thumb above that higher duration bonds are cheaper to deliver as yields rise before the end-of-month period.\nThe Timing Option The short has not only the option to choose which contract to deliver, but they can also choose when to deliver it within the delivery month. This is the timing option. There are three factors that can affect the timing option: carry, aftermarket price moves, and limit moves.\nCarry If the cost of financing a bond is less than the coupon income on the bond, carry is positive. If carry is positive, a long basis position (long cash, short futures) has positive cash flow since the bond position earns the positive carry. Therefore, it is advantageous for one who is short futures to wait to make delivery. If conversely, carry is negative, then it may pay for the short to make delivery at the beginning of the month to avoid negative carry costs. However, delivering early means that the short gives up the value of the other delivery options, so it may not be advantageous. There is a tradeoff for delivering early.\nAftermarket Price Moves This option is known as the wild card option and has to do with how the futures trading is structured. During the delivery month, at 2:00PM (Central Time), futures trading closes and the CME determines the final settlement price for all futures contracts that expire in the delivery month. All delivery invoices declared that day are determined by the final settlement price. However, the cash treasuries market doesn\u0026rsquo;t close until 4:30PM. This means that there is a two and a half hour period where the futures settlement price is fixed but cash can still trade. Since the short has until 8:00PM to declare intent to deliver, they can take advantage of price moves in the cash market during this time period. If the cash price of a deliverable bond changes substantially, it may pay for the short to declare delivery that day and buy the cash treasuries. Note that, assuming that the short is hedged, the cash purchase only consists of the \u0026ldquo;tail\u0026rdquo; on their position, or the remaining amount of cash treasuries the short must buy to make delivery.\nLimit Moves During the delivery month, there are no price limits for the treasury futures contracts. However, the short can give notice of delivery during the last two days of the previous month, where there are price limits. If there is a limit move in those two days, the futures settlement price essentially is determined early and the wild card play is more likely to happen. In practice, this option has little value because it only lasts two days.\nConclusion We can see that the delivery options for the short are complex and numerous. I hope that this blog post is an interesting overview of the short\u0026rsquo;s delivery options and helps the reader understand some of the complexity.\nSources http://www.rnfc.org/courses/finance/modules/bond-yields-modelling/Understanding_Duration_Volatility.pdf https://www.cmegroup.com/trading/interest-rates/files/us-treasury-futures-delivery-process.pdf https://www.cmegroup.com/education/files/treasury-futures-basis-spreads.pdf Belton, Terrence M., et al. The Treasury Bond Basis: an in-Depth Analysis for Hedgers, Speculators, and Arbitrageurs. McGraw-Hill, 2005. ","permalink":"https://lucaspauker.com/articles/us-treasury-futures-delivery-options/","summary":"Introduction This blog post will discuss the delivery options for someone who is short US Treasury (UST) futures contracts. During the last month of trading, the short \u0026ldquo;delivers\u0026rdquo; the USTs specified in the futures contract to the long. The short has various options when they make delivery of the treasuries. Understanding these options is interesting and important for anyone who trades UST futures and the UST basis.\nUST futures are some of the most liquid financial contracts in the world.","title":"US Treasury Futures Delivery Options"},{"content":"Why Exchange Clocks Matter In high frequency trading (HFT), time is literally money. An edge of a few microseconds could translate to millions in profits. The most popular markets for cash products (equity, bonds, etc.) are in New York (NYSE, Nasdaq), while the most popular futures and options market is in Chicago (CME). Since cash and futures influence each other, getting data between New York and Chicago as fast as possible is important. If one has the fastest transmission time, they can look for price discrepancies and book profits.\nAn example of implementing this arbitrage strategy is in 2009, Spread Networks layed down an ultra high speed fiber-optic cable between Chicago and New York. In order to do it, they bored through mountains in Pennsylvania in order to make the path of the wire as straight as possible, reducing the distance information had to travel. In total, the project cost $300 million. The result? They can send information in 13.3 milliseconds round trip, 3 milliseconds faster than the competition at the time. So we can see that a 3 millisecond edge is worth at least $300 million.\nMap of Chicago to New York, taken from here\nNow, companies such as McKay Brothers use microwave communication to transmit information in 8 milliseconds round trip. We are close to reaching the lower limit on transmission time since information can’t travel faster than light (i.e. microwaves). With many modern HFT firms competing on the microsecond scale, it is important to understand how exchanges process orders and ensure fair play. For example, if I trade one microsecond faster than a competitor, the exchange needs to ensure that my order gets processed first.\nHaving accurate clocks helps prevent fraudulent activity. One illegal form of market manipulation is front running. Front running is when a firm “cuts in line” to make money off of orders. For example, let’s say someone submits an order to a broker to buy large size of shares of a company. Since the broker sees that this order will increase the price of the company stock, they can put in buy orders from their own account before their client’s orders and book a profit. If exchange clocks are imprecise\u0026ndash;say they are only accurate to one second\u0026ndash;and a HFT can execute many orders in a second, then the exchange could reorder all the trades in the second in any way they want, without being detected by an auditor! This is clearly a problem, since in trading, when there is easy money to be made through fraud, people typically take advantage of it\u0026hellip;\nAccurate clocks are also important to settle trades correctly when abnormal events happen. One example of this is the 2010 flash crash. After the market closed, FINRA decided to cancel trades made after 2:40 PM and at a certain price depending on the security. Therefore, having accurate trade logs was essential to apply this rule. If there are any discrepancies about who traded, the exchange should be able to accurately retrace every trade in order. For these reasons, there is a need for accurate clocks across all computers in the exchange.\nLaws In Place There are many laws in place that exchanges must abide by related to how accurate their clocks need to be and how closely the clocks need to be synced with National Institute of Standards and Technology (NIST) clocks.\n1998 NASD Order Audit Trail System (OATS) rule 6953: This rule came after a 1996 SEC report found that dealers did not always act in the best interest of their clients. In 1998, FINRA established the OATS audit system to track transactions made in the market. Rule 6953 required all time stamping devices to be synchronized within three seconds of the NIST atomic clock. The three second rule required clocks without second precision to be discarded. In 2008, this rule was replaced by rule 7430 which required clocks to be within one second of NIST.\n2012 Consolidated Audit Trail (CAT) rule: In 2012, the SEC implemented the CAT rule, which was a more expansive version of the OATS rule. This rule required every registered broker and broker-dealer to keep track of every order and send it to a central location every day. This means every trade can be monitored by the SEC. Furthermore, timestamps were required to be recorded in milliseconds. Lastly, timestamps for automated orders were required to be within 50 milliseconds of NIST and timestamps for manual orders were required to be within 1 second of NIST. Automated orders are machine-initiated trades (algorithmic trading), while manual orders are human-initiated trades.\nHow Does the NIST Atomic Clock Work? In 1968, the second was defined (and remains defined) as a certain number of cycles of radiation corresponding to the transition between two energy levels of the ground state of the cesium-133 atom at absolute zero. Essentially, the cesium atom oscillates, and by counting the oscillations, we can get a consistent measure of time. In fact, when the cesium atom changes energy levels, it emits light (photons) which can be measured by using the photoelectric effect.\nAn atomic clock has a beam of cesium atoms that absorb energy in the form of microwaves and vibrate. The frequency of the microwaves is varied until a maximum number of cesium atoms change state, indicating that the correct frequency has been reached.\nNIST-F1, taken from here\nOne of the principle NIST clocks used by stock exchanges is NIST-F1 which lives in Boulder, Colorado. One factor that is controlled for in this clock is the temperature of the atoms. Since the second is defined at absolute zero, NIST-F1 uses lasers to slow the movement of the atoms so that the temperature is close to absolute zero: “a few millionths of a degree above absolute zero,” according to their website. Traditional cesium clocks measure atoms that are quickly moving through the air, so they only have a few milliseconds to adjust the microwave frequency. In contrast, since NIST-F1 has close to stationary atoms, they have around a second to tune the frequency. This means that the frequency is more accurate than other clocks, leading to more accurate times.\nAs of 2013, NIST-F1 has an uncertainty of \\(3*10^{-16}\\), which means that it would take 100 million years for the clock to be off by a second, according to their website. Impressive! And certainly accurate enough for stock exchanges.\nHow are Clocks Synced? Given the laws described above, a practical problem that exchanges have is how to sync their clocks to the NIST standard. The simplest way to do it would be to connect to a service that streams the NIST time out and read it into your clock to sync it. However, doing it this way will result in your clock being off by the time it takes for the signal to travel to you, which is not insignificant (it is on the scale of milliseconds).\nSatellite clock syncing, taken from here\nThe way that this problem is amended is by having a third party that syncs the clocks. For example, if I want to sync my clock in Chicago with NIST, which transmits data from Boulder, then we can both connect to a satellite \\(S\\) that is equidistant to both Boulder and Chicago. The satellite has a clock on board, and when it receives the data from Boulder and Chicago, the satellite records the time differences $$\\Delta t_{S, Boulder}=t_S-t_{Boulder}$$ and $$\\Delta t_{S, Chicago}=t_S-t_{Chicago}.$$ From this, we can infer the difference between the two clocks \\begin{eqnarray} \\Delta t_{Chicago, Boulder} \u0026amp;=\u0026amp; \\Delta t_{S, Boulder} - \\Delta t_{S, Chicago} \\\\ \u0026amp;=\u0026amp; (t_S-t_{Boulder}) - (t_S-t_{Chicago})\\\\ \u0026amp;=\u0026amp; t_{Chicago}-t_{Boulder} \\end{eqnarray} Now I can adjust my clock until \\(\\Delta t_{Chicago, Boulder} = 0\\) and the two clocks will be synced!\nIf the satellite is not equidistant from both clocks (i.e. \\(d_{S, Chicago}\\neq d_{S, Boulder}\\)), then we can correct for this error as long as we know where the satellite is relative to both clocks since we know that the data travels at the speed of light. The ionosphere and troposphere layers of the atmosphere may also introduce error terms that must be taken into account in practice.\nConclusion As we can see, clocks in stock markets are vital for trading to function fairly among all participants. However, coming up with accurate clocks and syncing them has many layers: building the clock, syncing the clock. I hope this article provides some new insights into how a seemingly simple aspect of markets works.\nReferences https://nvlpubs.nist.gov/nistpubs/jres/121/jres.121.023.pdf https://www.nist.gov/pml/time-and-frequency-division/time-realization/primary-standard-nist-f1 https://jpm.pm-research.com/content/37/2/118 \u0026ndash;\nSee discussion on HackerNews\n","permalink":"https://lucaspauker.com/articles/timekeeping-in-financial-exchanges/","summary":"Why Exchange Clocks Matter In high frequency trading (HFT), time is literally money. An edge of a few microseconds could translate to millions in profits. The most popular markets for cash products (equity, bonds, etc.) are in New York (NYSE, Nasdaq), while the most popular futures and options market is in Chicago (CME). Since cash and futures influence each other, getting data between New York and Chicago as fast as possible is important.","title":"Timekeeping in Financial Exchanges"},{"content":"The Question I, like many others, started learning to play piano when I was around 8 years old. I took lessons, learned songs, and gained an appreciation for the instrument. I have been playing (with some breaks) until today. Although I have been playing for a while, it wasn’t until taking my first music theory class in college that I started thinking deeply about why music is constructed in the way it is. In the theory class, we learned about chord progressions and voice leading but I found that as I dug deeper into the theory, there were fundamental parts I didn’t understand.\nHere is a picture of the piano keys:\nPiano keyboard layout\nAs you can see, the keys are structured in a very intentional way; there is a common pattern across the keys. There is a set of 5 black keys that repeat, a set of 7 white keys that repeat, and a set of 12 total notes (white + black) that repeat.\nIn fact, these numbers (5, 7, and 12) are fundamental to western music. The most fundamental scales are the 5-note pentatonic scale, the 7-note diatonic scale, and the 12-note chromatic scale. One might ask: why these numbers? There are infinite frequencies that one could choose, so why choose discrete sets of 5, 7, and 12 notes for these common scales?\nIn this blog post I will explore this question and give a backing for why these choices are not arbitrary, and in fact have interestin properties related to fundamental principles about harmonics. When I first found this way of thinking about music, it helped me realize the beauty of the western music system that we hear in pop, rock, and jazz. I have tried to write this in a way that no music background is required to understand it, so without further ado let’s dive in!\nHarmonics and Overtones For those of us who have (maybe begrudgingly) taken physics classes, there is a concept of harmonics that appears everywhere. Let’s take plucking a guitar string for example. When you pluck the string, it vibrates at a certain frequency and it produces a noise at that frequency. Frequency \\(f\\) is simply how fast the string is vibrating. Higher frequencies sound like higher-pitched notes, and lower frequencies sound like lower-pitched notes Wavelength \\(\\lambda\\) is the distance that a wave’s shape repeats. Wavelength and frequency are inversely related:\n$$f\\alpha\\frac{1}{\\lambda}$$ Intuitively, plucking a shorter string creates a higher-pitched sound and vice versa. Depending on how fast a string is vibrating (the frequency), there are harmonics that arise that produce different sounds. Let’s first look at it graphically:\nFrom Wikipedia\nThe first harmonic is the “easiest” (lowest energy) way to make the string vibrate. The second harmonic is the next easiest, and so on. Let’s say the first harmonic has a wavelength of 1. This means that the frequency of the first harmonic is 1 (in the ratio). Therefore, since the second harmonic has wavelength of ½, the frequency is 1 Furthermore, the third harmonic has a frequency of 3. We can see that the ratio of frequencies from the second to first harmonics is 2, and the ratio of frequencies from the third to second harmonics is 3/2. These ratios are important, and we will come back to them shortly.\nBuilding an Instrument Now we are ready to use this knowledge to build an instrument. When one plays a note on an instrument (i.e. a note at a frequency \\(f\\)), although the note at frequency \\(f\\) is the fundamental tone, there are other frequencies that are also present; these frequencies are called overtones. For example, if I play an A at 440 Hz on a flute, A is the fundamental tone, but the frequencies of 880 Hz (A an octave up, and the second harmonic) and 660 Hz (E, which is a fifth above A and is the third harmonic) are also present. Here is a video of Leonard Bernstein demonstrating overtones. If one plays a note as well as one of its overtones, it sounds good to the ear. This is why octaves (which are frequencies separated by a factor of 2) sound good when played.\nAnother concept we can use to build our instrument is equal-temperament tuning. This means that the ratio of frequencies between any two subsequent notes should be the same. This is convenient for us since it means that transposing music (moving a melody up or down some notes) is trivial since it doesn’t change the relationships between the notes. Furthermore, pianos and other instruments such as guitars have equal-temperament tuning.\nThe simplest instrument we could construct then would be a one-note instrument that consists of some frequency \\(f\\) and all the power of 2 factors of \\(f\\), such as \\(2f\\), \\(4f\\), etc. In Western music, these are all thought of as the same note since the frequency of each subsequent note is separated by a factor of 2, which is an octave. The one-note instrument introduces an important concept: we want to be able to play octaves on our multiple-note instruments. However, the one-note instrument is boring since we are very limited in what we can play, so let’s add more notes.\nNow let’s build a two-note instrument. We will essentially add a note between each octave. Enforcing the equal-temperament rule, we must have the ratio between any two notes equal to \\(\\sqrt2\\). For our one-note instrument, each subsequent note was the second harmonic of the first note since the frequency was doubled. The next harmonic is the third harmonic and the third harmonic has \\(\\frac{3}{2}\\) times the frequency of the second harmonic. We can see that \\(\\sqrt{2}\\) is pretty close to \\(\\frac{3}{2}\\). Thus, we can almost play the fifth with our two-note intrument. Ideally, we want to approximate the interval of a fifth (i.e. \\(\\frac{3}{2}\\) times the frequency of another note) as close as possible since fifths sound good to the ear. However, we can see that with this construction, we miss the fifth by a decent amount, so it would be ideal to be able to play fifths more accurately.\nNow let’s build a five-note instrument. Since five notes make up an octave and we want equal-temperament tuning, the ratio between any two subsequent notes is \\(\\sqrt[5]{2}\\). We can see that \\(\\sqrt[5]2^3 = 1.516\\), so using the five-note scale lets us play intervals that are closer to fifths compared to the two-note instrument. In contrast, if we built a four-note scale the closest ratio we can get to the fifth is \\(\\sqrt[4]2^3=1.414\\), so we would prefer the five-note scale over the four-note scale.\nThe Equation Now we have some intuition into an aspect of what makes a good scale: we want to have the fifth encoded into the scale as closely as possible. Let’s abstract this concept a bit. For any integer \\(n\\)-note scale, the ratio between any two notes is \\(\\sqrt[n]2\\). So we want to find an integer \\(n\\) such that for some integer \\(k\\),\n$$\\sqrt[n]2 ^k\\approx\\frac{3}{2}.$$\nWe can rewrite this idea as trying to find an integer \\(k\\) such that \\(1\\leq k \u0026lt; n\\) such that we minimize \\(k - \\log_2(\\frac{3}{2}) * n\\). So we can see that to find the optimal \\(k\\), we can solve\n$$k=round(\\log_2(\\frac{3}{2}) * n),$$\nwhere \\(round(x)\\) rounds \\(x\\) to the nearest integer.\nLet’s see what we get for different values of \\(n\\).\nWe can use the following Python code to do it programmatically:\nfor n in range(1, 20): k = round(n * math.log2(3/2)) fifth_error = 3/2 - 2 ** (k / n) print(n, k, fifth_error) Let\u0026rsquo;s look at the output:\nData output for python code\nWe can see that the smallest errors between the perfect fifth and the closest ratio we can reach with an \\(n\\)-note scale occur for \\(n=5\\), \\(n=7\\), and \\(n=12\\). \\(n=12\\) has the smallest error with only 0.0017. This is very cool since those numbers represent the pentatonic, diatonic, and chromatic scales. Extending this concept, we can see that \\(n=17\\), \\(n=19\\), and \\(n=24\\) are also very close to approximating the fifth with equal temperament. All these numbers are common tunings used in scales around the world.\nConclusion In conclusion, we have derived many common tunings by simply seeking to approximate the fifth as closely as possible using an equal-temperament approach. From this study, we can see that there are real reasons to choose certain number of notes in a scale based on harmonics. I will note that this method is simply a way of understanding why scales have certain numbers of notes and is not the way that the scales were generated historically. Specifically, the western diatonic and pentatonic scales are not equally tempered. However, this is still a valuable and interesting way of thinking about music scales.\n","permalink":"https://lucaspauker.com/articles/why-do-musical-scales-have-certain-numbers-of-notes/","summary":"The Question I, like many others, started learning to play piano when I was around 8 years old. I took lessons, learned songs, and gained an appreciation for the instrument. I have been playing (with some breaks) until today. Although I have been playing for a while, it wasn’t until taking my first music theory class in college that I started thinking deeply about why music is constructed in the way it is.","title":"Why do Musical Scales Have Certain Numbers of Notes?"},{"content":"Introduction I spent the summer of 2019 as a physics research intern at the Stanford University Solar Lab. I was very fortunate to have a wonderful advisor and had a great summer overall. I created machine learning models to characterize time series data for solar flare prediction. In this article, I will first provide some physics background about solar flares, then dive into my research. For a more in-depth analysis, check out the source code and my poster.\nWhat are solar flares? Solar flares are sudden releases of energy due to rapidly changing magnetic fields on the sun. Here is a video of a solar flare. Solar flares can release up to 1025 joules of energy. To put this in perspective, the estimated world energy consumption in 2018 was about 5*1020 joules; the largest solar flares release almost 20,000 times more energy than that!\nExtreme ultraviolet movie of a solar flare taken by the AIA instrument.\nSolar flares are a result of the sun\u0026rsquo;s constantly changing magnetic fields. The sun has many areas of strong magnetic field on its surface that rotate with the sun: these areas are called active regions. Within active regions, there may be sunspots (dark spots with a stronger magnetic field than the rest of the sun\u0026rsquo;s surface). Active regions tend to have areas of positive and negative magnetic field (like a bar magnet), so we can imagine that the magnetic field above the active region is a loop connecting the positive region to the negative region. When these loops get twisted, for example due to the movement of plasma on the sun, energy builds. The loops sometimes connect to other loops to release a burst of energy in a short period of time: this burst is a solar flare.\nScientists have been interested in properties of the sun for a long time: it is the closest star we can study! There have been two major missions for studying the sun: SOHO and SDO.\nSOHO: Solar and Heliospheric Observatory: Launched in 1995, this satellite was originally planned to be operational for only two years, however it is still running and taking data. SOHO is at the L1 point between the earth and the sun, meaning that it never changes its distance from the earth or the sun. SDO: Solar Dynamics Observatory: Launched in 2010, this satellite contains state-of-the-art instruments for taking beautiful pictures and magnetic data of the sun. The instrument that takes magnetic data is called HMI (Helioseismic and Magnetic Imager); the data I used for analyzing solar flares is from this instrument. Magnetogram taken by HMI: white represents positive magnetic field, black represents negative magnetic field. Note that there are distinct large patches of black and white: these are active regions.\nWhy should we care about solar flares? There are both practical and scienitific reasons for studying solar flares.\nFirst, solar flares have a direct effect on space and earth; they can produce particles in the solar wind which can alter the earth’s magnetic field and emit radiation that affects spacecraft. Solar flares are often accompanied by coronal mass ejections that can affect satellites and power grids on earth. For example, in 1989, a geomagnetic storm caused a blackout in Quebec due to variations in the Earth\u0026rsquo;s magnetic field resulting from the storm. A better understanding of the factors that cause solar flares could allow for more accurate predictions of solar weather to keep satellites and grids functional, as well as improve planning for space travel.\nSecond, understanding the factors behind solar flares and how the magnetically active regions of the sun change will increase our knowledge of other stars. Many stars have starspots (and therefore magnetic field patterns) similar to the sun and understanding how the active region of the sun changes would provide insight into how these other stars behave. This is important for finding stars that could have orbiting planets that support life, and for understanding differences between stars in our universe.\nWhy is time series important? The problem of predicting flares is not new. My approach to the problem was different in two main ways.\nUsing time series: Much of the previous literature in the field has focused on using discrete values for flare prediction. For example, active regions with more magnetic flux tend to have a higher probability of flaring. After discussing with my advisor, I instead studied time series data. Rapid emergence of magnetic flux and free magnetic energy often indicates an increased probability of future flaring activity; if the magnetic loops get twisted faster, they are more likely to flare. It is reasonable to assume that two active regions with the same amount of magnetic flux, but different flux emergence rates have different flaring activity. I set out to model and quantify this activity. Extracting physical meaning: Another goal of my project was to extract some physical meaning from the time series data. This meant choosing a model that was simple and interpretable. For this reason, I ruled out neural networks, such as LSTMs, due to to their low interpretability. What does the data look like? The data for this project comes from 10 years of HMI measurements aboard SDO. SDO captures and sends over a terabyte of data to earth every day. This is equivalent to amount of data the Hubble Space Telescope generates in over a month. The SDO database has amassed petabytes of data. The HMI instrument records magnetic data, which is represented in the magnetogram picture above. Since storage and analysis of data for the entire disk of the sun is expensive and cumbersome, a data product called SHARP tracks active regions and their physical properties. The SHARP database contains time series data for many different quantities calculated from the raw HMI data, including magnetic flux and active region area. I created models with the SHARP data for my project.\nHow do we characterize time series data for machine learning? One of the biggest challenges for my project was featurizing the time series data. In order to do this, I first segmented the data into positive and negative classes, then used spline fitting to featurize the data segments.\nSegmenting data: Since flaring is a binary event, I wanted to create input data for a machine learning binary classification algorithm. In order to effectively learn on time series data, all the variables are scaled (linearly) down to zero at the start of the time series data. Furthermore, I defined the positive and negative classes (note: this classification was chosen because it is similar to previous positive/negative classifications in the field) as follows: Positive class: A 24-hour period before a flare. Negative class: A 24-hour period without a flare in an active region that does flare. This last distinction is important since the problem of distinguishing between flaring and non-flaring active regions is easier than distinguishing between flaring and non-flaring regions within a flaring active region. Fitting: In order to featurize the time series data, I first tried polynomial fits. I fit a polynomial using regression for each segmented 24-hour time series, then used the polynomial coefficients as features for the machine learning classifier. However, this approach is not ideal for two reasons. First, polynomial fits returned low accuracy due to Runge\u0026rsquo;s phenomenon, where small changes in the time series data could result in large changes in the polynomial coefficents. Second, the sun does not (necessarily) work according to some polynomial function over time. A better way to represent the changing variables on the sun is to use data that is divided even further. Instead of polynomial fits, I used spline fits. Spline functions are smoothed piecewise polynomials. This means that the data at the beginning of the time series will not affect the coefficients of the spline for the end of the time series. Nice! I used the interpolated spline fit coefficients as features for a machine learning algorithm to distinguish between the positive and negative case (I specifically used B-splines due to their simplicity and interpretability). Using a stochastic gradient descent and Adaboost classifier, I was able to accurately predict whether a time series will flare according to the spline fit coefficients. Both models produced similar trends, while Adaboost had higher accuracy.\nWhat did we learn? For each variable recorded using the HMI instrument, I fit the variable to a model to predict flares. The features for the model (as mentioned before) are the spline fit coefficients (fit on variable vs. time data). Lag time is defined as the length of time series data used for fitting and learning. I ran trials for each variable from HMI for lag times from 2 hours to 24 hours. As lag time changed, the same positive and negative class data was used. The graph below shows the lag time versus accuracy for select variables.\nGraph of accuracy lag time for select variables with error bars. Left: features where accuracy does not increase with lag time. Middle: features where accuracy increases with lag time. Right: combined feature graph.\nFrom the graph above, it is evident that more lag time, which means more time series to learn on, improves the accuracy of the model for some features. Thus, time series is important for predicting flares; part of the signal for flares is encoded in the time series. The accuracy seems to flatten out near the 15-hour mark, however the error bars (width of the lines) are too large to make a definite conclusion. Note: although the individual accuracies of the features are low, I expect that combining the features as well as adding discrete features would yield a higher accuracy. The goal of this study was not to create the highest-accuracy model, instead it was to investigate the importance of time series data in flare prediction.\nThere are some variables that far outperform the others.\nThe highest-performing variables are unsigned flux (USFLUX), total electric current helicity (TOTUSJH), and total free energy (TOTPOT). Unsigned flux is a measure of how much magnetic field there is in an area. The formula for flux is magnetic field times area. Changing flux indicates changes in fundamental properties of the active region, so it makes sense that total flux would be a strong predictor. Total electric current helicity is a measure of the rotation of the active region. It makes sense that current would be a strong predictor since changing magnetic fields cause flares, and current measures how a magnetic field is changing. Total free energy is a measure of potential energy in an active region, which reflects how twisted the magnetic fields are. This is one of the best-performing features because flares are dependent upon a buildup of potential energy. The lowest-performing variables are mean electric current helicity (MEANJZH), and polarity inversion line flux (R_VALUE). Mean electric current helicity is the average electric current helicity value across the entire active region. Since mean values are subject to more error than total values, this value is not a good predictor of flares. Polarity inversion line flux is the magnetic flux near the line that divides the positive and negative regions on an active region. This feature may be a weak predictor because much of the flare signal is not located around the polarity inversion line. What are the next steps forward? Although the above discussion shows that time series data is important for predicting solar flares, this is by no means an exhaustive study of all the work that can be done in the future with time series analysis. In fact, this project is just a first step in time series analysis of flaring active regions. Here are a few ways that this work can be extended:\nA polynomial or spline model does not accurately collect all the data in the time series. The sun could be better modeled by fitting the probabilities that govern how the state of different magnetic variables change. A state space model would likely produce a more robust and accurate model of the sun. Further analysis can be done by changing the negative case definition. I defined the negative class as a 24-hour period without a flare. However, there could conceivably be a negative case with a flare at hour 25 (one hour after the time series). This would likely look similar to the positive case, where there is a flare at hour 24. Thus, one could explore changing the negative case to assert that there are no flares for some time after the time series data ends. Lastly, the methods used in this project could be used to predict different solar events with time series data. \u0026ndash;\nI am grateful for all the support and kindness from the Stanford solar physics group.\n","permalink":"https://lucaspauker.com/articles/understanding-solar-flares-with-time-series-data/","summary":"Introduction I spent the summer of 2019 as a physics research intern at the Stanford University Solar Lab. I was very fortunate to have a wonderful advisor and had a great summer overall. I created machine learning models to characterize time series data for solar flare prediction. In this article, I will first provide some physics background about solar flares, then dive into my research. For a more in-depth analysis, check out the source code and my poster.","title":"Solar Flare Time Series Research"},{"content":"Recently, I wondered if I could create a random number generator using the digits of pi.\nDisclaimer: this article\u0026rsquo;s purpose is not to create a computationally efficient random number generator; it is just a fun proof of concept.\nIn order to create a random number generator with pi, we must first verify that the digits of pi are randomly distributed. This can be accomplished in a few lines of python code.\nWe will use the mpmath library, which provides high precision for constants such as pi.\nfrom mpmath import mp Let\u0026rsquo;s write a function to find the distribution of the base 10 digits in pi.\ndef get_pi_frequency(num_digits): mp.dps = num_digits digits = range(10) pi = mp.pi frequency_dict = {} for digit in digits: frequency_dict[str(digit)] = 0 for d in str(pi): if d == \u0026#34;.\u0026#34;: continue # Disregard the decimal in pi frequency_dict[d] += 1 return frequency_dict The frequency_dict that gets returned from get_pi_frequency will map each digit (0…9) to the number of times it occurs in num_digits of pi. Let\u0026rsquo;s test out the function and see if the digits in pi are randomly distributed.\nn = 100_000 print(get_pi_frequency(n)) Here is the output:\n{\u0026#39;0\u0026#39;: 9999, \u0026#39;1\u0026#39;: 10137, \u0026#39;2\u0026#39;: 9908, \u0026#39;3\u0026#39;: 10026, \u0026#39;4\u0026#39;: 9970, \u0026#39;5\u0026#39;: 10027, \u0026#39;6\u0026#39;: 10028, \u0026#39;7\u0026#39;: 10025, \u0026#39;8\u0026#39;: 9978, \u0026#39;9\u0026#39;: 9902} This looks very random by inspection - it is an even distribution. The standard deviation is 67.52, which is small compared to n.\nNow let\u0026rsquo;s jump into random number generation. The general idea is that we will write a function that takes a seed value s and then we will return a random list of numbers after the s digit of pi. For example, if the seed is 100 and we want 10 random numbers, we will return the 100th-110th digits of pi.\ndef get_random_number(k, seed=22_000): \u0026#34;\u0026#34;\u0026#34; Returns k random numbers as an array. \u0026#34;\u0026#34;\u0026#34; mp.dps = seed + k pi = mp.pi out = [] for d in str(pi)[seed + 1:]: out.append(d) return out Cool! Now we can output random numbers. However, there is one remaining issue: if we call get_random_number twice with the same seed, we will get the same \u0026ldquo;random\u0026rdquo; numbers. Thus, we should think of a way to systematically update the seed. One way to do it is to pick a random five-digit number to be the new seed.\ndef get_new_seed(seed): return int(\u0026#39;\u0026#39;.join(get_random_number(5, seed))) Now, we can run a test and see if we get random numbers. We will print 10 sets of 10 random numbers, updating the seed value each time.\nseed = 22_000 for _ in range(10): print(get_random_number(10, seed)) seed = get_new_seed(seed) Here is the output:\n[\u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;0\u0026#39;, \u0026#39;0\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;0\u0026#39;, \u0026#39;3\u0026#39;] [\u0026#39;7\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;6\u0026#39;, \u0026#39;0\u0026#39;, \u0026#39;9\u0026#39;, \u0026#39;6\u0026#39;, \u0026#39;3\u0026#39;] [\u0026#39;6\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;9\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;0\u0026#39;, \u0026#39;9\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;1\u0026#39;] [\u0026#39;1\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;0\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;9\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;3\u0026#39;] [\u0026#39;8\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;0\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;8\u0026#39;] [\u0026#39;8\u0026#39;, \u0026#39;0\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;9\u0026#39;, \u0026#39;0\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;5\u0026#39;] [\u0026#39;1\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;9\u0026#39;, \u0026#39;0\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;0\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;9\u0026#39;] [\u0026#39;7\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;6\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;6\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;6\u0026#39;] [\u0026#39;8\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;0\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;4\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;3\u0026#39;] [\u0026#39;4\u0026#39;, \u0026#39;0\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;1\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;9\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;2\u0026#39;] Now we have a fully functioning pseudorandom number generator based on the digits of pi!\n","permalink":"https://lucaspauker.com/articles/random-number-generator-with-pi/","summary":"Recently, I wondered if I could create a random number generator using the digits of pi.\nDisclaimer: this article\u0026rsquo;s purpose is not to create a computationally efficient random number generator; it is just a fun proof of concept.\nIn order to create a random number generator with pi, we must first verify that the digits of pi are randomly distributed. This can be accomplished in a few lines of python code.","title":"Random Number Generator with Pi"},{"content":"Introduction This project done for my CS221 class aims to classify classical music by musical era (Baroque, Classical, Romantic, Modern) with composers as a proxy.\nUsing audio processing techniques, such as Short-time Fourier Transform, we extracted features such as the spectrogram and chromagram of the audio data from two datasets, Free Music Archive and MAESTRO.\nWe used two ensemble classifiers, AdaBoost and Random Forest, and found that although Adaboost performed marginally better than Random Forest, the latter made more generalizable predictions. Both models achieve an accuracy rate of 60% on the test data, which is significantly better than the baseline prediction of 45%. Our project reveals the complexity of the era classification task, and we expect more complex models trained on a larger data set to achieve higher success.\nDatasets Our model was trained, developed, and tested on a combination of the following two datasets:\nFMA (Free Music Archive): a large-scale dataset of audio files with rich metadata, containing over 100,000 tracks across 161 genres (Defferrard et al). From this dataset, we found 441 tracks that were categorized as \u0026ldquo;classical\u0026rdquo; and had a valid composer. MAESTRO (MIDI and Audio Edited for Synchronous TRacks and Organization): a dataset with over 200 hours of labeled piano performances from 10 years of the International Piano-e-Competition, a piano competition that uses digital piano and records the music. This dataset contained 1282 songs that we could use in our model, making it the largest source of data for our project. Furthermore, we used the suggested 80-10-10 split, which eliminates overlap of pieces between training and test set. In total, our dataset consisted of 1723 songs, and we sampled the first 30 seconds of each song for feature extraction because practically a classifier should be able to make predictions with a limited exposure to the data. The songs were overwhelmingly from the Romantic period, with 789 Romantic songs in our dataset. In addition, we had 541 songs from the Baroque era, 280 from the Classical period, and 113 from the Modern era. The imbalance in the dataset may pose an over-fitting problem.\nFeature Extraction We used an audio processing library for Python called librosa to extract the following features using Short-time Fourier Transform from each audio file: chromagram, spectral centroid, spectral rolloff, root mean square value (RMS), and tonal centroid of a track.\nThe chromagram is a distribution of the pitches in a track. The spectral centroid is a measure of the weighted mean of the frequencies over time in a sample of each data point. The spectral rolloff is the frequency below which 85% of the spectral energy lies. We expected this feature to be a strong factor because the most distinguishing factor across era is the distribution of spectral energy. The root mean square feature takes the root mean square value of the spectrogram, the distribution of frequency over time. Lastly, the tonal centroid meaures harmonic properties of the pitches. We also expected to see high weight on this feature because another major distinction between the music from the earlier eras (Baroque and Classical) and the later eras, (Romantic and Modern), are the extent or lack of harmony. For each of the five features, we normalized and computed the mean and standard deviation. Algorithms For this project, we used two main algorithms: Adaboost and Random Forest. Both algorithms have pros and cons, which is discussed below.\nAdaboost AdaBoost is a boosting algorithm that iterates on the weights with multiple decision stumps to boost performance of these weak classifiers. The weights are first initialized as 1/n, with n being the size of the training set. These decision stumps are “weak” because the value of each weight is a threshold for each feature, dividing the dataset into four subsets based on the threshold value. In each iteration, the weight is updated according to the error measured by an exponential loss function.\nWe chose Adaboost as our preliminary model because features we are using are simple and straightforward relative to the complex nature of the audio data. In order to reconcile the simplicity of the features with the complexity of task, we chose ensemble models that use weak classifiers to model a complex system.\nRandom Forest Random Forest is an ensemble model that combines multiple simple decisions trees for added complexity. One key strength of Random Forest is the robustness against possible overfitting: each decision tree is instantiated randomly, it is more likely to capture the variation in the data.\nAnother difference between Random Forest and Adaboost is that while the number of estimator parameter specifies the number of iteration in Adaboost, the number of estimator is a parameter for the number of decision trees to be constructed in a Random Forest classifier model. Instead of iterating each weight n times, Random Forest takes an average over n decision tree at the end.\n##Comparison of Models Both models were trained with the number of estimator parameter = 50, and performed moderately well with an accuracy rate of ~0.60, which is 0.15 over the threshold value (0.45) and is comparable to the current level of success with genre classification. Nevertheless, an improvement in the performance is desired 60% is barely over a majority value. Interestingly, the accuracy of models have consistently decreased from our preliminary results as we increased our data by one order of magnitude (100 -\u0026gt; +1700 data points). This suggests the failure of simple models in processing larger variations in a bigger data set and the necessity of more complex techniques, such as recurrent neural networks. Furthermore, while we expected Random Forest to perform better because it is more robust against over-fitting, Adaboost performed marginally better than Random Forest in both training and test set.\nAn examination of the weight assignment yields more insights into the performance of the two models. We observe that the weight assignment for the Adaboost is more uniform across the ten features compared to Random Forest. In fact, the weights for the Adaboost are all around 0.1 for every feature except the mean of the spectral centroid. This is consistent with our understanding of the algorithms because the Adaboost initializes the weight for each feature with 1/n = 1/10 = 0.1 and then updates the weights after each iteration. With our low number of iterations (n=50), it is understandable that the feature weights do not deviate strongly from the initial values. However, we surprisingly found that the Adaboost performs worse with a larger estimator, which suggests the inherent limitation in the algorithm to process complex data. Since the Random Forest merges observations of randomly initialized decision trees, it is more precise than the Adaboost in that it weighs stronger features more heavily.\nTwo notable features for the Random Forest model are the standard deviation of the spectral rolloff (=0.35, over 1/3) and the standard deviation of tonal centroid (=.21). We\u0026rsquo;ve seen the importance of the former feature consistently in our previous experiments. For each audio segment, (i.e. for each audio frame), spectral rolloff is the frequency at which 85% of the energy lies below the frequency, so the standard deviation of this feature reflects how the energy of a piece changes over time. Intuitively, this comes close to human \u0026ldquo;feeling\u0026rdquo; of a piece, which makes it a good feature. To illustrate, we expect the spectral energy to be more concentrated in the midrange of piano for a Baroque piece, in contrast with a modern piece, whose spectral energy is more likely to be widely dispersed throughout the possible frequency range. Similarly, the standard deviation of tonal centroid measures how harmonious the chords are in a piece, which is understandably a distinguishing factor for pieces across different eras.\nError Analysis First, we can see that the Adaboost classifier never predicted Classical or Modern on the test dataset, which reveals the limitation of the model based on weak classifiers. This model failed to account for the minority data of the pieces from the Classical and Modern eras and over-fitted to the more abundant data.\nIn contrast, the Random Forest model seems provides a more nuanced classification, and we expect that this model would outperform the Adaboost with a larger scale data. We suspect the lower performance on our test set of the Random Forest to the relatively small size of our data.\nSince a near majority of the data was from the Romantic era, both models performed most successfully in predicting Romantic pieces. We expect to see a notable increase in the performance, especially in the Baroque, Classical, and Baroque data with more song data and more complex models.\n","permalink":"https://lucaspauker.com/articles/music-matcher/","summary":"Introduction This project done for my CS221 class aims to classify classical music by musical era (Baroque, Classical, Romantic, Modern) with composers as a proxy.\nUsing audio processing techniques, such as Short-time Fourier Transform, we extracted features such as the spectrogram and chromagram of the audio data from two datasets, Free Music Archive and MAESTRO.\nWe used two ensemble classifiers, AdaBoost and Random Forest, and found that although Adaboost performed marginally better than Random Forest, the latter made more generalizable predictions.","title":"Classical Music Classifier Project"},{"content":"Introduction I recently became interested in neural networks and sought interesting applications of what they could do. Neural networks are based on how the human brain works; the networks are made up of many nodes that take inputs and predict outputs. Over time, with more data, the networks improve by correcting for error, known as backpropagation. Neural networks become especially useful when there are many “layers” between the input and output that allow for more fine-tuned fitting of data.\nRecurrent neural networks have a sense of sequence built in, making them good for predicting text, market prices, or any other ordered data. Andrej Karpathy has written an easy-to-use module for recurrent neural networks that uses Torch, which I used for my neural network exploration.\nI set out to predict song lyrics for a given artist by compiling their lyrics and feeding it into the neural network. In order to do this, I first built a Python script that searched for and combined the lyrics for an artist into a single file. I used the musixmatch API for gathering the data. I first gathered the track ids in batches of 20:\ndef get_track_ids(artist): page_number = 1 track_ids = [] while True: url = \u0026#34;track.search?q_artist=\u0026#34; + artist + \u0026#34;\u0026amp;page_size=20\u0026amp;page=\u0026#34; + str(page_number) + \u0026#34;\u0026amp;s_track_rating=desc\u0026#34; response = call_api(url) if response[\u0026#34;message\u0026#34;][\u0026#34;header\u0026#34;][\u0026#34;status_code\u0026#34;] == 400: break track_list = response[\u0026#34;message\u0026#34;][\u0026#34;body\u0026#34;][\u0026#34;track_list\u0026#34;] for track in track_list: track_ids.append(track[\u0026#34;track\u0026#34;][\u0026#34;track_id\u0026#34;]) page_number += 1 return track_ids Then iterated through the track ids and got the lyrics:\ndef get_lyrics(track_id): url = \u0026#34;track.lyrics.get?track_id=\u0026#34; + str(track_id) response = call_api(url) return response[\u0026#34;message\u0026#34;][\u0026#34;body\u0026#34;][\u0026#34;lyrics\u0026#34;][\u0026#34;lyrics_body\u0026#34;] And wrote them all to a single file. I then ran it through a 2-layer recurrent neural network. Here are some snippets of the output data:\nBeatles lyrics:\nIf I needed someone of like girlen’s going look, oh my baby\nI just have to the someout in my mad\nSaid at the band’s finey\nI need even my love babe\nDrake lyrics:\nBut the city, she a mix for me,\nHey there yeah\nIt’s tryna keep?\n(Oh my last me)\nIn we said que this ash-we’re you, exsed to ya\nGucci Mane lyrics:\nGucci baby-hood,\nLike your girlfriends, you can’t keep up\nI get that break of it, I just mean\nWhile this couldn’t replace songwriters (and the lyrics only half make sense), the lyrics for each artist are clearly distinct and easily identifiable as the style of the artist.\n","permalink":"https://lucaspauker.com/articles/recurrent-neural-networks-for-song-lyrics/","summary":"Introduction I recently became interested in neural networks and sought interesting applications of what they could do. Neural networks are based on how the human brain works; the networks are made up of many nodes that take inputs and predict outputs. Over time, with more data, the networks improve by correcting for error, known as backpropagation. Neural networks become especially useful when there are many “layers” between the input and output that allow for more fine-tuned fitting of data.","title":"Recurrent Neural Networks for Song Lyrics"},{"content":"This project done for my CS106X class mixes two given mp3 files together and has a visualization to go with it.\nIntroduction I have always been awestruck by DJing and have had little idea of how it is done. I set out to create a python script to mix songs to understand the process a bit better. I first found that DJs use many effects in their mixes. These effects include changing tempo of songs, changing the pitch, as well as adding noises from other tracks.\nFor my project, I focused on three effects: repeat, fade, and speed. The repeat effects repeats a portion of the audio segment some number of times. The fade effect fades the audio by some amount of decibels. Lastly, the speed command allows the user to speed up or slow down the song.\nThe general flow of mixing the songs was this:\nNormalize tempo of the songs Add drum file at the same tempo as the songs Add effects according to a user-created effects file Add a GUI that is syncronized with the song Libraries Used In order to accomplish the above, I leveraged many python libraries.\nAubio: Aubio is a library that helps with analysis of sound files. I used aubio to get the tempo of songs and to detect beats. Pydub: Pydub is a great library for audio manipulation. I used pydub to apply audio effects. Pygame: Pygame is a library that allows you to create graphics and games. I used pygame to create the graphic to go along with the audio. Challenges Synchronization of songs My first pass at a solution for normalizing the tempo of the songs included calculating the bpm of both songs, then speeding one up to match. This was not a successful approach, however, since the bpm of songs is not constant throughout due to tempo changes as well as speeding up or slowing down. To solve this problem, I built a system that iterated through all the beats in the songs and made sure they were lined up properly. By adjusting the fidelity of how often to assert that the beats were aligned with the drums, I could line the beats up more accurately. However, this took much more time, creating a tradeoff between speed and accuracy. Coming up with a way to mix songs Finding a good approach to mixing songs was difficult, as I had little prior experience. I settled on two ways of mixing: overlaying, and adding custom effects. Customizing drum track I overlaid a custom drum track made using csound. Making this drum track flexible with the needs of my project was difficult and required becoming familiar with csound. Extensions Adding more effects Adding effects such as added noises and changing pitch would present interesting technical challenges. Random element I am curious to see how randomizing effects would sound. There is a possibility for using machine learning for a computer to make dj mixes that sound good. Working with audio stream An interesting capability would be for a program to be able to edit an audio stream on the fly. This could be used to mix live music. ","permalink":"https://lucaspauker.com/articles/automatic-dj/","summary":"This project done for my CS106X class mixes two given mp3 files together and has a visualization to go with it.\nIntroduction I have always been awestruck by DJing and have had little idea of how it is done. I set out to create a python script to mix songs to understand the process a bit better. I first found that DJs use many effects in their mixes. These effects include changing tempo of songs, changing the pitch, as well as adding noises from other tracks.","title":"Automatic DJ Project"},{"content":"I am from Chicago, IL and work in trading. I graduated from Stanford in 2023 with a BS in physics and MS in computer science. I use this blog to write about things that are interesting to me.\n","permalink":"https://lucaspauker.com/about/","summary":"about","title":"About Me"},{"content":"Here are some of the projects I have made or worked on in the past few years. Check out my GitHub for more.\nTerracotta AI Terracotta is a platform for fine-tuning and evaluating large language models. With Terracotta, we make iterating on the LLM workflow quick and easy. Here are a few features that we have:\nTraining: Users can upload a dataset and fine-tune different LLMs provided by OpenAI within minutes using our training dashboard. Qualitative evaluation: A playground for qualitatively comparing base models vs. fine-tuned models from OpenAI and Cohere. Quantitative evaluation: An evaluation tool that lets you run inference on a dataset with any model with a few clicks, and compare several models across different NLP metrics of your choice. We calculate relevant metrics depending on what kind of task you want to do. Lifta Lifta is a free iOS app to track your lifting progress over time and display analytics about it. With Lifta, you can:\nRecord workouts when you go to the gym See your past workouts See graphs of specific workouts and sets/reps over time Track total volume over time Edit previous workouts Share workouts with the larger community H-2A Vision H-2A Vision is a free tool for easily finding information about H-2A violations. This website can be used by people looking for H-2A work who want to search potential employers. This website can also be used people researching H-2A. The data is from the US Department of Labor Wage and Hour Division.\nClass projects NLP Paper: Automated Basketball Video Captioning RL Paper: Hierarchical Reinforcement Learning Astronomy Paper: Measuring Stellar Formation Rates Stats Paper: Identifying Pairs for Pairs Trading NLU Paper: Understanding Congressional Debates Solar physics research Predicting Solar Flares Using Time Series Analysis Paper Time Series Analysis of Flaring Active Regions Poster Magnetic Signatures of Sympathetic Flares Poster ","permalink":"https://lucaspauker.com/projects/","summary":"My projects","title":"Projects"}]