<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Lucas Pauker</title>
<meta name="keywords" content="">
<meta name="description" content="05/04/2021 #Reinforcement Learning Applied to Blackjack
##1. Introduction I recently read Ed Thorpe&rsquo;s Beat the Dealer, a book about how Thorpe, a mathematician, found a way to gain an edge in blackjack. In the book, Thorpe uses computer simulations to calculate the best blackjack strategy as well as card-counting strategies. Since I took a reinforcement learning class last quarter, I wanted to apply one of the most common algorithms, Q-learning, to to find the best strategy for blackjack.">
<meta name="author" content="Lucas Pauker">
<link rel="canonical" href="//localhost:1313/posts/blackjack_rl/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="//localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="//localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="//localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="//localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="//localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="//localhost:1313/posts/blackjack_rl/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="" />
<meta property="og:description" content="05/04/2021 #Reinforcement Learning Applied to Blackjack
##1. Introduction I recently read Ed Thorpe&rsquo;s Beat the Dealer, a book about how Thorpe, a mathematician, found a way to gain an edge in blackjack. In the book, Thorpe uses computer simulations to calculate the best blackjack strategy as well as card-counting strategies. Since I took a reinforcement learning class last quarter, I wanted to apply one of the most common algorithms, Q-learning, to to find the best strategy for blackjack." />
<meta property="og:type" content="article" />
<meta property="og:url" content="//localhost:1313/posts/blackjack_rl/" /><meta property="og:image" content="//localhost:1313/images/papermod-cover.png"/><meta property="article:section" content="posts" />



<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="//localhost:1313/images/papermod-cover.png"/>

<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="05/04/2021 #Reinforcement Learning Applied to Blackjack
##1. Introduction I recently read Ed Thorpe&rsquo;s Beat the Dealer, a book about how Thorpe, a mathematician, found a way to gain an edge in blackjack. In the book, Thorpe uses computer simulations to calculate the best blackjack strategy as well as card-counting strategies. Since I took a reinforcement learning class last quarter, I wanted to apply one of the most common algorithms, Q-learning, to to find the best strategy for blackjack."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "//localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "",
      "item": "//localhost:1313/posts/blackjack_rl/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "",
  "name": "",
  "description": "05/04/2021 #Reinforcement Learning Applied to Blackjack\n##1. Introduction I recently read Ed Thorpe\u0026rsquo;s Beat the Dealer, a book about how Thorpe, a mathematician, found a way to gain an edge in blackjack. In the book, Thorpe uses computer simulations to calculate the best blackjack strategy as well as card-counting strategies. Since I took a reinforcement learning class last quarter, I wanted to apply one of the most common algorithms, Q-learning, to to find the best strategy for blackjack.",
  "keywords": [
    
  ],
  "articleBody": "05/04/2021 #Reinforcement Learning Applied to Blackjack\n##1. Introduction I recently read Ed Thorpe’s Beat the Dealer, a book about how Thorpe, a mathematician, found a way to gain an edge in blackjack. In the book, Thorpe uses computer simulations to calculate the best blackjack strategy as well as card-counting strategies. Since I took a reinforcement learning class last quarter, I wanted to apply one of the most common algorithms, Q-learning, to to find the best strategy for blackjack.\n(200)(center)Beat the Dealer book cover. Taken from here\n##2. Rules In order to find the best way to play blackjack, it is essential to understand all the rules. For my simulations, I used a simplified version of blackjack where the player can only hit or stay. In a game, the player plays against the dealer. The round starts by dealing both the player and the dealer two cards. The player’s cards are dealt face up, while the dealer has one face down card and one face up card.\n(500)(center)Taken from here\n###Winning the game The goal of the game is for the player’s cards to sum up to as close to 21 as possible without going over 21, which is called busting. All face cards have a value of 10 and aces can take a value of either 1 or 11. The player wins if their cards are higher than the dealer’s without busting. If the dealer busts and the player doesn’t then the player wins. Else, the dealer wins.\n###Player actions In the simplified version of blackjack I used for this article, after the cards are dealt, the player has a choice of two actions–they can stay or hit. If the player stays, then their turn is over. If the player hits, then they are dealt another card.\n###Dealer actions The dealer’s actions are deterministic, i.e. the dealer has no agency. The dealer must hit on hands with a value of 16 or less. Furthermore, they hit on hands that have a value of 17 if one of the cards is an ace (and the ace is counted as an 11). Hands with an ace where the ace is counted as an 11 are known as “soft” hands since if the hand value goes over 21, the ace can turn into a 1. So the dealer must hit on soft 17s.\n###Other considerations Another consideration that we must take into account are the number of decks. Usually there are between 1 and 8 decks at a casino. I used 6 decks for my tests.\n##3. State Space and Actions To frame the blackjack game as a reinforcement learning problem, we have to define the set of states and the set of actions.\n###States For blackjack, the only information the player has is what they can see on the table. This includes their hand and the face up dealer card (dealer hole card). Since only the values of the cards matter, we can define the state space as the value of the hand as well as the dealer hole card. However, since aces can have a value of either 1 or 11, we must include whether there is an ace in the hand that can have a value of either 1 or 11 without going over 21. As noted earlier, hands with such an ace are called soft. If the value of a certain hand is high enough, the ace is forced to be a 1 so that the player doesn’t bust. Such a hand is considered hard, not soft even though there is an ace in the hand. So our state space is the set of all possible values of hands and dealer face up cards, where the value of a hand is the sum of all the card values as well as whether the hand is soft or hard. We can therefore represent the state of a game as a tuple: (hand sum, is the hand soft, dealer hole card).\nThe numerical value of a hand can take on any value between 4 and 21 (18 possible values). Values of hands 12 or above can be either soft or hard, so there are 28 total possible hand values. The dealer card can take on 10 distinct values. Therefore, our state space includes 28*10=280 possible states.\n###Policies As discussed before, the player has two possible actions: stay and hit. A policy is a map of states to actions. A policy decides what action to take given a state. The goal of reinforcement learning is to find the optimal policy, which is the optimal action for each state. There are therefore $2^{280}=1.94*10^{84}$ possible policies that we could create. Clearly, iterating through all such policies would be impossible with a laptop, so we will instead find better ways to find an optimal policy.\n##4. Q-Learning Since we can model blackjack as a Markov decision process, using reinforcement learning (RL) to learn an optimal policy is natural. I used Q-learning to find the best way to play blackjack. I will describe what Q-learning is in this section.\n###Q function The $Q$ function, or the state-action value function is central to Q-learning. $Q$ is defined with respect to a state $s\\in\\mathcal{S}$ and an action $a\\in\\mathcal{A}$, where $\\mathcal{S}$ is the set of all states and $\\mathcal{A}$ is the set of all actions.\n$$Q^\\pi(s,a)$$\nThe function is parameterized by a policy $\\pi$ and represents the expected return if one starts from state $s$, takes action $a$ and then follows policy $\\pi$. Essentially, it represents how good an action is from a state.\n###Q update (learning) A core idea that we will use for learning an optimal policy is testing out different actions, then evaluating the actions. This is represented in the following diagram:\n(500)(center)Relationship between world and agent, taken from the CS234 lecture slides (Winter 2021 lecture 4)\nThe $Q$ function is updated as new data comes in. Each data point is in the form of a (state, action, reward, next state) tuple. For a timestep $t$, this is represented as $(s_t, a_t, r_t, s_{t+1})$. Note that since we use $Q(s_{t+1}, a)$ in the Q update, we are boostrapping. The Q update is described below.\nQ update: $$Q(s_t,a_t)\\leftarrow Q(s_t,a_t) + \\alpha * (r_t + \\gamma * \\text{arg}\\max_a Q(s_{t+1}, a) - Q(s_t,a_t))$$\n$\\alpha$ is the learning rate, and $\\gamma$ is the discount factor. $\\alpha$ controls how fast the policy will update based on new information. $\\gamma$ controls the weighting of data in the future vs. data now. Both $\\alpha$ and $\\gamma$ are values between 0 and 1. To understand the Q update, we can break it down into simpler components.\nError term: $$r_t + \\gamma * \\text{arg}\\max_a Q(s_{t+1}, a_t) - Q(s_t,a_t)$$\nWe can see that $r_t + \\gamma * \\text{arg}\\max_a Q(s_{t+1}, a)$ is the discounted future reward starting from state $s_t$ and taking action $a_t$. The error term represents how far off the predicted value of the $Q$ function is versus the stored value.\nNow that we understand the $Q$ function and the Q update, we will discuss some properties of Q-learning. These properties will be important for tuning the algorithm and acheiving good results.\n###Bootstrapping Q-learning uses bootstrapping to estimate the state-value function. Bootstrapping in the context of reinforcement learning means that the model uses estimated $Q$ function values to update the $Q$ function. It is clear that the Q update uses bootstrapping since it uses estimated $Q$ function values when it computes the $Q(s_{t+1}, a)$ for all $a$.\n###Model-free and off-policy Q-learning is a model-free RL algorithm, meaning that it does not keep a “model” of the world. A model of the world is the transition probabilities and rewards between states. Q-learning, instead of keeping track of an estimated model, directly learns the $Q$ function from the data.\nQ-learning is also an off-policy RL algorithm since it estimates the value of the optimal policy while acting on a different policy. This idea is present in this term in the Q update: $$\\text{arg}\\max_a Q(s_{t+1}, a_t).$$ The on-policy update would be to use $Q(s_{t+1}, a_{t+1})$. This on-policy update would estimate the value of the current policy. However, when we add the max term, we have off-policy learning.\n###Exploration vs. exploitation How will we decide which actions to take in blackjack to best update our $Q$ function? On one hand, we want to maximize the expected return from the Q function, which means we want to exploit the values we have alreadly learned. However, we also want to explore new actions in case we got unlucky in a trial, leading to an incorrect Q value. One way to balance exploration and exploitation is with an epsilon-greedy policy. For some value of $\\epsilon$ between 0 and 1, we take a random action with probability $\\epsilon$ and we take the best action with probability $1-\\epsilon$. Therefore, by adjusting $\\epsilon$, we can control the amount of exploration that our agent does. We typically take $\\epsilon$ to zero after many timesteps.\n##5. Model We will now focus on how to learn the best policy, which is the function that decides which action to take for each state. There are two steps to do model-free policy iteration: policy evaluation and policy improvement. The policy evaluation step consists of computing the Q values for a specific policy. The policy improvement step consists of updating the policy given the new Q values. Here is some pseudo-code for these two steps as used to learn a good blackjack policy:\ndef learn_policy(): t = 0 repeat NUMBER_OF_TIMESTEPS times: simulate 100,000 games of blackjack using the learned policy update Q values based on the (state, action, reward, next state) tuples from the simulations update the policy to be epsilon greedy with respect to the learned Q values t += 1 Simulating the games of blackjack and updating the Q values constitutes the policy evaluation step. The policy improvement step is simply creating the new policy with respect to the Q values.\n##6. Tuning the Model ###Alpha Recall that $\\alpha$ is the learning rate for the model. It controls how fast the Q values will be updated. To ensure that Q-learning converges to the optimal $Q$ values, we must visit all state-action pairs infinitely often, and the step sizes $\\alpha_t$ must satisfy the Robbins-Munro sequence. To satisfy the first condition, we can uniformly pick states and actions to make sure that we visit all state-action pairs infinitely often. For the second condition, we choose $\\alpha_t=1/t$. This satisfies the Robbins-Munro sequence, which is the following two inequalities: $$\\sum_{t=1}^\\infty\\alpha_t=\\infty$$ and $$\\sum_{t=1}^\\infty\\alpha_t^2\u003c\\infty.$$\n###Epsilon $\\epsilon$ is the amount of exploration we conduct with the $\\epsilon$-greedy policy. To ensure that Q-learning converges to the optimal policy, we must visit all state-action pairs infinitely often and ensure that the $\\epsilon$-greedy policy converges to a greedy policy. A common strategy to satisfy these conditions (known as GLIE) is setting $\\epsilon_t=1/t$.\n###Gamma $\\gamma$ is the discount rate. I set $\\gamma$=1 since it performed the best in trials.\n##7. Results Now, we will see how well our Q-learning function does compared to the optimal blackjack strategy! I took the optimal strategy from this website. The optimal strategy has a win rate of 0.428 and a loss rate of 0.478.\n###Baselines I used two suboptimal baseline strategies to compare against our learned policies: random strategy and stand only strategy. The random strategy has a win rate of 0.282 and a loss rate of 0.676. The stand only strategy has a win rate of 0.382 and a loss rate of 0.566.\n###Win rate We will see how the Q-learning strategy compares with the baselines as well as the optimal strategy after many time steps. Here is a plot of the win rate of the Q-learning strategy versus number of time steps. Each time steps is 100,000 games of blackjack.\n(500)(center)Win rate versus number of time steps\nWe can see that the Q-learning strategy reaches the optimal win rate in around 50 time steps. Furthermore, as time goes on, the win rate seems to stabilize, indicating it is converging on the optimal strategy.\n###Policy Here is the optimal policy for the game. S=stand and H=hit.\n(500)(center)Hard total optimal policy\n(500)(center)Soft total optimal policy\nHere is the learned policy for the game after 250 iterations. Places where the learned policy deviates from the optimal policy are crossed out.\n(500)(center)Hard total learned policy\n(500)(center)Soft total learned policy\nAs we can see, our learned policy is very close to the optimal policy: we have two total errors. We can see from the win rate vs. time steps graph that this small variation in policy does not affect the win rate much. I believe that with more time steps, it is possible to achieve the optimal policy.\n##8. Conclusion I hope that this article gave some insight into Q-learning and showed an application for the algorithm. Here is the code I used for my simulation: https://github.com/lucaspauker/blackjack_reinforcement_learning. Feel free to play with it and see if there is a way to improve the algorithm.\n",
  "wordCount" : "2157",
  "inLanguage": "en",
  "datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Lucas Pauker"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "//localhost:1313/posts/blackjack_rl/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Lucas Pauker",
    "logo": {
      "@type": "ImageObject",
      "url": "//localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="//localhost:1313/" accesskey="h" title="Lucas Pauker (Alt + H)">Lucas Pauker</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="//localhost:1313/archive/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="//localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="//localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      
    </h1>
    <div class="post-meta">11 min&nbsp;·&nbsp;Lucas Pauker&nbsp;|&nbsp;<a href="https://github.com/adityatelange/hugo-PaperMod/tree/exampleSite/content/posts/blackjack_rl.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> 

  <div class="post-content"><p>05/04/2021
#Reinforcement Learning Applied to Blackjack</p>
<p>##1. Introduction
I recently read Ed Thorpe&rsquo;s <em>Beat the Dealer</em>, a book about how Thorpe, a mathematician, found a way to gain an edge in blackjack.
In the book, Thorpe uses computer simulations to calculate the best blackjack strategy as well as card-counting strategies.
Since I took a reinforcement learning class last quarter, I wanted to apply one of the most common algorithms, Q-learning, to to find the best strategy for blackjack.</p>
<p><img loading="lazy" src="blackjack/beat_the_dealer.png" alt="Beat the Dealer"  />

(200)(center)Beat the Dealer book cover. Taken from <a href="https://www.amazon.com/Beat-Dealer-Winning-Strategy-Twenty-One/dp/0394703103">here</a></p>
<p>##2. Rules
In order to find the best way to play blackjack, it is essential to understand all the rules.
For my simulations, I used a simplified version of blackjack where the player can only hit or stay.
In a game, the player plays against the dealer.
The round starts by dealing both the player and the dealer two cards.
The player&rsquo;s cards are dealt face up, while the dealer has one face down card and one face up card.</p>
<p><img loading="lazy" src="blackjack/playing_blackjack.jpg" alt="Playing blackjack"  />

(500)(center)Taken from <a href="https://www.forbes.com/sites/johnnavin/2018/04/01/interview-with-a-blackjack-pro-i-still-win-playing-21-in-las-vegas/?sh=6aaad9953ce6">here</a></p>
<p>###Winning the game
The goal of the game is for the player&rsquo;s cards to sum up to as close to 21 as possible without going over 21, which is called busting.
All face cards have a value of 10 and aces can take a value of either 1 or 11.
The player wins if their cards are higher than the dealer&rsquo;s without busting.
If the dealer busts and the player doesn&rsquo;t then the player wins.
Else, the dealer wins.</p>
<p>###Player actions
In the simplified version of blackjack I used for this article, after the cards are dealt, the player has a choice of two actions&ndash;they can <em>stay</em> or <em>hit</em>.
If the player stays, then their turn is over.
If the player hits, then they are dealt another card.</p>
<p>###Dealer actions
The dealer&rsquo;s actions are deterministic, i.e. the dealer has no agency.
The dealer must hit on hands with a value of 16 or less.
Furthermore, they hit on hands that have a value of 17 if one of the cards is an ace (and the ace is counted as an 11).
Hands with an ace where the ace is counted as an 11 are known as &ldquo;soft&rdquo; hands since if the hand value goes over 21, the ace can turn into a 1.
So the dealer must hit on soft 17s.</p>
<p>###Other considerations
Another consideration that we must take into account are the number of decks.
Usually there are between 1 and 8 decks at a casino.
I used 6 decks for my tests.</p>
<p>##3. State Space and Actions
To frame the blackjack game as a reinforcement learning problem, we have to define the set of states and the set of actions.</p>
<p>###States
For blackjack, the only information the player has is what they can see on the table.
This includes their hand and the face up dealer card (dealer hole card).
Since only the values of the cards matter, we can define the state space as the value of the hand as well as the dealer hole card.
However, since aces can have a value of either 1 or 11, we must include whether there is an ace in the hand that can have a value of either 1 or 11 without going over 21.
As noted earlier, hands with such an ace are called soft.
If the value of a certain hand is high enough, the ace is forced to be a 1 so that the player doesn&rsquo;t bust.
Such a hand is considered hard, not soft even though there is an ace in the hand.
So our state space is the set of all possible values of hands and dealer face up cards, where the value of a hand is the sum of all the card values as well as whether the hand is soft or hard.
We can therefore represent the state of a game as a tuple: (hand sum, is the hand soft, dealer hole card).</p>
<p>The numerical value of a hand can take on any value between 4 and 21 (18 possible values).
Values of hands 12 or above can be either soft or hard, so there are 28 total possible hand values.
The dealer card can take on 10 distinct values.
Therefore, our state space includes 28*10=280 possible states.</p>
<p>###Policies
As discussed before, the player has two possible actions: <em>stay</em> and <em>hit</em>.
A policy is a map of states to actions.
A policy decides what action to take given a state.
The goal of reinforcement learning is to find the optimal policy, which is the optimal action for each state.
There are therefore $2^{280}=1.94*10^{84}$ possible policies that we could create.
Clearly, iterating through all such policies would be impossible with a laptop, so we will instead find better ways to find an optimal policy.</p>
<p>##4. Q-Learning
Since we can model blackjack as a Markov decision process, using reinforcement learning (RL) to learn an optimal policy is natural.
I used Q-learning to find the best way to play blackjack.
I will describe what Q-learning is in this section.</p>
<p>###Q function
The $Q$ function, or the state-action value function is central to Q-learning.
$Q$ is defined with respect to a state $s\in\mathcal{S}$ and an action $a\in\mathcal{A}$, where $\mathcal{S}$ is the set of all states and $\mathcal{A}$ is the set of all actions.</p>
<p>$$Q^\pi(s,a)$$</p>
<p>The function is parameterized by a policy $\pi$ and represents the expected return if one starts from state $s$, takes action $a$ and then follows policy $\pi$.
Essentially, it represents how good an action is from a state.</p>
<p>###Q update (learning)
A core idea that we will use for learning an optimal policy is testing out different actions, then evaluating the actions.
This is represented in the following diagram:</p>
<p><img loading="lazy" src="blackjack/world_agent_relationship.png" alt="World-agent relationship"  />

(500)(center)Relationship between world and agent, taken from the CS234 lecture slides (Winter 2021 lecture 4)</p>
<p>The $Q$ function is updated as new data comes in.
Each data point is in the form of a (state, action, reward, next state) tuple.
For a timestep $t$, this is represented as $(s_t, a_t, r_t, s_{t+1})$.
Note that since we use $Q(s_{t+1}, a)$ in the Q update, we are boostrapping.
The Q update is described below.</p>
<p><strong>Q update:</strong>
$$Q(s_t,a_t)\leftarrow Q(s_t,a_t) + \alpha * (r_t + \gamma * \text{arg}\max_a Q(s_{t+1}, a) - Q(s_t,a_t))$$</p>
<p>$\alpha$ is the learning rate, and $\gamma$ is the discount factor.
$\alpha$ controls how fast the policy will update based on new information.
$\gamma$ controls the weighting of data in the future vs. data now.
Both $\alpha$ and $\gamma$ are values between 0 and 1.
To understand the Q update, we can break it down into simpler components.</p>
<p><strong>Error term:</strong>
$$r_t + \gamma * \text{arg}\max_a Q(s_{t+1}, a_t) - Q(s_t,a_t)$$</p>
<p>We can see that $r_t + \gamma * \text{arg}\max_a Q(s_{t+1}, a)$ is the discounted future reward starting from state $s_t$ and taking action $a_t$.
The error term represents how far off the <em>predicted</em> value of the $Q$ function is versus the stored value.</p>
<p>Now that we understand the $Q$ function and the Q update, we will discuss some properties of Q-learning.
These properties will be important for tuning the algorithm and acheiving good results.</p>
<p>###Bootstrapping
Q-learning uses bootstrapping to estimate the state-value function.
Bootstrapping in the context of reinforcement learning means that the model uses <em>estimated</em> $Q$ function values to update the $Q$ function.
It is clear that the Q update uses bootstrapping since it uses estimated $Q$ function values when it computes the $Q(s_{t+1}, a)$ for all $a$.</p>
<p>###Model-free and off-policy
Q-learning is a model-free RL algorithm, meaning that it does not keep a &ldquo;model&rdquo; of the world.
A model of the world is the transition probabilities and rewards between states.
Q-learning, instead of keeping track of an estimated model, directly learns the $Q$ function from the data.</p>
<p>Q-learning is also an off-policy RL algorithm since it estimates the value of the optimal policy while acting on a different policy.
This idea is present in this term in the Q update:
$$\text{arg}\max_a Q(s_{t+1}, a_t).$$
The <em>on-policy</em> update would be to use $Q(s_{t+1}, a_{t+1})$.
This on-policy update would estimate the value of the current policy.
However, when we add the max term, we have off-policy learning.</p>
<p>###Exploration vs. exploitation
How will we decide which actions to take in blackjack to best update our $Q$ function?
On one hand, we want to maximize the expected return from the Q function, which means we want to <em>exploit</em> the values we have alreadly learned.
However, we also want to <em>explore</em> new actions in case we got unlucky in a trial, leading to an incorrect Q value.
One way to balance exploration and exploitation is with an epsilon-greedy policy.
For some value of $\epsilon$ between 0 and 1, we take a random action with probability $\epsilon$ and we take the best action with probability $1-\epsilon$.
Therefore, by adjusting $\epsilon$, we can control the amount of exploration that our agent does.
We typically take $\epsilon$ to zero after many timesteps.</p>
<p>##5. Model
We will now focus on how to learn the best policy, which is the function that decides which action to take for each state.
There are two steps to do model-free policy iteration: policy evaluation and policy improvement.
The policy evaluation step consists of computing the Q values for a specific policy.
The policy improvement step consists of updating the policy given the new Q values.
Here is some pseudo-code for these two steps as used to learn a good blackjack policy:</p>
<pre tabindex="0"><code>def learn_policy():
  t = 0
  repeat NUMBER_OF_TIMESTEPS times:
    simulate 100,000 games of blackjack using the learned policy
    update Q values based on the (state, action, reward, next state) tuples from the simulations
    update the policy to be epsilon greedy with respect to the learned Q values
    t += 1
</code></pre><p>Simulating the games of blackjack and updating the Q values constitutes the policy evaluation step.
The policy improvement step is simply creating the new policy with respect to the Q values.</p>
<p>##6. Tuning the Model
###Alpha
Recall that $\alpha$ is the learning rate for the model.
It controls how fast the Q values will be updated.
To ensure that Q-learning converges to the optimal $Q$ values, we must visit all state-action pairs infinitely often, and the step sizes $\alpha_t$ must satisfy the Robbins-Munro sequence.
To satisfy the first condition, we can uniformly pick states and actions to make sure that we visit all state-action pairs infinitely often.
For the second condition, we choose $\alpha_t=1/t$.
This satisfies the Robbins-Munro sequence, which is the following two inequalities:
$$\sum_{t=1}^\infty\alpha_t=\infty$$
and
$$\sum_{t=1}^\infty\alpha_t^2&lt;\infty.$$</p>
<p>###Epsilon
$\epsilon$ is the amount of exploration we conduct with the $\epsilon$-greedy policy.
To ensure that Q-learning converges to the optimal policy, we must visit all state-action pairs infinitely often and ensure that the $\epsilon$-greedy policy converges to a greedy policy.
A common strategy to satisfy these conditions (known as GLIE) is setting $\epsilon_t=1/t$.</p>
<p>###Gamma
$\gamma$ is the discount rate.
I set $\gamma$=1 since it performed the best in trials.</p>
<p>##7. Results
Now, we will see how well our Q-learning function does compared to the optimal blackjack strategy!
I took the optimal strategy from <a href="https://www.blackjackapprenticeship.com/blackjack-strategy-charts/">this website</a>.
The optimal strategy has a win rate of 0.428 and a loss rate of 0.478.</p>
<p>###Baselines
I used two suboptimal baseline strategies to compare against our learned policies: random strategy and stand only strategy.
The random strategy has a win rate of 0.282 and a loss rate of 0.676.
The stand only strategy has a win rate of 0.382 and a loss rate of 0.566.</p>
<p>###Win rate
We will see how the Q-learning strategy compares with the baselines as well as the optimal strategy after many time steps.
Here is a plot of the win rate of the Q-learning strategy versus number of time steps.
Each time steps is 100,000 games of blackjack.</p>
<p><img loading="lazy" src="blackjack/win_loss_plot.png" alt="Win rate versus number of time steps"  />

(500)(center)Win rate versus number of time steps</p>
<p>We can see that the Q-learning strategy reaches the optimal win rate in around 50 time steps.
Furthermore, as time goes on, the win rate seems to stabilize, indicating it is converging on the optimal strategy.</p>
<p>###Policy
Here is the optimal policy for the game.
S=stand and H=hit.</p>
<p><img loading="lazy" src="blackjack/hard_optimal_policy.png" alt="Hard total optimal policy"  />

(500)(center)Hard total optimal policy</p>
<p><img loading="lazy" src="blackjack/soft_optimal_policy.png" alt="Soft total optimal policy"  />

(500)(center)Soft total optimal policy</p>
<p>Here is the learned policy for the game after 250 iterations.
Places where the learned policy deviates from the optimal policy are crossed out.</p>
<p><img loading="lazy" src="blackjack/hard_learned_policy.png" alt="Hard total learned policy"  />

(500)(center)Hard total learned policy</p>
<p><img loading="lazy" src="blackjack/soft_learned_policy.png" alt="Soft total learned policy"  />

(500)(center)Soft total learned policy</p>
<p>As we can see, our learned policy is very close to the optimal policy: we have two total errors.
We can see from the win rate vs. time steps graph that this small variation in policy does not affect the win rate much.
I believe that with more time steps, it is possible to achieve the optimal policy.</p>
<p>##8. Conclusion
I hope that this article gave some insight into Q-learning and showed an application for the algorithm.
Here is the code I used for my simulation: <a href="https://github.com/lucaspauker/blackjack_reinforcement_learning">https://github.com/lucaspauker/blackjack_reinforcement_learning</a>.
Feel free to play with it and see if there is a way to improve the algorithm.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="//localhost:1313/posts/auto_mixer/">
    <span class="title">« Prev</span>
    <br>
    <span></span>
  </a>
  <a class="next" href="//localhost:1313/posts/challenges_facing_h2a_farmers_and_workers/">
    <span class="title">Next »</span>
    <br>
    <span></span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share  on x"
            href="https://x.com/intent/tweet/?text=&amp;url=%2f%2flocalhost%3a1313%2fposts%2fblackjack_rl%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share  on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2f%2flocalhost%3a1313%2fposts%2fblackjack_rl%2f&amp;title=&amp;summary=&amp;source=%2f%2flocalhost%3a1313%2fposts%2fblackjack_rl%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share  on reddit"
            href="https://reddit.com/submit?url=%2f%2flocalhost%3a1313%2fposts%2fblackjack_rl%2f&title=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share  on facebook"
            href="https://facebook.com/sharer/sharer.php?u=%2f%2flocalhost%3a1313%2fposts%2fblackjack_rl%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share  on whatsapp"
            href="https://api.whatsapp.com/send?text=%20-%20%2f%2flocalhost%3a1313%2fposts%2fblackjack_rl%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share  on telegram"
            href="https://telegram.me/share/url?text=&amp;url=%2f%2flocalhost%3a1313%2fposts%2fblackjack_rl%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share  on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=&u=%2f%2flocalhost%3a1313%2fposts%2fblackjack_rl%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="//localhost:1313/">Lucas Pauker</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
