<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>CS on Lucas Pauker</title>
    <link>//localhost:1311/tags/cs/</link>
    <description>Recent content in CS on Lucas Pauker</description>
    <image>
      <title>Lucas Pauker</title>
      <url>//localhost:1311/images/papermod-cover.png</url>
      <link>//localhost:1311/images/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 15 Oct 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="//localhost:1311/tags/cs/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>OpenAI Model Timing</title>
      <link>//localhost:1311/articles/openai-model-timing/</link>
      <pubDate>Sun, 15 Oct 2023 00:00:00 +0000</pubDate>
      <guid>//localhost:1311/articles/openai-model-timing/</guid>
      <description>Introduction The goal of this article is to explore the latency of different OpenAI models. When using AI models in production, latency is an important factor to consider.
Comparing Model Architectures First, I test the latency for different OpenAI models. I test the following models: gpt-4, gpt-4-0613, gpt-3.5-turbo, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613, text-davinci-003, text-davinci-002, text-davinci-001, text-curie-001, text-babbage-001, text-ada-001, davinci-002, babbage-002, davinci, curie, babbage, and ada. These are all the OpenAI models that are available for inference through the chat and completions endpoints.</description>
    </item>
    <item>
      <title>LLMs Unleashed: The Power of Fine-Tuning</title>
      <link>//localhost:1311/articles/llms-unleashed-the-power-of-fine-tuning/</link>
      <pubDate>Sun, 23 Jul 2023 00:00:00 +0000</pubDate>
      <guid>//localhost:1311/articles/llms-unleashed-the-power-of-fine-tuning/</guid>
      <description>Disclaimer: This article mentions https://terra-cotta.ai/, an LLM experimentation platform I am building
Introduction ChatGPT, Bard, and other large language models (LLMs) are very useful for a wide variety of tasks from writing code to answering complex questions to aiding with education. However, these models are ultimately limited by the data that they are trained on. Also, these models are trained to be able to answer a wide variety of questions which may not be sufficient for domain-specific questions.</description>
    </item>
    <item>
      <title>50 AI Applications</title>
      <link>//localhost:1311/articles/50-practical-applications-of-artificial-intelligence-and-language-models/</link>
      <pubDate>Tue, 04 Apr 2023 00:00:00 +0000</pubDate>
      <guid>//localhost:1311/articles/50-practical-applications-of-artificial-intelligence-and-language-models/</guid>
      <description>Advancements in artificial intelligence and language models have made significant impacts in various fields from healthcare to finance to entertainment. Here are 50 practical applications of AI that are currently in use or have the potential to be implemented in various industries. Let me know if any of these ideas inspire you or if you build any of them!
Text Analysis Automatically generate outlines or summaries of news articles. Find fake news and provide a citation with the real source.</description>
    </item>
    <item>
      <title>Blackjack Reinforcement Learning</title>
      <link>//localhost:1311/articles/reinforcement-learning-applied-to-blackjack/</link>
      <pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate>
      <guid>//localhost:1311/articles/reinforcement-learning-applied-to-blackjack/</guid>
      <description>Introduction I recently read Ed Thorpe&amp;rsquo;s Beat the Dealer, a book about how Thorpe, a mathematician, found a way to gain an edge in blackjack. In the book, Thorpe uses computer simulations to calculate the best blackjack strategy as well as card-counting strategies. Since I took a reinforcement learning class last quarter, I wanted to apply one of the most common algorithms, Q-learning, to to find the best strategy for blackjack.</description>
    </item>
    <item>
      <title>Achieving Quantum Supremacy, Qubit by Qubit</title>
      <link>//localhost:1311/articles/achieving-quantum-supremacy-qubit-by-qubit/</link>
      <pubDate>Thu, 11 Mar 2021 00:00:00 +0000</pubDate>
      <guid>//localhost:1311/articles/achieving-quantum-supremacy-qubit-by-qubit/</guid>
      <description>Faster than a Supercomputer? In the 1980s, American physicist Richard Feynman proposed the idea of quantum computers to model complex quantum systems. In October 2019, around 40 years later, Google AI and NASA scientists unveiled a quantum computer which ran an experiment in a few minutes that would take the fastest supercomputer 10,000 years. The quantum computer sped up the computation by a factor of 1 billion! This was one of the first major successes in the nascent field of quantum computing.</description>
    </item>
    <item>
      <title>Simple Stock Market Models with Python</title>
      <link>//localhost:1311/articles/simple-stock-market-models-with-python/</link>
      <pubDate>Sun, 20 Dec 2020 00:00:00 +0000</pubDate>
      <guid>//localhost:1311/articles/simple-stock-market-models-with-python/</guid>
      <description>Introduction In this blog post, I will implement a few simple time series models of a stock price over time. I will also see how they do if we trade using them. We will look at moving averages (MA) and exponential moving averages (EMA).
Data First, we need to download the price data. For this article, we will use SPY historical open price data. We can download this from Yahoo Finance.</description>
    </item>
    <item>
      <title>Solar Flare Time Series Research</title>
      <link>//localhost:1311/articles/understanding-solar-flares-with-time-series-data/</link>
      <pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate>
      <guid>//localhost:1311/articles/understanding-solar-flares-with-time-series-data/</guid>
      <description>Introduction I spent the summer of 2019 as a physics research intern at the Stanford University Solar Lab. I was very fortunate to have a wonderful advisor and had a great summer overall. I created machine learning models to characterize time series data for solar flare prediction. In this article, I will first provide some physics background about solar flares, then dive into my research. For a more in-depth analysis, check out the source code and my poster.</description>
    </item>
    <item>
      <title>Classical Music Classifier Project</title>
      <link>//localhost:1311/articles/music-matcher/</link>
      <pubDate>Fri, 28 Jun 2019 00:00:00 +0000</pubDate>
      <guid>//localhost:1311/articles/music-matcher/</guid>
      <description>Introduction This project done for my CS221 class aims to classify classical music by musical era (Baroque, Classical, Romantic, Modern) with composers as a proxy.
Using audio processing techniques, such as Short-time Fourier Transform, we extracted features such as the spectrogram and chromagram of the audio data from two datasets, Free Music Archive and MAESTRO.
We used two ensemble classifiers, AdaBoost and Random Forest, and found that although Adaboost performed marginally better than Random Forest, the latter made more generalizable predictions.</description>
    </item>
  </channel>
</rss>
